================================================================
Generated: 2025-07-22 21:03:19
System Version: Production Grade v1.0
================================================================

2025-07-22 21:03:19,629 |     INFO | Production report saved to: production_mail_calls_system\reports\PRODUCTION_DEPLOYMENT_REPORT.txt
2025-07-22 21:03:19,630 |     INFO |
======================================================================
2025-07-22 21:03:19,632 |     INFO | PRODUCTION PIPELINE COMPLETED SUCCESSFULLY!
2025-07-22 21:03:19,633 |     INFO | Execution time: 0.4 minutes
2025-07-22 21:03:19,634 |     INFO | Best approach: standard
2025-07-22 21:03:19,635 |     INFO | Output directory: production_mail_calls_system
2025-07-22 21:03:19,637 |     INFO | ======================================================================

==================================================
PRODUCTION SYSTEM DEPLOYED SUCCESSFULLY!
==================================================

READY FOR PRODUCTION USE:
 Best Model: standard
 Mail Types: 15 processed
 Execution Time: 0.4 minutes

YOUR MAIL TYPES INTEGRATED:
   Cheque
   Scheduled PAYMENT CHECKS
   Proxy (US)
   DRP Stmt.
   Envision
  ... and 10 more

CAPABILITIES:
 Single day predictions from your mail data
 Multi-day campaign impact analysis
 5-day business forecasting
 Confidence intervals and business insights

 All files saved to: production_mail_calls_system
 See USAGE_GUIDE.txt for integration instructions
PS C:\Users\BhungarD\OneDrive - Computershare\Desktop\finprod> & C:/Users/BhungarD/python.exe "c:/Users/BhungarD/OneDrive - Computershare/Desktop/finprod/model.py"
================================================================================
OPTIMIZED MAIL-TO-CALLS PREDICTION SYSTEM
================================================================================
SYSTEMATIC OPTIMIZATION APPROACH:
  1. Analyze mail types by VOLUME and CORRELATION
  2. Find optimal tradeoff between volume and correlation
  3. Test comprehensive feature combinations
  4. Systematic model testing and selection
  5. Output the HIGHEST ACCURACY configuration

GOAL: Maximum prediction accuracy through systematic optimization
================================================================================

2025-07-22 21:13:25,077 | INFO | ================================================================================
2025-07-22 21:13:25,078 | INFO | OPTIMIZED MAIL-TO-CALLS PREDICTION SYSTEM
2025-07-22 21:13:25,078 | INFO | ================================================================================
2025-07-22 21:13:25,079 | INFO | STRATEGY: Systematic optimization of:
2025-07-22 21:13:25,080 | INFO |   1. Mail types (Volume vs Correlation tradeoff)
2025-07-22 21:13:25,081 | INFO |   2. Feature engineering combinations
2025-07-22 21:13:25,081 | INFO |   3. Model selection and tuning
2025-07-22 21:13:25,095 | INFO |   4. Find highest accuracy configuration
2025-07-22 21:13:25,096 | INFO | ================================================================================
2025-07-22 21:13:25,097 | INFO |
PHASE 1: DATA LOADING AND MAIL TYPE ANALYSIS
2025-07-22 21:13:25,098 | INFO | Loading call intent data...
2025-07-22 21:13:25,099 | INFO | Loading callintent.csv...
2025-07-22 21:13:34,791 | INFO | Loaded data/callintent.csv: 1053601 rows, 42 columns
2025-07-22 21:13:36,476 | INFO | Found 1053601 call records from 2025+
2025-07-22 21:13:37,431 | INFO | Created intent data for 40 intents
2025-07-22 21:13:37,634 | INFO | Loading mail data...
2025-07-22 21:13:37,635 | INFO | Loading mail.csv...
2025-07-22 21:13:38,769 | INFO | Loaded data/mail.csv: 1409780 rows, 4 columns
2025-07-22 21:13:39,147 | INFO | Processed 582365 mail records from 2025+
2025-07-22 21:13:39,377 | INFO | Created daily mail data: 138 days, 197 types
2025-07-22 21:13:39,386 | INFO | Starting comprehensive mail type analysis...
2025-07-22 21:13:39,434 | INFO | Analyzing 106 overlapping days with 197 mail types
2025-07-22 21:13:39,435 | INFO | Analyzing mail types by volume...
2025-07-22 21:13:39,486 | INFO | Top 5 by volume: ['Cheque', 'DRP Stmt.', 'Notice', 'Envision', 'Proxy (US)']
2025-07-22 21:13:39,486 | INFO | Analyzing mail types by correlation...
2025-07-22 21:13:40,424 | INFO | Top 5 by correlation: ['Repl_Chks', 'ACH_Debit_Enrollment', 'ESPP_Statements', 'ESPP_Terminations', 'General Comm']
2025-07-22 21:13:40,425 | INFO | Best correlations: [('Repl_Chks', np.float64(0.551)), ('ACH_Debit_Enrollment', np.float64(0.522)), ('ESPP_Statements', np.float64(0.507)), ('ESPP_Terminations', np.float64(-0.505)), ('General Comm', np.float64(0.491))]
2025-07-22 21:13:40,425 | INFO | Finding optimal tradeoff between volume and correlation...
2025-07-22 21:13:40,429 | INFO | volume_heavy top 5: ['DRS_Advices', 'ACH Conf.', 'DRP Stmt.', 'Cheque', 'DRS']
2025-07-22 21:13:40,429 | INFO | balanced top 5: ['Repl_Chks', 'DRS_Advices', 'ACH Conf.', 'Transfer', 'Rep_1099Div']
2025-07-22 21:13:40,430 | INFO | correlation_heavy top 5: ['Repl_Chks', 'Transfer', 'Rep_1099Div', 'ACH Conf.', 'DRS_Advices']
2025-07-22 21:13:40,431 | INFO | correlation_only top 5: ['Repl_Chks', 'ACH_Debit_Enrollment', 'ESPP_Statements', 'ESPP_Terminations', 'General Comm']       
2025-07-22 21:13:40,431 | INFO | volume_only top 5: ['Cheque', 'DRP Stmt.', 'Notice', 'Envision', 'Proxy (US)']
2025-07-22 21:13:40,432 | INFO |
PHASE 2: SYSTEMATIC MODEL OPTIMIZATION
2025-07-22 21:13:40,433 | INFO | === STARTING SYSTEMATIC OPTIMIZATION ===
2025-07-22 21:13:40,441 | INFO | Optimization dataset: 82 samples
2025-07-22 21:13:40,443 | INFO | Testing 480 configurations...
2025-07-22 21:13:40,444 | INFO | Limiting to 200 configurations for efficiency
2025-07-22 21:13:40,595 | INFO | New best configuration: volume_only_minimal_extended_lags_elastic
2025-07-22 21:13:40,596 | INFO |   Score (cv_r2): -1678705.664
2025-07-22 21:13:40,596 | INFO |   Mail strategy: volume_only
2025-07-22 21:13:40,597 | INFO |   Features: 83
2025-07-22 21:13:40,597 | INFO |   Mail types: ['Cheque', 'DRP Stmt.', 'Notice']...
2025-07-22 21:13:40,652 | ERROR | Configuration volume_heavy_standard_focused_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:40,753 | ERROR | Configuration volume_only_comprehensive_extended_lags_linear failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:41,006 | ERROR | Configuration volume_heavy_advanced_extended_lags_lasso failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:41,256 | ERROR | Configuration volume_heavy_advanced_comprehensive_lags_ridge failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:41,328 | ERROR | Configuration correlation_only_comprehensive_basic_lags_random_forest failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:41,433 | ERROR | Configuration correlation_heavy_comprehensive_extended_lags_random_forest failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:41,511 | ERROR | Configuration balanced_comprehensive_comprehensive_lags_lasso failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:44,041 | INFO | New best configuration: volume_only_minimal_extended_lags_random_forest
2025-07-22 21:13:44,042 | INFO |   Score (cv_r2): -1.380
2025-07-22 21:13:44,043 | INFO |   Mail strategy: volume_only
2025-07-22 21:13:44,044 | INFO |   Features: 83
2025-07-22 21:13:44,045 | INFO |   Mail types: ['Cheque', 'DRP Stmt.', 'Notice']...
2025-07-22 21:13:44,234 | ERROR | Configuration balanced_comprehensive_comprehensive_lags_elastic failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:47,681 | ERROR | Configuration volume_only_standard_extended_lags_elastic failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:50,234 | ERROR | Configuration balanced_standard_extended_lags_lasso failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:52,641 | INFO | New best configuration: correlation_heavy_minimal_extended_lags_random_forest
2025-07-22 21:13:52,642 | INFO |   Score (cv_r2): -0.954
2025-07-22 21:13:52,642 | INFO |   Mail strategy: correlation_heavy
2025-07-22 21:13:52,642 | INFO |   Features: 83
2025-07-22 21:13:52,643 | INFO |   Mail types: ['Repl_Chks', 'Transfer', 'Rep_1099Div']...
2025-07-22 21:13:52,707 | ERROR | Configuration volume_heavy_comprehensive_basic_lags_ridge failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:52,850 | ERROR | Configuration volume_heavy_advanced_extended_lags_random_forest failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:52,910 | ERROR | Configuration correlation_only_comprehensive_extended_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:52,911 | INFO | Progress: 20/200 configurations tested
2025-07-22 21:13:52,942 | ERROR | Configuration correlation_only_standard_focused_lags_lasso failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:52,981 | ERROR | Configuration correlation_heavy_standard_focused_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:53,094 | ERROR | Configuration volume_only_advanced_focused_lags_elastic failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:56,420 | ERROR | Configuration correlation_only_comprehensive_basic_lags_linear failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:56,575 | ERROR | Configuration correlation_only_advanced_comprehensive_lags_elastic failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:56,633 | ERROR | Configuration correlation_only_standard_comprehensive_lags_ridge failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:56,763 | ERROR | Configuration correlation_only_advanced_focused_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:56,920 | ERROR | Configuration volume_heavy_standard_comprehensive_lags_elastic failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:57,121 | ERROR | Configuration volume_only_advanced_focused_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:57,179 | ERROR | Configuration volume_heavy_standard_comprehensive_lags_ridge failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:57,327 | ERROR | Configuration volume_heavy_advanced_extended_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:57,387 | ERROR | Configuration balanced_standard_comprehensive_lags_linear failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:57,425 | ERROR | Configuration volume_only_standard_basic_lags_random_forest failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:57,598 | ERROR | Configuration volume_heavy_advanced_basic_lags_random_forest failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:57,849 | ERROR | Configuration correlation_heavy_advanced_extended_lags_lasso failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:57,959 | ERROR | Configuration correlation_heavy_comprehensive_extended_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:58,026 | ERROR | Configuration correlation_heavy_standard_basic_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:58,115 | ERROR | Configuration balanced_comprehensive_basic_lags_gradient_boost failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:58,116 | INFO | Progress: 40/200 configurations tested
2025-07-22 21:13:58,214 | ERROR | Configuration correlation_heavy_standard_extended_lags_ridge failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:58,280 | ERROR | Configuration correlation_only_standard_comprehensive_lags_random_forest failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:58,492 | ERROR | Configuration volume_heavy_advanced_extended_lags_ridge failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:58,550 | ERROR | Configuration correlation_only_standard_basic_lags_lasso failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:58,774 | ERROR | Configuration balanced_advanced_basic_lags_ridge failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:58,998 | ERROR | Configuration volume_heavy_comprehensive_basic_lags_lasso failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:59,049 | ERROR | Configuration balanced_standard_comprehensive_lags_ridge failed: 'DatetimeIndex' object has no attribute 'week'
2025-07-22 21:13:59,244 | ERROR | Configuration correlation_heavy_standard_extended_lags_lasso failed: 'DatetimeIndex' object has no attribute 'week'

Optimization interrupted by user



#!/usr/bin/env python
"""
OPTIMIZED MAIL-TO-CALLS PREDICTION SYSTEM
=========================================

OPTIMIZATION STRATEGY:
1. Analyze mail types by VOLUME (top 25)
2. Analyze mail types by CORRELATION (top 25) 
3. Find optimal tradeoff between volume and correlation
4. Test different feature combinations systematically
5. Output the highest accuracy model

SYSTEMATIC APPROACH:
- Mail type selection: Volume vs Correlation analysis
- Feature engineering: Comprehensive feature testing
- Model optimization: Find best performing combination
- Production deployment: Bulletproof prediction system
"""

import warnings
warnings.filterwarnings('ignore')

import os
import sys
from pathlib import Path
import json
import logging
import traceback
from datetime import datetime, timedelta
import time
from typing import Dict, List, Tuple, Optional, Union
from collections import defaultdict
import itertools

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ML Libraries
from sklearn.model_selection import TimeSeriesSplit, cross_validate
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import joblib

# Statistical Libraries
from scipy import stats
from scipy.stats import pearsonr, spearmanr

# ============================================================================
# OPTIMIZATION CONFIGURATION
# ============================================================================

CONFIG = {
    # Data files
    "call_intent_file": "callintent.csv",
    "mail_file": "mail.csv",
    
    # Mail type analysis
    "top_n_by_volume": 25,
    "top_n_by_correlation": 25,
    "min_correlation_threshold": 0.01,
    "correlation_lags_to_test": [0, 1, 2, 3, 4, 5],
    
    # Feature engineering options
    "feature_combinations": {
        "minimal": ["basic_lags", "temporal_basic"],
        "standard": ["basic_lags", "rolling", "temporal_basic", "temporal_advanced"],
        "comprehensive": ["basic_lags", "rolling", "weighted", "temporal_basic", "temporal_advanced", "interaction"],
        "advanced": ["basic_lags", "rolling", "weighted", "temporal_basic", "temporal_advanced", "interaction", "trend"]
    },
    
    # Lag configurations
    "lag_options": {
        "basic_lags": [1, 2, 3],
        "extended_lags": [1, 2, 3, 4, 5],
        "focused_lags": [1, 2],
        "comprehensive_lags": [1, 2, 3, 4, 5, 6, 7]
    },
    
    # Model options
    "models_to_test": {
        "linear": LinearRegression(),
        "ridge": Ridge(alpha=10.0, random_state=42),
        "lasso": Lasso(alpha=1.0, random_state=42, max_iter=2000),
        "elastic": ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42, max_iter=2000),
        "random_forest": RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42),
        "gradient_boost": GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42)
    },
    
    # Optimization settings
    "cv_folds": 5,
    "test_split": 0.2,
    "optimization_metric": "cv_r2",  # or "cv_mae"
    "min_samples_required": 30,
    
    # Output
    "output_dir": "optimized_mail_calls_system",
    "save_analysis": True,
    "save_best_model": True,
    "random_state": 42
}

# ============================================================================
# SAFE LOGGING AND UTILITIES
# ============================================================================

def setup_logging():
    """Setup safe logging"""
    output_dir = Path(CONFIG["output_dir"])
    output_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(output_dir / "optimization.log", mode='w', encoding='utf-8')
        ]
    )
    return logging.getLogger(__name__)

LOG = setup_logging()

def safe_print(message: str):
    """Print safely without encoding issues"""
    try:
        clean_msg = str(message).encode('ascii', 'ignore').decode('ascii')
        print(clean_msg)
    except:
        print(str(message))

# ============================================================================
# DATA LOADER AND ANALYZER
# ============================================================================

class OptimizationDataLoader:
    """Load data and perform comprehensive mail type analysis"""
    
    def __init__(self):
        self.call_data = None
        self.mail_data = None
        self.intent_data = None
        self.mail_analysis = {}
        self.best_mail_types = []
        
    def load_data_robust(self, filename: str) -> pd.DataFrame:
        """Robust data loading"""
        LOG.info(f"Loading {filename}...")
        
        paths = [filename, f"data/{filename}", f"data\\{filename}"]
        strategies = [
            {'encoding': 'utf-8', 'sep': ','},
            {'encoding': 'utf-8', 'sep': ';'},
            {'encoding': 'latin1', 'sep': ','}
        ]
        
        for path in paths:
            if not Path(path).exists():
                continue
            for strategy in strategies:
                try:
                    df = pd.read_csv(path, low_memory=False, **strategy)
                    if len(df) > 0:
                        LOG.info(f"Loaded {path}: {len(df)} rows, {df.shape[1]} columns")
                        return df
                except:
                    continue
        
        raise FileNotFoundError(f"Could not load {filename}")
    
    def detect_columns(self, df: pd.DataFrame) -> Dict[str, str]:
        """Smart column detection"""
        columns = {}
        df_cols = [str(col).lower() for col in df.columns]
        
        # Date column
        for i, col in enumerate(df_cols):
            if any(kw in col for kw in ['date', 'time', 'start']):
                try:
                    sample = df.iloc[:100, i].dropna()
                    parsed = pd.to_datetime(sample, errors='coerce')
                    if parsed.notna().sum() / len(sample) > 0.7:
                        columns['date'] = df.columns[i]
                        break
                except:
                    continue
        
        # Volume column
        for i, col in enumerate(df_cols):
            if any(kw in col for kw in ['volume', 'count', 'amount']):
                if df.iloc[:, i].dtype in ['int64', 'float64']:
                    columns['volume'] = df.columns[i]
                    break
        
        # Type column
        for i, col in enumerate(df_cols):
            if any(kw in col for kw in ['type', 'category']):
                unique_count = df.iloc[:, i].nunique()
                if 2 <= unique_count <= 500:
                    columns['type'] = df.columns[i]
                    break
        
        # Intent column
        for i, col in enumerate(df_cols):
            if 'intent' in col:
                columns['intent'] = df.columns[i]
                break
        
        return columns
    
    def load_call_data(self) -> Tuple[pd.Series, pd.DataFrame]:
        """Load and process call data"""
        LOG.info("Loading call intent data...")
        
        df = self.load_data_robust(CONFIG["call_intent_file"])
        columns = self.detect_columns(df)
        
        # Process dates
        date_col = columns['date']
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df = df.dropna(subset=[date_col])
        df = df[df[date_col].dt.year >= 2025]
        
        LOG.info(f"Found {len(df)} call records from 2025+")
        
        # Daily call volumes
        df['call_date'] = df[date_col].dt.date
        daily_calls = df.groupby('call_date').size()
        daily_calls.index = pd.to_datetime(daily_calls.index)
        daily_calls = daily_calls.sort_index()
        
        # Intent data
        daily_intents = None
        if 'intent' in columns:
            intent_col = columns['intent']
            df[intent_col] = df[intent_col].fillna('Unknown').astype(str)
            
            # Keep common intents
            intent_counts = df[intent_col].value_counts()
            common_intents = intent_counts[intent_counts >= 10].index
            df_filtered = df[df[intent_col].isin(common_intents)]
            
            if len(df_filtered) > 0:
                intent_pivot = df_filtered.groupby(['call_date', intent_col]).size().unstack(fill_value=0)
                intent_pivot.index = pd.to_datetime(intent_pivot.index)
                daily_intents = intent_pivot.div(intent_pivot.sum(axis=1), axis=0).fillna(0)
                LOG.info(f"Created intent data for {len(daily_intents.columns)} intents")
        
        self.call_data = daily_calls
        self.intent_data = daily_intents
        return daily_calls, daily_intents
    
    def load_mail_data(self) -> pd.DataFrame:
        """Load and process mail data"""
        LOG.info("Loading mail data...")
        
        df = self.load_data_robust(CONFIG["mail_file"])
        columns = self.detect_columns(df)
        
        if 'date' not in columns or 'volume' not in columns:
            raise ValueError("Required mail columns not found")
        
        date_col = columns['date']
        volume_col = columns['volume']
        type_col = columns.get('type')
        
        # Process data
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df = df.dropna(subset=[date_col])
        df = df[df[date_col].dt.year >= 2025]
        
        df[volume_col] = pd.to_numeric(df[volume_col], errors='coerce')
        df = df.dropna(subset=[volume_col])
        df = df[df[volume_col] >= 0]
        
        LOG.info(f"Processed {len(df)} mail records from 2025+")
        
        # Create daily mail data
        df['mail_date'] = df[date_col].dt.date
        
        if type_col:
            df[type_col] = df[type_col].astype(str)
            mail_daily = df.groupby(['mail_date', type_col])[volume_col].sum().unstack(fill_value=0)
        else:
            mail_daily = df.groupby('mail_date')[volume_col].sum().to_frame('total_mail')
        
        mail_daily.index = pd.to_datetime(mail_daily.index)
        mail_daily = mail_daily.sort_index()
        
        LOG.info(f"Created daily mail data: {len(mail_daily)} days, {len(mail_daily.columns)} types")
        
        self.mail_data = mail_daily
        return mail_daily
    
    def analyze_mail_types_comprehensive(self) -> Dict:
        """Comprehensive analysis of mail types by volume and correlation"""
        
        LOG.info("Starting comprehensive mail type analysis...")
        
        if self.call_data is None or self.mail_data is None:
            raise ValueError("Call and mail data must be loaded first")
        
        # Find overlapping dates
        common_dates = self.call_data.index.intersection(self.mail_data.index)
        if len(common_dates) < 20:
            raise ValueError(f"Insufficient overlapping data: {len(common_dates)} days")
        
        aligned_calls = self.call_data.loc[common_dates]
        aligned_mail = self.mail_data.loc[common_dates]
        
        LOG.info(f"Analyzing {len(common_dates)} overlapping days with {len(aligned_mail.columns)} mail types")
        
        analysis = {}
        
        # 1. VOLUME ANALYSIS
        LOG.info("Analyzing mail types by volume...")
        volume_analysis = {}
        
        for mail_type in aligned_mail.columns:
            total_volume = aligned_mail[mail_type].sum()
            avg_daily_volume = aligned_mail[mail_type].mean()
            days_active = (aligned_mail[mail_type] > 0).sum()
            volume_std = aligned_mail[mail_type].std()
            
            volume_analysis[mail_type] = {
                'total_volume': total_volume,
                'avg_daily_volume': avg_daily_volume,
                'days_active': days_active,
                'activity_ratio': days_active / len(common_dates),
                'volume_std': volume_std,
                'volume_cv': volume_std / (avg_daily_volume + 1e-10),  # Coefficient of variation
                'volume_score': total_volume * (days_active / len(common_dates))  # Volume weighted by consistency
            }
        
        # Top mail types by volume
        volume_rankings = sorted(volume_analysis.items(), key=lambda x: x[1]['volume_score'], reverse=True)
        top_by_volume = [item[0] for item in volume_rankings[:CONFIG["top_n_by_volume"]]]
        
        analysis['volume_analysis'] = volume_analysis
        analysis['top_by_volume'] = top_by_volume
        
        LOG.info(f"Top 5 by volume: {top_by_volume[:5]}")
        
        # 2. CORRELATION ANALYSIS
        LOG.info("Analyzing mail types by correlation...")
        correlation_analysis = {}
        
        for mail_type in aligned_mail.columns:
            mail_series = aligned_mail[mail_type]
            
            # Test different lags
            best_correlation = 0
            best_lag = 0
            lag_correlations = {}
            
            for lag in CONFIG["correlation_lags_to_test"]:
                try:
                    if lag == 0:
                        corr, p_value = pearsonr(mail_series, aligned_calls)
                    else:
                        # Lag the calls (positive lag means mail leads calls)
                        lagged_calls = aligned_calls.shift(-lag).dropna()
                        if len(lagged_calls) > 10:
                            mail_subset = mail_series.loc[lagged_calls.index]
                            corr, p_value = pearsonr(mail_subset, lagged_calls)
                        else:
                            corr, p_value = 0, 1
                    
                    if not np.isnan(corr):
                        lag_correlations[lag] = {'correlation': corr, 'p_value': p_value}
                        
                        if abs(corr) > abs(best_correlation):
                            best_correlation = corr
                            best_lag = lag
                            
                except Exception as e:
                    lag_correlations[lag] = {'correlation': 0, 'p_value': 1}
            
            correlation_analysis[mail_type] = {
                'best_correlation': best_correlation,
                'best_lag': best_lag,
                'lag_correlations': lag_correlations,
                'abs_correlation': abs(best_correlation),
                'correlation_strength': 'Strong' if abs(best_correlation) > 0.3 else 
                                      'Moderate' if abs(best_correlation) > 0.1 else 'Weak'
            }
        
        # Top mail types by correlation
        correlation_rankings = sorted(
            correlation_analysis.items(), 
            key=lambda x: x[1]['abs_correlation'], 
            reverse=True
        )
        
        # Filter by minimum threshold
        significant_correlations = [
            (mail_type, data) for mail_type, data in correlation_rankings 
            if data['abs_correlation'] >= CONFIG["min_correlation_threshold"]
        ]
        
        top_by_correlation = [item[0] for item in significant_correlations[:CONFIG["top_n_by_correlation"]]]
        
        analysis['correlation_analysis'] = correlation_analysis
        analysis['top_by_correlation'] = top_by_correlation
        
        LOG.info(f"Top 5 by correlation: {top_by_correlation[:5]}")
        LOG.info(f"Best correlations: {[(t, round(correlation_analysis[t]['best_correlation'], 3)) for t in top_by_correlation[:5]]}")
        
        # 3. COMBINED ANALYSIS - FIND OPTIMAL TRADEOFF
        LOG.info("Finding optimal tradeoff between volume and correlation...")
        
        combined_analysis = {}
        
        for mail_type in aligned_mail.columns:
            volume_data = volume_analysis[mail_type]
            corr_data = correlation_analysis[mail_type]
            
            # Normalize scores to 0-1 range
            volume_rank = next(i for i, (name, _) in enumerate(volume_rankings) if name == mail_type) + 1
            corr_rank = next(i for i, (name, _) in enumerate(correlation_rankings) if name == mail_type) + 1
            
            volume_score_norm = 1 - (volume_rank - 1) / len(volume_rankings)
            correlation_score_norm = 1 - (corr_rank - 1) / len(correlation_rankings)
            
            # Test different weighting schemes
            weighting_schemes = {
                'volume_heavy': 0.8 * volume_score_norm + 0.2 * correlation_score_norm,
                'balanced': 0.5 * volume_score_norm + 0.5 * correlation_score_norm,
                'correlation_heavy': 0.2 * volume_score_norm + 0.8 * correlation_score_norm,
                'correlation_only': correlation_score_norm,
                'volume_only': volume_score_norm
            }
            
            combined_analysis[mail_type] = {
                'volume_rank': volume_rank,
                'correlation_rank': corr_rank,
                'volume_score_norm': volume_score_norm,
                'correlation_score_norm': correlation_score_norm,
                'weighting_schemes': weighting_schemes,
                'volume_raw': volume_data['total_volume'],
                'correlation_raw': corr_data['best_correlation'],
                'best_lag': corr_data['best_lag']
            }
        
        analysis['combined_analysis'] = combined_analysis
        
        # Create different mail type selections for testing
        mail_type_selections = {}
        
        for scheme_name, _ in weighting_schemes.items():
            ranked_by_scheme = sorted(
                combined_analysis.items(),
                key=lambda x: x[1]['weighting_schemes'][scheme_name],
                reverse=True
            )
            mail_type_selections[scheme_name] = [item[0] for item in ranked_by_scheme[:15]]  # Top 15
        
        analysis['mail_type_selections'] = mail_type_selections
        
        # Log the different selections
        for scheme, types in mail_type_selections.items():
            LOG.info(f"{scheme} top 5: {types[:5]}")
        
        self.mail_analysis = analysis
        return analysis
    
    def get_aligned_data(self) -> Dict:
        """Get aligned call and mail data"""
        common_dates = self.call_data.index.intersection(self.mail_data.index)
        
        # Filter to business days
        business_dates = [d for d in common_dates if d.weekday() < 5]
        
        return {
            'calls': self.call_data.loc[business_dates],
            'mail': self.mail_data.loc[business_dates],
            'intents': self.intent_data.loc[business_dates] if self.intent_data is not None else None,
            'dates': business_dates
        }

# ============================================================================
# FEATURE ENGINEERING ENGINE
# ============================================================================

class AdvancedFeatureEngine:
    """Advanced feature engineering with systematic testing"""
    
    def __init__(self):
        self.feature_sets = {}
        self.feature_importance = {}
        
    def create_mail_features(self, mail_data: pd.DataFrame, mail_types: List[str], 
                           feature_config: List[str], lag_config: List[int]) -> pd.DataFrame:
        """Create mail-based features"""
        
        features = pd.DataFrame(index=mail_data.index)
        
        # Select mail types
        available_types = [t for t in mail_types if t in mail_data.columns]
        
        for mail_type in available_types:
            clean_name = str(mail_type).replace(' ', '').replace('-', '').replace('_', '')[:10]
            mail_series = mail_data[mail_type]
            
            # Basic lag features
            if "basic_lags" in feature_config:
                for lag in lag_config:
                    if lag == 0:
                        features[f"{clean_name}_today"] = mail_series
                    else:
                        features[f"{clean_name}_lag{lag}"] = mail_series.shift(lag)
            
            # Rolling features
            if "rolling" in feature_config:
                for window in [3, 7]:
                    if window <= len(mail_data):
                        features[f"{clean_name}_avg{window}"] = mail_series.rolling(window, min_periods=1).mean()
                        features[f"{clean_name}_sum{window}"] = mail_series.rolling(window, min_periods=1).sum()
                        if window >= 3:
                            features[f"{clean_name}_std{window}"] = mail_series.rolling(window, min_periods=1).std()
            
            # Weighted features
            if "weighted" in feature_config:
                weights = {1: 0.4, 2: 0.3, 3: 0.2, 4: 0.1, 5: 0.05}
                weighted_sum = pd.Series(0.0, index=mail_series.index)
                
                for lag in lag_config:
                    weight = weights.get(lag, 0.05)
                    if lag == 0:
                        weighted_sum += mail_series * weight
                    else:
                        weighted_sum += mail_series.shift(lag).fillna(0) * weight
                
                features[f"{clean_name}_weighted"] = weighted_sum
            
            # Trend features
            if "trend" in feature_config and len(mail_data) >= 7:
                trend = mail_series.rolling(7, min_periods=3).apply(
                    lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 2 else 0
                )
                features[f"{clean_name}_trend"] = trend
        
        # Aggregate features
        total_mail = mail_data[available_types].sum(axis=1)
        
        for lag in lag_config:
            if lag == 0:
                features['total_mail_today'] = total_mail
            else:
                features[f'total_mail_lag{lag}'] = total_mail.shift(lag)
        
        if "rolling" in feature_config:
            features['total_mail_avg7'] = total_mail.rolling(7, min_periods=1).mean()
            features['total_mail_std7'] = total_mail.rolling(7, min_periods=1).std()
        
        # Interaction features
        if "interaction" in feature_config and len(available_types) >= 2:
            # Top 2 mail types interaction
            top_types = available_types[:2]
            if len(top_types) == 2:
                type1, type2 = top_types
                features['interaction_top2'] = mail_data[type1] * mail_data[type2]
                features['ratio_top2'] = (mail_data[type1] + 1) / (mail_data[type2] + 1)
        
        # Fill NaN values
        features = features.fillna(0)
        
        return features
    
    def create_temporal_features(self, dates: pd.DatetimeIndex, feature_config: List[str]) -> pd.DataFrame:
        """Create temporal features"""
        
        features = pd.DataFrame(index=dates)
        
        # Basic temporal
        if "temporal_basic" in feature_config:
            features['weekday'] = dates.weekday
            features['month'] = dates.month
            features['quarter'] = dates.quarter
            features['day_of_month'] = dates.day
            features['is_weekend'] = (dates.weekday >= 5).astype(int)
        
        # Advanced temporal
        if "temporal_advanced" in feature_config:
            # Business calendar
            features['is_month_start'] = (dates.day <= 5).astype(int)
            features['is_month_end'] = (dates.day >= 25).astype(int)
            features['is_quarter_end'] = dates.to_series().apply(
                lambda x: 1 if x.month in [3, 6, 9, 12] and x.day >= 25 else 0
            ).values
            
            # Cyclical encoding
            features['weekday_sin'] = np.sin(2 * np.pi * features['weekday'] / 7)
            features['weekday_cos'] = np.cos(2 * np.pi * features['weekday'] / 7)
            features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)
            features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)
            
            # Week of year
            features['week_of_year'] = dates.week
            features['week_sin'] = np.sin(2 * np.pi * features['week_of_year'] / 52)
            features['week_cos'] = np.cos(2 * np.pi * features['week_of_year'] / 52)
        
        return features
    
    def create_call_history_features(self, call_data: pd.Series) -> pd.DataFrame:
        """Create call history features"""
        
        features = pd.DataFrame(index=call_data.index)
        
        # Basic lags
        for lag in [1, 2, 3, 7]:
            features[f'calls_lag{lag}'] = call_data.shift(lag)
        
        # Rolling statistics
        for window in [3, 7, 14, 30]:
            if len(call_data) >= window:
                features[f'calls_mean{window}'] = call_data.rolling(window, min_periods=1).mean()
                features[f'calls_std{window}'] = call_data.rolling(window, min_periods=1).std()
                features[f'calls_max{window}'] = call_data.rolling(window, min_periods=1).max()
                features[f'calls_min{window}'] = call_data.rolling(window, min_periods=1).min()
        
        # Trend and momentum
        if len(call_data) >= 7:
            features['calls_trend7'] = call_data.rolling(7, min_periods=3).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 2 else 0
            )
            
            # Momentum (rate of change)
            features['calls_momentum3'] = call_data.pct_change(3)
            features['calls_momentum7'] = call_data.pct_change(7)
        
        # Fill NaN with reasonable defaults
        features = features.fillna(method='ffill').fillna(call_data.mean())
        
        return features
    
    def create_feature_set(self, aligned_data: Dict, mail_types: List[str], 
                          feature_config: List[str], lag_config: List[int]) -> Tuple[pd.DataFrame, pd.Series]:
        """Create complete feature set"""
        
        calls = aligned_data['calls']
        mail = aligned_data['mail']
        
        # Target: next day calls
        y = calls.shift(-1).dropna()
        target_dates = y.index
        
        feature_components = []
        
        # 1. Mail features
        if mail is not None and len(mail_types) > 0:
            mail_features = self.create_mail_features(mail, mail_types, feature_config, lag_config)
            mail_features = mail_features.reindex(target_dates, fill_value=0)
            feature_components.append(mail_features)
        
        # 2. Temporal features
        temporal_features = self.create_temporal_features(target_dates, feature_config)
        feature_components.append(temporal_features)
        
        # 3. Call history features
        call_features = self.create_call_history_features(calls)
        call_features = call_features.reindex(target_dates, fill_value=0)
        feature_components.append(call_features)
        
        # Combine all features
        X = pd.concat(feature_components, axis=1)
        X = X.fillna(0)
        
        return X, y

# ============================================================================
# SYSTEMATIC MODEL OPTIMIZER
# ============================================================================

class SystematicModelOptimizer:
    """Systematic optimization engine"""
    
    def __init__(self, data_loader: OptimizationDataLoader):
        self.data_loader = data_loader
        self.feature_engine = AdvancedFeatureEngine()
        self.optimization_results = {}
        self.best_configuration = None
        self.best_model = None
        self.best_score = -float('inf')
        
    def evaluate_model_configuration(self, model, X: pd.DataFrame, y: pd.Series, config_name: str) -> Dict:
        """Evaluate a single model configuration"""
        
        if len(X) < CONFIG["min_samples_required"]:
            return {"error": "insufficient_samples", "samples": len(X)}
        
        try:
            results = {"config_name": config_name, "features": len(X.columns), "samples": len(X)}
            
            # Cross-validation
            n_splits = min(CONFIG["cv_folds"], len(X) // 10, 5)
            tscv = TimeSeriesSplit(n_splits=max(2, n_splits))
            
            cv_results = cross_validate(
                model, X, y, cv=tscv,
                scoring=['neg_mean_absolute_error', 'r2'],
                return_train_score=False
            )
            
            results['cv_mae'] = -cv_results['test_neg_mean_absolute_error'].mean()
            results['cv_mae_std'] = cv_results['test_neg_mean_absolute_error'].std()
            results['cv_r2'] = cv_results['test_r2'].mean()
            results['cv_r2_std'] = cv_results['test_r2'].std()
            
            # Holdout test
            split_idx = int(len(X) * (1 - CONFIG["test_split"]))
            X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
            y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
            
            if len(X_test) >= 3:  # Minimum test set
                model.fit(X_train, y_train)
                test_pred = model.predict(X_test)
                
                results['test_mae'] = mean_absolute_error(y_test, test_pred)
                results['test_r2'] = r2_score(y_test, test_pred)
                results['test_mape'] = np.mean(np.abs((y_test - test_pred) / (y_test + 1e-10))) * 100
                
                # Final model with all data
                model.fit(X, y)
                results['model'] = model
                
                # Feature importance if available
                if hasattr(model, 'feature_importances_'):
                    importance = dict(zip(X.columns, model.feature_importances_))
                    results['feature_importance'] = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15]
                elif hasattr(model, 'coef_'):
                    importance = dict(zip(X.columns, np.abs(model.coef_)))
                    results['feature_importance'] = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15]
            else:
                results['test_mae'] = float('inf')
                results['test_r2'] = -float('inf')
            
            return results
            
        except Exception as e:
            return {"error": str(e), "config_name": config_name}
    
    def run_systematic_optimization(self) -> Dict:
        """Run systematic optimization across all combinations"""
        
        LOG.info("=== STARTING SYSTEMATIC OPTIMIZATION ===")
        
        # Get aligned data
        aligned_data = self.data_loader.get_aligned_data()
        mail_analysis = self.data_loader.mail_analysis
        
        LOG.info(f"Optimization dataset: {len(aligned_data['calls'])} samples")
        
        # Test configurations
        configurations_to_test = []
        
        # 1. Mail type selection strategies
        mail_type_strategies = mail_analysis['mail_type_selections']
        
        # 2. Feature combinations
        feature_combinations = CONFIG["feature_combinations"]
        
        # 3. Lag configurations
        lag_configurations = CONFIG["lag_options"]
        
        # 4. Models
        models = CONFIG["models_to_test"]
        
        # Create all combinations (but limit to reasonable number)
        test_combinations = []
        
        for mail_strategy, mail_types in mail_type_strategies.items():
            for feature_name, feature_config in feature_combinations.items():
                for lag_name, lag_config in lag_configurations.items():
                    for model_name, model in models.items():
                        
                        config = {
                            'mail_strategy': mail_strategy,
                            'mail_types': mail_types[:10],  # Limit to top 10
                            'feature_config': feature_config,
                            'lag_config': lag_config,
                            'model_name': model_name,
                            'model': model,
                            'config_id': f"{mail_strategy}_{feature_name}_{lag_name}_{model_name}"
                        }
                        test_combinations.append(config)
        
        LOG.info(f"Testing {len(test_combinations)} configurations...")
        
        # Limit combinations to avoid excessive computation
        if len(test_combinations) > 200:
            LOG.info("Limiting to 200 configurations for efficiency")
            
            # Prioritize certain combinations
            priority_configs = []
            other_configs = []
            
            for config in test_combinations:
                if (config['mail_strategy'] in ['balanced', 'correlation_heavy'] and 
                    config['feature_config'] in ['standard', 'comprehensive'] and
                    config['model_name'] in ['ridge', 'random_forest']):
                    priority_configs.append(config)
                else:
                    other_configs.append(config)
            
            # Take top priority + random sample of others
            np.random.shuffle(other_configs)
            test_combinations = priority_configs + other_configs[:200-len(priority_configs)]
        
        # Run optimization
        all_results = {}
        progress_counter = 0
        
        for config in test_combinations:
            progress_counter += 1
            if progress_counter % 20 == 0:
                LOG.info(f"Progress: {progress_counter}/{len(test_combinations)} configurations tested")
            
            try:
                # Create feature set
                X, y = self.feature_engine.create_feature_set(
                    aligned_data,
                    config['mail_types'],
                    config['feature_config'],
                    config['lag_config']
                )
                
                # Evaluate model
                results = self.evaluate_model_configuration(
                    config['model'],
                    X, y,
                    config['config_id']
                )
                
                # Store configuration details
                results['configuration'] = {
                    'mail_strategy': config['mail_strategy'],
                    'mail_types': config['mail_types'],
                    'feature_config': config['feature_config'],
                    'lag_config': config['lag_config'],
                    'model_name': config['model_name']
                }
                
                all_results[config['config_id']] = results
                
                # Track best configuration
                if "error" not in results:
                    optimization_score = results.get(CONFIG["optimization_metric"], -float('inf'))
                    
                    if optimization_score > self.best_score:
                        self.best_score = optimization_score
                        self.best_configuration = config
                        self.best_model = results.get('model')
                        
                        LOG.info(f"New best configuration: {config['config_id']}")
                        LOG.info(f"  Score ({CONFIG['optimization_metric']}): {optimization_score:.3f}")
                        LOG.info(f"  Mail strategy: {config['mail_strategy']}")
                        LOG.info(f"  Features: {results['features']}")
                        LOG.info(f"  Mail types: {config['mail_types'][:3]}...")
                
            except Exception as e:
                LOG.error(f"Configuration {config['config_id']} failed: {str(e)}")
                all_results[config['config_id']] = {"error": str(e), "configuration": config}
        
        self.optimization_results = all_results
        
        # Log final results
        if self.best_configuration:
            LOG.info(f"\n=== OPTIMIZATION COMPLETE ===")
            LOG.info(f"Best configuration: {self.best_configuration['config_id']}")
            LOG.info(f"Best score: {self.best_score:.3f}")
            LOG.info(f"Total configurations tested: {len(test_combinations)}")
            LOG.info(f"Successful configurations: {len([r for r in all_results.values() if 'error' not in r])}")
        else:
            LOG.warning("No successful configurations found!")
        
        return all_results
    
    def get_top_configurations(self, top_n: int = 10) -> List[Dict]:
        """Get top N configurations"""
        
        successful_configs = []
        
        for config_id, results in self.optimization_results.items():
            if "error" not in results and CONFIG["optimization_metric"] in results:
                score = results[CONFIG["optimization_metric"]]
                successful_configs.append({
                    'config_id': config_id,
                    'score': score,
                    'results': results
                })
        
        # Sort by score
        successful_configs.sort(key=lambda x: x['score'], reverse=True)
        
        return successful_configs[:top_n]
    
    def analyze_optimization_insights(self) -> Dict:
        """Analyze insights from optimization results"""
        
        insights = {}
        
        # Get successful results
        successful = [r for r in self.optimization_results.values() if "error" not in r]
        
        if len(successful) == 0:
            return {"error": "No successful configurations to analyze"}
        
        # Mail strategy effectiveness
        strategy_scores = defaultdict(list)
        for result in successful:
            if 'configuration' in result:
                strategy = result['configuration']['mail_strategy']
                score = result.get(CONFIG["optimization_metric"], 0)
                strategy_scores[strategy].append(score)
        
        strategy_analysis = {}
        for strategy, scores in strategy_scores.items():
            strategy_analysis[strategy] = {
                'avg_score': np.mean(scores),
                'best_score': max(scores),
                'count': len(scores)
            }
        
        insights['mail_strategy_analysis'] = strategy_analysis
        
        # Model effectiveness
        model_scores = defaultdict(list)
        for result in successful:
            if 'configuration' in result:
                model = result['configuration']['model_name']
                score = result.get(CONFIG["optimization_metric"], 0)
                model_scores[model].append(score)
        
        model_analysis = {}
        for model, scores in model_scores.items():
            model_analysis[model] = {
                'avg_score': np.mean(scores),
                'best_score': max(scores),
                'count': len(scores)
            }
        
        insights['model_analysis'] = model_analysis
        
        # Feature combination effectiveness
        feature_scores = defaultdict(list)
        for result in successful:
            if 'configuration' in result:
                features = str(result['configuration']['feature_config'])
                score = result.get(CONFIG["optimization_metric"], 0)
                feature_scores[features].append(score)
        
        feature_analysis = {}
        for features, scores in feature_scores.items():
            feature_analysis[features] = {
                'avg_score': np.mean(scores),
                'best_score': max(scores),
                'count': len(scores)
            }
        
        insights['feature_analysis'] = feature_analysis
        
        return insights

# ============================================================================
# MAIN ORCHESTRATOR
# ============================================================================

class OptimizationOrchestrator:
    """Main orchestrator for the optimization process"""
    
    def __init__(self):
        self.start_time = time.time()
        self.output_dir = Path(CONFIG["output_dir"])
        self.output_dir.mkdir(exist_ok=True)
        
        # Create subdirectories
        for subdir in ["analysis", "models", "results"]:
            (self.output_dir / subdir).mkdir(exist_ok=True)
    
    def run_complete_optimization(self) -> Dict:
        """Run the complete optimization process"""
        
        LOG.info("="*80)
        LOG.info("OPTIMIZED MAIL-TO-CALLS PREDICTION SYSTEM")
        LOG.info("="*80)
        LOG.info("STRATEGY: Systematic optimization of:")
        LOG.info("  1. Mail types (Volume vs Correlation tradeoff)")
        LOG.info("  2. Feature engineering combinations")
        LOG.info("  3. Model selection and tuning")
        LOG.info("  4. Find highest accuracy configuration")
        LOG.info("="*80)
        
        try:
            # Phase 1: Load and Analyze Data
            LOG.info("\nPHASE 1: DATA LOADING AND MAIL TYPE ANALYSIS")
            data_loader = OptimizationDataLoader()
            
            # Load data
            data_loader.load_call_data()
            data_loader.load_mail_data()
            
            # Comprehensive mail type analysis
            mail_analysis = data_loader.analyze_mail_types_comprehensive()
            
            # Phase 2: Systematic Optimization
            LOG.info("\nPHASE 2: SYSTEMATIC MODEL OPTIMIZATION")
            optimizer = SystematicModelOptimizer(data_loader)
            optimization_results = optimizer.run_systematic_optimization()
            
            # Phase 3: Analysis and Insights
            LOG.info("\nPHASE 3: ANALYZING OPTIMIZATION RESULTS")
            top_configs = optimizer.get_top_configurations(10)
            insights = optimizer.analyze_optimization_insights()
            
            # Phase 4: Save Results
            LOG.info("\nPHASE 4: SAVING OPTIMIZATION RESULTS")
            self.save_optimization_results(data_loader, optimizer, top_configs, insights)
            
            # Phase 5: Generate Report
            LOG.info("\nPHASE 5: GENERATING OPTIMIZATION REPORT")
            report_path = self.generate_optimization_report(data_loader, optimizer, top_configs, insights)
            
            execution_time = (time.time() - self.start_time) / 60
            
            LOG.info(f"\n{'='*80}")
            LOG.info("OPTIMIZATION COMPLETE!")
            LOG.info(f"Execution time: {execution_time:.1f} minutes")
            if optimizer.best_configuration:
                LOG.info(f"Best configuration: {optimizer.best_configuration['config_id']}")
                LOG.info(f"Best {CONFIG['optimization_metric']}: {optimizer.best_score:.3f}")
            LOG.info(f"Results saved to: {self.output_dir}")
            LOG.info(f"{'='*80}")
            
            return {
                'success': True,
                'execution_time_minutes': execution_time,
                'best_score': optimizer.best_score,
                'best_configuration': optimizer.best_configuration,
                'best_model': optimizer.best_model,
                'top_configurations': top_configs,
                'optimization_insights': insights,
                'mail_analysis': mail_analysis,
                'output_directory': str(self.output_dir),
                'report_path': report_path
            }
            
        except Exception as e:
            LOG.error(f"OPTIMIZATION FAILED: {str(e)}")
            LOG.error(traceback.format_exc())
            
            return {
                'success': False,
                'error': str(e),
                'execution_time_minutes': (time.time() - self.start_time) / 60
            }
    
    def save_optimization_results(self, data_loader, optimizer, top_configs, insights):
        """Save all optimization results"""
        
        try:
            # Save mail analysis
            with open(self.output_dir / "analysis" / "mail_type_analysis.json", 'w') as f:
                json.dump(data_loader.mail_analysis, f, indent=2, default=str)
            
            # Save optimization results (simplified for JSON)
            simplified_results = {}
            for config_id, results in optimizer.optimization_results.items():
                simplified_results[config_id] = {}
                for key, value in results.items():
                    if key == 'model':
                        simplified_results[config_id][key] = str(type(value).__name__)
                    elif key == 'feature_importance' and isinstance(value, list):
                        simplified_results[config_id][key] = value[:10]  # Top 10
                    else:
                        simplified_results[config_id][key] = value
            
            with open(self.output_dir / "results" / "optimization_results.json", 'w') as f:
                json.dump(simplified_results, f, indent=2, default=str)
            
            # Save top configurations
            with open(self.output_dir / "results" / "top_configurations.json", 'w') as f:
                json.dump(top_configs, f, indent=2, default=str)
            
            # Save insights
            with open(self.output_dir / "results" / "optimization_insights.json", 'w') as f:
                json.dump(insights, f, indent=2, default=str)
            
            # Save best model if available
            if optimizer.best_model and CONFIG["save_best_model"]:
                joblib.dump(optimizer.best_model, self.output_dir / "models" / "best_optimized_model.pkl")
                
                # Save best configuration details
                best_config_details = {
                    'configuration': optimizer.best_configuration,
                    'performance': {
                        'best_score': optimizer.best_score,
                        'optimization_metric': CONFIG['optimization_metric']
                    }
                }
                
                with open(self.output_dir / "models" / "best_model_config.json", 'w') as f:
                    json.dump(best_config_details, f, indent=2, default=str)
            
            LOG.info("All optimization results saved successfully")
            
        except Exception as e:
            LOG.error(f"Failed to save optimization results: {str(e)}")
    
    def generate_optimization_report(self, data_loader, optimizer, top_configs, insights) -> str:
        """Generate comprehensive optimization report"""
        
        try:
            execution_time = (time.time() - self.start_time) / 60
            
            report = f"""
================================================================
OPTIMIZED MAIL-TO-CALLS PREDICTION SYSTEM
================================================================
OPTIMIZATION REPORT

EXECUTIVE SUMMARY:
-----------------
Optimization Status: {'SUCCESS' if optimizer.best_configuration else 'PARTIAL'}
Execution Time: {execution_time:.1f} minutes
Configurations Tested: {len(optimizer.optimization_results)}
Best {CONFIG['optimization_metric'].upper()}: {optimizer.best_score:.3f}
Best Configuration: {optimizer.best_configuration['config_id'] if optimizer.best_configuration else 'None'}

MAIL TYPE ANALYSIS RESULTS:
--------------------------"""
            
            # Mail type analysis summary
            if 'top_by_volume' in data_loader.mail_analysis:
                report += f"\nTop 5 Mail Types by Volume:"
                for i, mail_type in enumerate(data_loader.mail_analysis['top_by_volume'][:5]):
                    volume = data_loader.mail_analysis['volume_analysis'][mail_type]['total_volume']
                    report += f"\n  {i+1}. {mail_type}: {volume:,.0f} total volume"
            
            if 'top_by_correlation' in data_loader.mail_analysis:
                report += f"\n\nTop 5 Mail Types by Correlation:"
                for i, mail_type in enumerate(data_loader.mail_analysis['top_by_correlation'][:5]):
                    corr = data_loader.mail_analysis['correlation_analysis'][mail_type]['best_correlation']
                    lag = data_loader.mail_analysis['correlation_analysis'][mail_type]['best_lag']
                    report += f"\n  {i+1}. {mail_type}: {corr:.3f} correlation (lag {lag} days)"
            
            report += f"\n\nOPTIMIZATION INSIGHTS:\n{'-'*25}"
            
            # Strategy effectiveness
            if 'mail_strategy_analysis' in insights:
                report += f"\nMail Strategy Effectiveness:"
                for strategy, data in insights['mail_strategy_analysis'].items():
                    report += f"\n  {strategy}: Avg {CONFIG['optimization_metric']} = {data['avg_score']:.3f} ({data['count']} tests)"
            
            # Model effectiveness
            if 'model_analysis' in insights:
                report += f"\n\nModel Effectiveness:"
                for model, data in insights['model_analysis'].items():
                    report += f"\n  {model}: Avg {CONFIG['optimization_metric']} = {data['avg_score']:.3f} ({data['count']} tests)"
            
            # Top configurations
            report += f"\n\nTOP {len(top_configs)} CONFIGURATIONS:\n{'-'*30}"
            
            for i, config in enumerate(top_configs[:10]):
                config_info = config['results']['configuration']
                report += f"""
{i+1}. Configuration: {config['config_id']}
   Score ({CONFIG['optimization_metric']}): {config['score']:.3f}
   Mail Strategy: {config_info['mail_strategy']}
   Model: {config_info['model_name']}
   Features: {config['results']['features']}
   Test R²: {config['results'].get('test_r2', 'N/A')}
   Test MAE: {config['results'].get('test_mae', 'N/A')}"""
            
            # Best configuration details
            if optimizer.best_configuration:
                best_config = optimizer.best_configuration
                best_results = optimizer.optimization_results[best_config['config_id']]
                
                report += f"""

BEST CONFIGURATION DETAILS:
===========================
Configuration ID: {best_config['config_id']}
Mail Strategy: {best_config['mail_strategy']}
Model: {best_config['model_name']}
Feature Configuration: {best_config['feature_config']}
Lag Configuration: {best_config['lag_config']}

Performance Metrics:
- CV {CONFIG['optimization_metric'].upper()}: {optimizer.best_score:.3f}
- CV MAE: {best_results.get('cv_mae', 'N/A')}
- Test R²: {best_results.get('test_r2', 'N/A')}
- Test MAE: {best_results.get('test_mae', 'N/A')}
- Features Used: {best_results.get('features', 'N/A')}

Top Mail Types Used:
{chr(10).join([f"  {i+1}. {mail_type}" for i, mail_type in enumerate(best_config['mail_types'][:10])])}"""
                
                if 'feature_importance' in best_results:
                    report += f"\n\nTop 10 Most Important Features:"
                    for i, (feature, importance) in enumerate(best_results['feature_importance'][:10]):
                        report += f"\n  {i+1}. {feature}: {importance:.4f}"
            
            report += f"""

PRODUCTION DEPLOYMENT:
=====================
Status: {'READY' if optimizer.best_model else 'NOT READY'}
Model File: {'best_optimized_model.pkl' if optimizer.best_model else 'Not available'}
Configuration File: {'best_model_config.json' if optimizer.best_model else 'Not available'}

Usage Instructions:
1. Load the best model: joblib.load('best_optimized_model.pkl')
2. Use the exact mail types and feature configuration from best_model_config.json
3. Ensure feature engineering matches the optimized pipeline
4. Apply to new mail data for call volume predictions

BUSINESS VALUE:
==============
• Optimized mail type selection based on volume-correlation tradeoff
• Systematic testing of {len(optimizer.optimization_results)} configurations
• Best performing model achieves {optimizer.best_score:.1%} {CONFIG['optimization_metric']} score
• Production-ready prediction system
• Data-driven mail campaign impact assessment

NEXT STEPS:
==========
1. Deploy the best optimized model to production
2. Set up automated daily predictions using optimized configuration
3. Monitor performance and retrain with new data
4. Use mail type insights for campaign planning

================================================================
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Optimization Method: Systematic Grid Search
Total Runtime: {execution_time:.1f} minutes
================================================================
"""
            
            # Save report
            report_path = self.output_dir / "OPTIMIZATION_REPORT.txt"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            # Print key sections
            safe_print(report)
            
            return str(report_path)
            
        except Exception as e:
            LOG.error(f"Report generation failed: {str(e)}")
            return ""

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main execution function"""
    
    safe_print("="*80)
    safe_print("OPTIMIZED MAIL-TO-CALLS PREDICTION SYSTEM")
    safe_print("="*80)
    safe_print("SYSTEMATIC OPTIMIZATION APPROACH:")
    safe_print("  1. Analyze mail types by VOLUME and CORRELATION")
    safe_print("  2. Find optimal tradeoff between volume and correlation")
    safe_print("  3. Test comprehensive feature combinations")
    safe_print("  4. Systematic model testing and selection")
    safe_print("  5. Output the HIGHEST ACCURACY configuration")
    safe_print("")
    safe_print("GOAL: Maximum prediction accuracy through systematic optimization")
    safe_print("="*80)
    safe_print("")
    
    try:
        orchestrator = OptimizationOrchestrator()
        results = orchestrator.run_complete_optimization()
        
        if results['success']:
            safe_print("\n" + "="*60)
            safe_print("OPTIMIZATION COMPLETED SUCCESSFULLY!")
            safe_print("="*60)
            safe_print("")
            safe_print("OPTIMAL CONFIGURATION FOUND:")
            
            if results['best_configuration']:
                best_config = results['best_configuration']
                safe_print(f"  Configuration: {best_config['config_id']}")
                safe_print(f"  Best Score: {results['best_score']:.3f}")
                safe_print(f"  Mail Strategy: {best_config['mail_strategy']}")
                safe_print(f"  Model: {best_config['model_name']}")
                safe_print(f"  Mail Types Used: {len(best_config['mail_types'])}")
                safe_print("")
                safe_print("TOP MAIL TYPES:")
                for i, mail_type in enumerate(best_config['mail_types'][:5]):
                    safe_print(f"    {i+1}. {mail_type}")
                safe_print("")
            
            safe_print("OPTIMIZATION INSIGHTS:")
            safe_print(f"  Total Configurations Tested: {len(results.get('optimization_insights', {}))}")
            safe_print(f"  Execution Time: {results['execution_time_minutes']:.1f} minutes")
            safe_print(f"  Results Saved: {results['output_directory']}")
            safe_print("")
            safe_print("PRODUCTION READY:")
            safe_print("  * Best optimized model saved")
            safe_print("  * Complete configuration documented") 
            safe_print("  * Mail type analysis completed")
            safe_print("  * Feature engineering optimized")
            
        else:
            safe_print("\n" + "="*50)
            safe_print("OPTIMIZATION FAILED")
            safe_print("="*50)
            safe_print(f"Error: {results['error']}")
            safe_print("Check logs for detailed information")
        
        return 0 if results['success'] else 1
        
    except KeyboardInterrupt:
        safe_print("\nOptimization interrupted by user")
        return 1
        
    except Exception as e:
        safe_print(f"\nSystem error: {str(e)}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
