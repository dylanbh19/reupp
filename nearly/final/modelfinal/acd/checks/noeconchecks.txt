# #!/usr/bin/env python
"""
COMPREHENSIVE MAIL-TO-CALLS PREDICTION SYSTEM

CLEAN APPROACH:

1. Load clean call data (Date, ACDCalls) + mail data
1. Full EDA with plots and correlations
1. Feature engineering with proper lags
1. Simple model first, then build complexity
1. Goal: Predict call volumes from mail volumes (daily/weekly)

CONFIGURABLE PATHS AND SYSTEMATIC BUILD-UP
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from pathlib import Path
import sys
from datetime import datetime, timedelta
import holidays


from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from scipy.stats import pearsonr
import joblib

# ============================================================================

# CONFIGURATION - CHANGE YOUR FILE PATHS HERE

# ============================================================================

CONFIG = {
# ============ YOUR FILE PATHS ============
"call_file": "ACDMail.csv",  # ← CHANGE THIS
"mail_file": "mail.csv",                    # ← CHANGE IF NEEDED


# ============ YOUR COLUMN NAMES ============
"call_date_col": "Date",        # Your call data date column
"call_volume_col": "ACDCalls",  # Your call data volume column

# Mail columns (keep as is unless different)
"mail_date_col": "mail_date",
"mail_volume_col": "mail_volume", 
"mail_type_col": "mail_type",

# ============ ANALYSIS SETTINGS ============
"output_dir": "mail_call_prediction_system",
"top_mail_types": 8,
"test_size": 0.25,
"random_state": 42,

# Feature engineering
"max_lag_days": 7,
"rolling_windows": [3, 7],

# Visualization
"figure_size": (15, 10),
"plot_style": "seaborn-v0_8",


}

def remove_us_holidays(df, date_col='date'):
    """Remove US holidays from the DataFrame using a pre-generated CSV file."""
    safe_print("   Removing US holidays from call data using CSV file...")

    try:
        # Load the list of holidays from your CSV
        holidays_df = pd.read_csv("us_holidays.csv")
        # Create a set of holiday date strings for fast lookup
        holiday_dates_to_remove = set(holidays_df['holiday_date'])
    except FileNotFoundError:
        safe_print("❌ ERROR: 'us_holidays.csv' not found!")
        safe_print("   Please make sure you have created the us_holidays.csv file.")
        # Return the original dataframe if the holiday file is missing
        return df

    # Create a boolean mask by converting the DataFrame's date column to 'YYYY-MM-DD' strings
    # and checking if they exist in our set of holidays.
    holiday_mask = df[date_col].dt.strftime('%Y-%m-%d').isin(holiday_dates_to_remove)

    holidays_found = df[holiday_mask]

    if not holidays_found.empty:
        safe_print(f"   Found {len(holidays_found)} US holidays to remove:")
        for _, row in holidays_found.sort_values(by=date_col).iterrows():
            # Get the date as a string to look up the name
            date_str = row[date_col].strftime('%Y-%m-%d')
            holiday_name = holidays_df[holidays_df['holiday_date'] == date_str]['holiday_name'].iloc[0]
            safe_print(f"     - {date_str}: {holiday_name}")
    else:
        safe_print("   No US holidays found in the provided date range.")

    # Invert the mask to keep non-holidays and create a copy.
    df_no_holidays = df[~holiday_mask].copy()

    safe_print(f"   Removed {len(holidays_found)} holiday rows.")
    safe_print(f"   Data after holiday removal: {len(df_no_holidays)} rows.")

    return df_no_holidays

def safe_print(msg):
    try:
        print(str(msg).encode('ascii', 'ignore').decode('ascii'))
    except:
        print(str(msg))

# ============================================================================

# STEP 1: DATA LOADING

# ============================================================================

class DataManager:
    def __init__(self):
        self.call_data = None
        self.mail_data = None
        self.merged_data = None
        self.output_dir = Path(CONFIG["output_dir"])
        self.output_dir.mkdir(exist_ok=True)


    def load_call_data(self):
        """Load your clean call data"""
        safe_print("=" * 80)
        safe_print("STEP 1A: LOADING CLEAN CALL DATA")
        safe_print("=" * 80)
        
        # Try multiple paths
        call_paths = [
            CONFIG["call_file"],
            f"data/{CONFIG['call_file']}",
            f"./{CONFIG['call_file']}"
        ]
        
        call_path = None
        for path in call_paths:
            if Path(path).exists():
                call_path = path
                break
        
        if not call_path:
            safe_print("❌ CALL FILE NOT FOUND!")
            safe_print("Please update CONFIG['call_file'] with the correct path")
            safe_print("Tried paths:")
            for path in call_paths:
                safe_print(f"  - {path}")
            raise FileNotFoundError("Call file not found")
        
        safe_print(f"✅ Loading: {call_path}")
        
        # Load with encoding attempts
        for encoding in ['utf-8', 'latin1', 'cp1252']:
            try:
                df = pd.read_csv(call_path, encoding=encoding)
                safe_print(f"   Loaded with {encoding} encoding")
                break
            except:
                continue
        else:
            raise ValueError("Could not load call file")
        
        safe_print(f"   Raw data: {len(df):,} rows")
        safe_print(f"   Columns: {df.columns.tolist()}")
        
        # Check for required columns
        date_col = CONFIG["call_date_col"]
        volume_col = CONFIG["call_volume_col"]
        
        if date_col not in df.columns:
            safe_print(f"❌ Date column '{date_col}' not found!")
            safe_print(f"   Available columns: {df.columns.tolist()}")
            raise ValueError(f"Date column '{date_col}' not found")
        
        if volume_col not in df.columns:
            safe_print(f"❌ Volume column '{volume_col}' not found!")
            safe_print(f"   Available columns: {df.columns.tolist()}")
            raise ValueError(f"Volume column '{volume_col}' not found")
        
        # Clean and process
        df_clean = df[[date_col, volume_col]].copy()
        df_clean.columns = ['date', 'call_volume']
        
        # Process dates
        df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')
        df_clean = df_clean.dropna(subset=['date'])
        
        # Process call volumes
        df_clean['call_volume'] = pd.to_numeric(df_clean['call_volume'], errors='coerce')
        df_clean = df_clean.dropna(subset=['call_volume'])
        df_clean = df_clean[df_clean['call_volume'] > 5]  # Remove negative values
        
        # Filter to business days only
        df_clean = df_clean[df_clean['date'].dt.weekday < 5]
        df_clean = remove_us_holidays(df_clean, 'date')
        print("REMOVEDDDDHOLIDAYSSSSSSSSSSS")
        # Sort by date
        df_clean = df_clean.sort_values('date').reset_index(drop=True)
        
        self.call_data = df_clean
        
        safe_print(f"✅ Clean call data: {len(df_clean)} business days")
        safe_print(f"   Date range: {df_clean['date'].min().date()} to {df_clean['date'].max().date()}")
        safe_print(f"   Call volume: {df_clean['call_volume'].min():.0f} to {df_clean['call_volume'].max():.0f}")
        safe_print(f"   Daily average: {df_clean['call_volume'].mean():.0f} calls")
        
        return df_clean
        
    def load_mail_data(self):
        """Load mail data"""
        safe_print("\n" + "=" * 80)
        safe_print("STEP 1B: LOADING MAIL DATA")
        safe_print("=" * 80)
        
        # Try multiple paths
        mail_paths = [
            CONFIG["mail_file"],
            f"data/{CONFIG['mail_file']}",
            f"./{CONFIG['mail_file']}"
        ]
        
        mail_path = None
        for path in mail_paths:
            if Path(path).exists():
                mail_path = path
                break
        
        if not mail_path:
            safe_print("❌ MAIL FILE NOT FOUND!")
            safe_print("Please update CONFIG['mail_file'] with the correct path")
            raise FileNotFoundError("Mail file not found")
        
        safe_print(f"✅ Loading: {mail_path}")
        
        # Load with encoding attempts
        for encoding in ['utf-8', 'latin1', 'cp1252']:
            try:
                df = pd.read_csv(mail_path, encoding=encoding, low_memory=False)
                safe_print(f"   Loaded with {encoding} encoding")
                break
            except:
                continue
        else:
            raise ValueError("Could not load mail file")
        
        safe_print(f"   Raw data: {len(df):,} rows, {len(df.columns)} columns")
        
        # Clean column names and find required columns
        df.columns = [str(col).lower().strip() for col in df.columns]
        
        date_col = volume_col = type_col = None
        for col in df.columns:
            if 'date' in col:
                date_col = col
            elif 'volume' in col:
                volume_col = col
            elif 'type' in col:
                type_col = col
        
        if not all([date_col, volume_col, type_col]):
            safe_print(f"❌ Required mail columns not found!")
            safe_print(f"   Available: {df.columns.tolist()}")
            safe_print(f"   Looking for: date, volume, type columns")
            raise ValueError("Required mail columns not found")
        
        safe_print(f"   Using: date={date_col}, volume={volume_col}, type={type_col}")
        
        # Process mail data
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df = df.dropna(subset=[date_col])
        
        df[volume_col] = pd.to_numeric(df[volume_col], errors='coerce')
        df = df.dropna(subset=[volume_col])
        df = df[df[volume_col] > 0]
        
        # Create daily mail by type
        df['mail_date'] = df[date_col].dt.date
        daily_mail = df.groupby(['mail_date', type_col])[volume_col].sum().reset_index()
        daily_mail.columns = ['date', 'mail_type', 'volume']
        daily_mail['date'] = pd.to_datetime(daily_mail['date'])
        
        # Filter to business days
        daily_mail = daily_mail[daily_mail['date'].dt.weekday < 5]
        
        # Pivot to mail types as columns
        mail_pivot = daily_mail.pivot(index='date', columns='mail_type', values='volume').fillna(0)
        mail_pivot = mail_pivot.reset_index()
        
        self.mail_data = mail_pivot
        
        safe_print(f"✅ Clean mail data: {len(mail_pivot)} business days")
        safe_print(f"   Date range: {mail_pivot['date'].min().date()} to {mail_pivot['date'].max().date()}")
        safe_print(f"   Mail types: {len(mail_pivot.columns)-1}")
        
        return mail_pivot
        
    def merge_data(self):
        """Merge call and mail data"""
        safe_print("\n" + "=" * 80)
        safe_print("STEP 1C: MERGING CALL AND MAIL DATA")
        safe_print("=" * 80)
        
        if self.call_data is None or self.mail_data is None:
            raise ValueError("Must load both call and mail data first")
        
        # Find overlapping dates
        call_dates = set(self.call_data['date'].dt.date)
        mail_dates = set(self.mail_data['date'].dt.date)
        common_dates = call_dates.intersection(mail_dates)
        
        safe_print(f"   Call data: {len(call_dates)} days")
        safe_print(f"   Mail data: {len(mail_dates)} days")
        safe_print(f"   Common dates: {len(common_dates)} days")
        
        if len(common_dates) < 30:
            safe_print(f"⚠️  WARNING: Only {len(common_dates)} overlapping days")
        
        # Filter to common dates and merge
        common_dates_dt = [pd.to_datetime(d) for d in common_dates]
        
        calls_filtered = self.call_data[self.call_data['date'].isin(common_dates_dt)]
        mail_filtered = self.mail_data[self.mail_data['date'].isin(common_dates_dt)]
        
        merged = pd.merge(calls_filtered, mail_filtered, on='date', how='inner')
        merged = merged.sort_values('date').reset_index(drop=True)
        
        self.merged_data = merged
        
        safe_print(f"✅ Merged dataset: {len(merged)} days")
        safe_print(f"   Columns: {len(merged.columns)} (date + calls + {len(merged.columns)-2} mail types)")
        safe_print(f"   Date range: {merged['date'].min().date()} to {merged['date'].max().date()}")
        
        return merged


# ============================================================================

# STEP 2: COMPREHENSIVE EDA WITH PLOTS

# ============================================================================

class EDATrendAnalysis:
    def __init__(self, merged_data, output_dir):
        self.data = merged_data
        self.output_dir = output_dir / "eda_plots"
        self.output_dir.mkdir(exist_ok=True)
        self.mail_columns = [col for col in merged_data.columns if col not in ['date', 'call_volume']]


        # Set plotting style
        plt.style.use('default')
        sns.set_palette("husl")
        
    def run_full_eda(self):
        """Run comprehensive EDA"""
        safe_print("\n" + "=" * 80)
        safe_print("STEP 2: COMPREHENSIVE EDA AND VISUALIZATION")
        safe_print("=" * 80)
        
        # Basic overview
        self.create_overview_plots()
        
        # Time series analysis
        self.create_time_series_plots()
        
        # Correlation analysis
        correlations = self.analyze_correlations()
        
        # Mail type analysis
        top_mail_types = self.analyze_mail_types()
        
        # Lag analysis
        best_lag_info = self.analyze_lag_relationships()
        
        safe_print(f"\n✅ EDA Complete! Plots saved to: {self.output_dir}")
        
        return {
            'correlations': correlations,
            'top_mail_types': top_mail_types,
            'best_lag': best_lag_info
        }

    def create_overview_plots(self):
        """Create overview plots"""
        safe_print("\n--- Creating Overview Plots ---")
        
        total_mail = self.data[self.mail_columns].sum(axis=1)
        overall_corr = self.data['call_volume'].corr(total_mail)
        
        fig, axes = plt.subplots(2, 2, figsize=CONFIG["figure_size"])
        fig.suptitle('Data Overview', fontsize=16, fontweight='bold')
        
        # Call volume over time
        axes[0, 0].plot(self.data['date'], self.data['call_volume'], 'b-', linewidth=2)
        axes[0, 0].set_title('Daily Call Volume (ACDCalls)')
        axes[0, 0].set_ylabel('Call Volume')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # Total mail over time
        axes[0, 1].plot(self.data['date'], total_mail, 'g-', linewidth=2)
        axes[0, 1].set_title('Daily Total Mail Volume')
        axes[0, 1].set_ylabel('Mail Volume')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # Scatter plot: mail vs calls
        axes[1, 0].scatter(total_mail, self.data['call_volume'], alpha=0.6)
        axes[1, 0].set_xlabel('Total Mail Volume')
        axes[1, 0].set_ylabel('Call Volume')
        axes[1, 0].set_title(f'Mail vs Calls (r={overall_corr:.3f})')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Basic statistics
        call_stats = self.data['call_volume'].describe()
        mail_stats = total_mail.describe()
        
        stats_text = f"""
        CALL VOLUME STATS:
        Mean: {call_stats['mean']:.0f}
        Std:  {call_stats['std']:.0f}
        Min:  {call_stats['min']:.0f}
        Max:  {call_stats['max']:.0f}
        
        MAIL VOLUME STATS:
        Mean: {mail_stats['mean']:.0f}
        Std:  {mail_stats['std']:.0f}
        Min:  {mail_stats['min']:.0f}
        Max:  {mail_stats['max']:.0f}
        
        CORRELATION: {overall_corr:.3f}
        DAYS: {len(self.data)}
        """
        
        axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=10, verticalalignment='center', fontfamily='monospace')
        axes[1, 1].set_title('Summary Statistics')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "01_overview.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        safe_print(f"   Overall correlation: {overall_corr:.3f}")
        
    def create_time_series_plots(self):
        """Create detailed time series plots"""
        safe_print("\n--- Creating Time Series Plots ---")
        
        total_mail = self.data[self.mail_columns].sum(axis=1)
        
        fig, axes = plt.subplots(3, 1, figsize=(16, 12))
        fig.suptitle('Time Series Analysis', fontsize=16, fontweight='bold')
        
        # Call volume time series
        axes[0].plot(self.data['date'], self.data['call_volume'], 'b-', linewidth=2, label='ACDCalls')
        axes[0].set_title('Daily Call Volume Over Time')
        axes[0].set_ylabel('Call Volume')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        
        # Total mail time series
        axes[1].plot(self.data['date'], total_mail, 'g-', linewidth=2, label='Total Mail')
        axes[1].set_title('Daily Total Mail Volume Over Time')
        axes[1].set_ylabel('Mail Volume')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        
        # Normalized overlay
        call_norm = (self.data['call_volume'] - self.data['call_volume'].min()) / (self.data['call_volume'].max() - self.data['call_volume'].min())
        mail_norm = (total_mail - total_mail.min()) / (total_mail.max() - total_mail.min())
        
        axes[2].plot(self.data['date'], call_norm, 'b-', linewidth=2, label='Calls (normalized)', alpha=0.8)
        axes[2].plot(self.data['date'], mail_norm, 'g-', linewidth=2, label='Mail (normalized)', alpha=0.8)
        axes[2].set_title('Normalized Comparison: Calls vs Mail')
        axes[2].set_ylabel('Normalized Values (0-1)')
        axes[2].set_xlabel('Date')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
        axes[2].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        
        for ax in axes:
            ax.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "02_time_series.png", dpi=150, bbox_inches='tight')
        plt.close()
        
    def analyze_correlations(self):
        """Comprehensive correlation analysis"""
        safe_print("\n--- Analyzing Correlations ---")
        
        # Calculate correlations for all mail types
        correlations = {}
        for mail_type in self.mail_columns:
            if self.data[mail_type].std() > 0:
                corr, p_value = pearsonr(self.data[mail_type], self.data['call_volume'])
                correlations[mail_type] = {'correlation': corr, 'p_value': p_value}
        
        # Sort by absolute correlation
        sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]['correlation']), reverse=True)
        
        safe_print("   Top 10 correlations with call volume:")
        for i, (mail_type, stats) in enumerate(sorted_corr[:10]):
            safe_print(f"   {i+1:2d}. {mail_type[:30]:<30}: r={stats['correlation']:>7.3f}")
        
        # Create correlation heatmap
        top_15_types = [item[0] for item in sorted_corr[:15]]
        corr_data = self.data[['call_volume'] + top_15_types].corr()
        
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(corr_data), k=1)
        sns.heatmap(corr_data, annot=True, fmt='.3f', cmap='RdBu_r', center=0,
                mask=mask, square=True, cbar_kws={'label': 'Correlation'})
        plt.title('Correlation Matrix: Calls vs Top 15 Mail Types')
        plt.tight_layout()
        plt.savefig(self.output_dir / "03_correlations.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        return sorted_corr

    def analyze_mail_types(self):
        """Analyze individual mail types"""
        safe_print("\n--- Analyzing Mail Types ---")
        
        # Calculate stats for each mail type
        mail_stats = {}
        for mail_type in self.mail_columns:
            stats_dict = {
                'total_volume': self.data[mail_type].sum(),
                'daily_average': self.data[mail_type].mean(),
                'max_day': self.data[mail_type].max(),
                'correlation': self.data[mail_type].corr(self.data['call_volume'])
            }
            mail_stats[mail_type] = stats_dict
        
        # Sort by total volume
        sorted_by_volume = sorted(mail_stats.items(), key=lambda x: x[1]['total_volume'], reverse=True)
        top_mail_types = [item[0] for item in sorted_by_volume[:CONFIG["top_mail_types"]]]
        
        safe_print(f"   Top {len(top_mail_types)} mail types by volume:")
        for i, (mail_type, stats) in enumerate(sorted_by_volume[:CONFIG["top_mail_types"]]):
            safe_print(f"   {i+1:2d}. {mail_type[:25]:<25}: {stats['total_volume']:>8,.0f} total, r={stats['correlation']:>6.3f}")
        
        # Create mail type analysis plots
        fig, axes = plt.subplots(2, 2, figsize=CONFIG["figure_size"])
        fig.suptitle('Mail Type Analysis', fontsize=16, fontweight='bold')
        
        # Volume ranking
        volumes = [mail_stats[mt]['total_volume'] for mt in top_mail_types]
        axes[0, 0].barh(range(len(top_mail_types)), volumes, alpha=0.7)
        axes[0, 0].set_yticks(range(len(top_mail_types)))
        axes[0, 0].set_yticklabels([mt[:20] for mt in top_mail_types])
        axes[0, 0].set_xlabel('Total Volume')
        axes[0, 0].set_title('Top Mail Types by Volume')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Correlation ranking
        correlations = [mail_stats[mt]['correlation'] for mt in top_mail_types]
        colors = ['red' if c < 0 else 'green' for c in correlations]
        
        axes[0, 1].barh(range(len(top_mail_types)), correlations, alpha=0.7, color=colors)
        axes[0, 1].set_yticks(range(len(top_mail_types)))
        axes[0, 1].set_yticklabels([mt[:20] for mt in top_mail_types])
        axes[0, 1].set_xlabel('Correlation with Calls')
        axes[0, 1].set_title('Correlation with Call Volume')
        axes[0, 1].axvline(x=0, color='black', linestyle='-', alpha=0.3)
        axes[0, 1].grid(True, alpha=0.3)
        
        # Time series of top 3 mail types
        top_3 = top_mail_types[:3]
        for i, mail_type in enumerate(top_3):
            axes[1, 0].plot(self.data['date'], self.data[mail_type], 
                        label=mail_type[:15], linewidth=2)
        
        axes[1, 0].set_title('Top 3 Mail Types Over Time')
        axes[1, 0].set_ylabel('Daily Volume')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # Best correlated mail type vs calls
        best_corr_type = max(mail_stats.items(), key=lambda x: abs(x[1]['correlation']))
        best_type_name = best_corr_type[0]
        best_corr_value = best_corr_type[1]['correlation']
        
        axes[1, 1].scatter(self.data[best_type_name], self.data['call_volume'], alpha=0.6)
        axes[1, 1].set_xlabel(f'{best_type_name[:20]} Volume')
        axes[1, 1].set_ylabel('Call Volume')
        axes[1, 1].set_title(f'Best Correlated: {best_type_name[:15]} (r={best_corr_value:.3f})')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "04_mail_types.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        return top_mail_types

    def analyze_lag_relationships(self):
        """Analyze lag relationships"""
        safe_print("\n--- Analyzing Lag Relationships ---")
        
        total_mail = self.data[self.mail_columns].sum(axis=1)
        
        # Calculate correlations for different lags
        lag_correlations = {}
        
        for lag in range(0, CONFIG["max_lag_days"] + 1):
            if len(self.data) > lag:
                if lag == 0:
                    corr = total_mail.corr(self.data['call_volume'])
                else:
                    # Mail today vs calls N days later
                    mail_today = total_mail[:-lag]
                    calls_later = self.data['call_volume'][lag:]
                    
                    if len(mail_today) > 10:
                        corr = mail_today.corr(calls_later)
                    else:
                        corr = 0
                
                lag_correlations[lag] = corr
                safe_print(f"   Lag {lag} days: correlation = {corr:.3f}")
        
        # Find best lag
        best_lag = max(lag_correlations.items(), key=lambda x: abs(x[1]))
        safe_print(f"   Best lag: {best_lag[0]} days (correlation: {best_lag[1]:.3f})")
        
        # Create lag analysis plot
        fig, axes = plt.subplots(1, 2, figsize=CONFIG["figure_size"])
        fig.suptitle('Lag Relationship Analysis', fontsize=16, fontweight='bold')
        
        # Correlation by lag
        lags = list(lag_correlations.keys())
        correlations = list(lag_correlations.values())
        
        bars = axes[0].bar(lags, correlations, alpha=0.7, color='purple')
        axes[0].set_xlabel('Lag (Days)')
        axes[0].set_ylabel('Correlation')
        axes[0].set_title('Correlation by Lag Days')
        axes[0].grid(True, alpha=0.3)
        
        # Highlight best lag
        best_idx = lags.index(best_lag[0])
        bars[best_idx].set_color('red')
        bars[best_idx].set_alpha(1.0)
        
        # Scatter plot for best lag
        if best_lag[0] == 0:
            x_data = total_mail
            y_data = self.data['call_volume']
            title = f'Same Day: Mail vs Calls (r={best_lag[1]:.3f})'
        else:
            x_data = total_mail[:-best_lag[0]]
            y_data = self.data['call_volume'][best_lag[0]:]
            title = f'Mail Today vs Calls +{best_lag[0]} Days (r={best_lag[1]:.3f})'
        
        axes[1].scatter(x_data, y_data, alpha=0.6)
        axes[1].set_xlabel('Mail Volume')
        axes[1].set_ylabel('Call Volume')
        axes[1].set_title(title)
        axes[1].grid(True, alpha=0.3)
        
        # Add trend line
        if len(x_data) > 1:
            z = np.polyfit(x_data, y_data, 1)
            p = np.poly1d(z)
            axes[1].plot(x_data, p(x_data), "r--", alpha=0.8)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "05_lag_analysis.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        return best_lag


# ============================================================================

# STEP 3: FEATURE ENGINEERING

# ============================================================================

class FeatureEngineer:
    def __init__(self, merged_data, top_mail_types, best_lag):
        self.data = merged_data
        self.top_mail_types = top_mail_types
        self.best_lag = best_lag[0] if best_lag else 1


    def create_features(self):
        """Create features for modeling"""
        safe_print("\n" + "=" * 80)
        safe_print("STEP 3: FEATURE ENGINEERING")
        safe_print("=" * 80)
        
        safe_print(f"   Using lag: {self.best_lag} days")
        safe_print(f"   Top mail types: {len(self.top_mail_types)}")
        
        features_list = []
        targets_list = []
        dates_list = []
        
        # Create features for each day
        max_lookback = max(7, self.best_lag)  # Need history for features
        
        for i in range(max_lookback, len(self.data) - self.best_lag):
            
            feature_row = {}
            current_date = self.data.iloc[i]['date']
            
            # === MAIL FEATURES ===
            for mail_type in self.top_mail_types:
                if mail_type in self.data.columns:
                    clean_name = mail_type.replace(' ', '').replace('-', '').replace('_', '')[:15]
                    
                    # Current day mail
                    feature_row[f"{clean_name}_today"] = self.data.iloc[i][mail_type]
                    
                    # Lag features (1, 2, 3 days ago)
                    for lag in [1, 2, 3]:
                        if i >= lag:
                            feature_row[f"{clean_name}_lag{lag}"] = self.data.iloc[i - lag][mail_type]
                    
                    # Rolling averages
                    for window in CONFIG["rolling_windows"]:
                        if i >= window - 1:
                            window_data = self.data.iloc[i - window + 1:i + 1][mail_type]
                            feature_row[f"{clean_name}_avg{window}"] = window_data.mean()
            
            # === TOTAL MAIL FEATURES ===
            total_mail = sum(self.data.iloc[i][mt] for mt in self.top_mail_types if mt in self.data.columns)
            feature_row['total_mail_today'] = total_mail
            
            # Total mail lags
            for lag in [1, 2, 3]:
                if i >= lag:
                    total_mail_lag = sum(self.data.iloc[i - lag][mt] for mt in self.top_mail_types if mt in self.data.columns)
                    feature_row[f'total_mail_lag{lag}'] = total_mail_lag
            
            # Total mail rolling averages
            for window in CONFIG["rolling_windows"]:
                if i >= window - 1:
                    window_totals = []
                    for j in range(i - window + 1, i + 1):
                        window_total = sum(self.data.iloc[j][mt] for mt in self.top_mail_types if mt in self.data.columns)
                        window_totals.append(window_total)
                    feature_row[f'total_mail_avg{window}'] = np.mean(window_totals)
            
            # === CALL HISTORY FEATURES ===
            feature_row['calls_yesterday'] = self.data.iloc[i - 1]['call_volume'] if i >= 1 else self.data.iloc[i]['call_volume']
            feature_row['calls_2days_ago'] = self.data.iloc[i - 2]['call_volume'] if i >= 2 else self.data.iloc[i]['call_volume']
            
            # Call rolling averages
            for window in CONFIG["rolling_windows"]:
                if i >= window - 1:
                    window_calls = self.data.iloc[i - window + 1:i + 1]['call_volume']
                    feature_row[f'calls_avg{window}'] = window_calls.mean()
            
            # === TEMPORAL FEATURES ===
            feature_row['weekday'] = current_date.weekday()
            #feature_row['month'] = current_date.month
            feature_row['day_of_month'] = current_date.day
            feature_row['is_month_end'] = 1 if current_date.day >= 25 else 0
            
            # === TARGET ===
            target_idx = i + self.best_lag
            if target_idx < len(self.data):
                target = self.data.iloc[target_idx]['call_volume']
                
                features_list.append(feature_row)
                targets_list.append(target)
                dates_list.append(current_date)
        
        # Convert to DataFrames
        X = pd.DataFrame(features_list)
        y = pd.Series(targets_list, name='call_volume')
        dates = pd.Series(dates_list, name='date')
        
        # Clean features
        X = X.fillna(0)
        X = X.replace([np.inf, -np.inf], 0)
        
        safe_print(f"✅ Created {len(X.columns)} features from {len(X)} samples")
        safe_print(f"   Mail features: {len([c for c in X.columns if any(mt.replace(' ', '')[:10] in c for mt in self.top_mail_types)])}")
        safe_print(f"   Call history: {len([c for c in X.columns if 'calls' in c])}")
        safe_print(f"   Temporal: {len([c for c in X.columns if any(t in c for t in ['weekday', 'month', 'day'])])}")
        
        return X, y, dates


# ============================================================================

# STEP 4: MODELING - START SIMPLE, BUILD UP

# ============================================================================

class ModelBuilder:
    def __init__(self, output_dir):
        self.output_dir = output_dir / "models"
        self.output_dir.mkdir(exist_ok=True)
        self.models = {}
        self.results = {}


    def train_simple_models(self, X, y, dates):
        """Start with simple models"""
        safe_print("\n" + "=" * 80)
        safe_print("STEP 4: SIMPLE MODEL TRAINING")
        safe_print("=" * 80)
        
        # Time-aware split
        split_idx = int(len(X) * (1 - CONFIG["test_size"]))
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        dates_train, dates_test = dates.iloc[:split_idx], dates.iloc[split_idx:]
        
        safe_print(f"   Train: {len(X_train)} samples")
        safe_print(f"   Test: {len(X_test)} samples")
        
        # Simple models to try
        models = {
            'linear': LinearRegression(),
            'ridge_light': Ridge(alpha=1.0, random_state=CONFIG["random_state"]),
            'ridge_strong': Ridge(alpha=10.0, random_state=CONFIG["random_state"]),
            'forest_simple': RandomForestRegressor(
                n_estimators=50, 
                max_depth=6, 
                min_samples_leaf=5,
                random_state=CONFIG["random_state"]
            )
        }
        
        results = {}
        best_model = None
        best_score = -float('inf')
        best_name = None
        
        for name, model in models.items():
            safe_print(f"\n--- Testing {name} ---")
            
            try:
                # Train
                model.fit(X_train, y_train)
                
                # Predictions
                y_pred_train = model.predict(X_train)
                y_pred_test = model.predict(X_test)
                
                # Evaluate
                train_r2 = r2_score(y_train, y_pred_train)
                test_r2 = r2_score(y_test, y_pred_test)
                test_mae = mean_absolute_error(y_test, y_pred_test)
                test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
                
                # MAPE
                mape = np.mean(np.abs((y_test - y_pred_test) / (y_test + 1e-10))) * 100
                mape = min(mape, 200)  # Cap extreme values
                
                overfitting = train_r2 - test_r2
                
                results[name] = {
                    'train_r2': train_r2,
                    'test_r2': test_r2,
                    'test_mae': test_mae,
                    'test_rmse': test_rmse,
                    'test_mape': mape,
                    'overfitting': overfitting,
                    'model': model,
                    'predictions': y_pred_test
                }
                
                safe_print(f"   Train R²: {train_r2:.3f}")
                safe_print(f"   Test R²:  {test_r2:.3f}")
                safe_print(f"   Test MAE: {test_mae:.0f}")
                safe_print(f"   Test MAPE: {mape:.1f}%")
                safe_print(f"   Overfitting: {overfitting:.3f}")
                
                # Select best (prioritize test R², penalize overfitting)
                adjusted_score = test_r2 - max(0, overfitting - 0.1) * 0.5
                
                if adjusted_score > best_score and test_r2 > 0.05:  # Must have some predictive power
                    best_score = adjusted_score
                    best_model = model
                    best_name = name
                    safe_print(f"   ★ NEW BEST! (Score: {adjusted_score:.3f})")
                
            except Exception as e:
                safe_print(f"   ✗ Failed: {e}")
                results[name] = {'error': str(e)}
        
        self.models = results
        self.results = results
        
        if best_model:
            safe_print(f"\n🎯 BEST MODEL: {best_name}")
            safe_print(f"   Test R²: {results[best_name]['test_r2']:.3f}")
            safe_print(f"   Test MAE: {results[best_name]['test_mae']:.0f}")
            safe_print(f"   Test MAPE: {results[best_name]['test_mape']:.1f}%")
            
            # Train on full dataset
            best_model.fit(X, y)
            
            # Save best model
            model_info = {
                'model': best_model,
                'model_name': best_name,
                'features': X.columns.tolist(),
                'performance': results[best_name]
            }
            
            joblib.dump(model_info, self.output_dir / "best_model.pkl")
            
            # Create validation plots
            self.create_model_validation_plots(X_test, y_test, results[best_name]['predictions'], 
                                            dates_test, best_name, results)
            
            return best_model, best_name, results
        else:
            safe_print("\n❌ NO MODEL ACHIEVED ACCEPTABLE PERFORMANCE")
            return None, None, results

    def create_model_validation_plots(self, X_test, y_test, y_pred, dates_test, best_name, results):
        """Create comprehensive validation plots"""
        safe_print("\n--- Creating Model Validation Plots ---")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'Model Validation: {best_name}', fontsize=16, fontweight='bold')
        
        # 1. Actual vs Predicted
        axes[0, 0].scatter(y_test, y_pred, alpha=0.6, color='blue')
        axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('Actual Calls')
        axes[0, 0].set_ylabel('Predicted Calls')
        axes[0, 0].set_title(f'Actual vs Predicted (R²={results[best_name]["test_r2"]:.3f})')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Time series comparison
        axes[0, 1].plot(dates_test, y_test.values, 'b-', label='Actual', linewidth=2, marker='o')
        axes[0, 1].plot(dates_test, y_pred, 'r-', label='Predicted', linewidth=2, marker='s', alpha=0.7)
        axes[0, 1].set_title('Predictions vs Actual Over Time')
        axes[0, 1].set_ylabel('Call Volume')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. Residuals
        residuals = y_test - y_pred
        axes[0, 2].scatter(y_pred, residuals, alpha=0.6, color='green')
        axes[0, 2].axhline(y=0, color='r', linestyle='--')
        axes[0, 2].set_xlabel('Predicted Calls')
        axes[0, 2].set_ylabel('Residuals')
        axes[0, 2].set_title('Residual Plot')
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. Residual histogram
        axes[1, 0].hist(residuals, bins=15, alpha=0.7, color='purple', edgecolor='black')
        axes[1, 0].axvline(x=0, color='r', linestyle='--')
        axes[1, 0].set_xlabel('Residuals')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Residual Distribution')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. Error metrics
        metrics_text = f"""
        MODEL PERFORMANCE:
        
        R² Score: {results[best_name]['test_r2']:.3f}
        MAE: {results[best_name]['test_mae']:.0f}
        RMSE: {results[best_name]['test_rmse']:.0f}
        MAPE: {results[best_name]['test_mape']:.1f}%
        
        Overfitting: {results[best_name]['overfitting']:.3f}
        
        Mean Actual: {y_test.mean():.0f}
        Mean Predicted: {y_pred.mean():.0f}
        """
        
        axes[1, 1].text(0.1, 0.5, metrics_text, transform=axes[1, 1].transAxes,
                        fontsize=11, verticalalignment='center', fontfamily='monospace')
        axes[1, 1].set_title('Performance Metrics')
        axes[1, 1].axis('off')
        
        # 6. Model comparison
        model_names = [name for name in results.keys() if 'error' not in results[name]]
        test_r2_scores = [results[name]['test_r2'] for name in model_names]
        
        bars = axes[1, 2].bar(range(len(model_names)), test_r2_scores, alpha=0.7, color='orange')
        axes[1, 2].set_xticks(range(len(model_names)))
        axes[1, 2].set_xticklabels(model_names, rotation=45)
        axes[1, 2].set_ylabel('Test R²')
        axes[1, 2].set_title('Model Comparison')
        axes[1, 2].grid(True, alpha=0.3)
        
        # Highlight best model
        best_idx = model_names.index(best_name)
        bars[best_idx].set_color('gold')
        bars[best_idx].set_edgecolor('red')
        bars[best_idx].set_linewidth(2)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "model_validation.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        safe_print(f"   Validation plots saved: {self.output_dir}/model_validation.png")


# ============================================================================

# STEP 5: PREDICTION SYSTEM

# ============================================================================

class PredictionSystem:
    def __init__(self, model_info, top_mail_types, best_lag):
        self.model = model_info['model']
        self.model_name = model_info['model_name']
        self.features = model_info['features']
        self.top_mail_types = top_mail_types
        self.best_lag = best_lag


    def predict_calls(self, mail_input, call_history=None):
        """
        Predict call volume from mail input
        
        Args:
            mail_input: dict like {'DRP Stmt.': 2000, 'Cheque': 1500, ...}
            call_history: dict like {'yesterday': 12000, '2_days_ago': 11500} (optional)
        
        Returns:
            dict with prediction and details
        """
        
        try:
            # Create feature vector
            features = {}
            
            # Mail features
            for mail_type in self.top_mail_types:
                clean_name = mail_type.replace(' ', '').replace('-', '').replace('_', '')[:15]
                volume = mail_input.get(mail_type, 0)
                
                # Today's mail
                features[f"{clean_name}_today"] = volume
                
                # Simulate lags (use current volume as approximation)
                for lag in [1, 2, 3]:
                    features[f"{clean_name}_lag{lag}"] = volume * 0.8 ** lag  # Decay approximation
                
                # Simulate averages
                for window in CONFIG["rolling_windows"]:
                    features[f"{clean_name}_avg{window}"] = volume * 0.9  # Approximation
            
            # Total mail features
            total_mail = sum(mail_input.get(mt, 0) for mt in self.top_mail_types)
            features['total_mail_today'] = total_mail
            
            for lag in [1, 2, 3]:
                features[f'total_mail_lag{lag}'] = total_mail * 0.8 ** lag
            
            for window in CONFIG["rolling_windows"]:
                features[f'total_mail_avg{window}'] = total_mail * 0.9
            
            # Call history (use provided or defaults)
            if call_history:
                features['calls_yesterday'] = call_history.get('yesterday', 12000)
                features['calls_2days_ago'] = call_history.get('2_days_ago', 12000)
            else:
                features['calls_yesterday'] = 12000  # Default average
                features['calls_2days_ago'] = 12000
            
            for window in CONFIG["rolling_windows"]:
                features[f'calls_avg{window}'] = features['calls_yesterday']
            
            # Temporal features (use current date)
            now = datetime.now()
            features['weekday'] = now.weekday()
            features['month'] = now.month
            features['day_of_month'] = now.day
            features['is_month_end'] = 1 if now.day >= 25 else 0
            
            # Convert to array (match training feature order)
            feature_vector = []
            for feat_name in self.features:
                feature_vector.append(features.get(feat_name, 0))
            
            # Predict
            prediction = self.model.predict([feature_vector])[0]
            prediction = max(0, round(prediction))
            
            return {
                'predicted_calls': int(prediction),
                'prediction_lag_days': self.best_lag,
                'model_used': self.model_name,
                'mail_input': mail_input,
                'total_mail_volume': int(total_mail),
                'status': 'success'
            }
            
        except Exception as e:
            return {'error': str(e), 'status': 'failed'}


    # ============================================================================

# MAIN ORCHESTRATOR

# ============================================================================

def main():


    safe_print("COMPREHENSIVE MAIL-TO-CALLS PREDICTION SYSTEM")
    safe_print("=" * 80)
    safe_print("APPROACH:")
    safe_print("1. Load clean call data (Date, ACDCalls) + mail data")
    safe_print("2. Comprehensive EDA with plots and correlations") 
    safe_print("3. Feature engineering with proper lag analysis")
    safe_print("4. Simple models first, evaluate thoroughly")
    safe_print("5. Build prediction system for daily/weekly inputs")
    safe_print("=" * 80)

    try:
        # STEP 1: Data Loading
        data_manager = DataManager()
        call_data = data_manager.load_call_data()
        mail_data = data_manager.load_mail_data()
        merged_data = data_manager.merge_data()
        
        if len(merged_data) < 50:
            safe_print(f"⚠️  WARNING: Only {len(merged_data)} days of data")
            safe_print("   Results may be unreliable with limited data")
        
        # STEP 2: Comprehensive EDA
        eda_analyzer = EDATrendAnalysis(merged_data, data_manager.output_dir)
        eda_results = eda_analyzer.run_full_eda()
        
        # STEP 3: Feature Engineering
        feature_engineer = FeatureEngineer(
            merged_data, 
            eda_results['top_mail_types'], 
            eda_results['best_lag']
        )
        X, y, dates = feature_engineer.create_features()
        
        if len(X) < 30:
            safe_print(f"⚠️  WARNING: Only {len(X)} samples for modeling")
            safe_print("   Consider collecting more data for better results")
        
        # STEP 4: Model Training
        model_builder = ModelBuilder(data_manager.output_dir)
        best_model, best_name, results = model_builder.train_simple_models(X, y, dates)
        
        if not best_model:
            safe_print("\n❌ MODELING FAILED - NO ACCEPTABLE MODEL FOUND")
            safe_print("\nPOSSIBLE REASONS:")
            safe_print("1. Insufficient data (need more days)")
            safe_print("2. Weak relationship between mail and calls")
            safe_print("3. Too much noise in the data")
            safe_print("4. Need different feature engineering approach")
            return {'success': False, 'error': 'No acceptable model found'}
        
        # STEP 5: Create Prediction System
        model_info = {
            'model': best_model,
            'model_name': best_name,
            'features': X.columns.tolist(),
            'performance': results[best_name]
        }
        
        prediction_system = PredictionSystem(
            model_info, 
            eda_results['top_mail_types'], 
            eda_results['best_lag'][0]
        )
        
        # Test the prediction system
        safe_print("\n" + "=" * 80)
        safe_print("STEP 5: TESTING PREDICTION SYSTEM")
        safe_print("=" * 80)
        
        # Create realistic test input
        test_mail_input = {}
        for i, mail_type in enumerate(eda_results['top_mail_types'][:5]):
            test_mail_input[mail_type] = [2000, 1500, 1200, 1000, 800][i]
        
        test_result = prediction_system.predict_calls(test_mail_input)
        
        if test_result['status'] == 'success':
            safe_print("✅ PREDICTION TEST SUCCESSFUL!")
            safe_print(f"   Mail Input: {test_mail_input}")
            safe_print(f"   Predicted Calls (+{test_result['prediction_lag_days']} days): {test_result['predicted_calls']:,}")
            safe_print(f"   Total Mail Volume: {test_result['total_mail_volume']:,}")
            safe_print(f"   Model Used: {test_result['model_used']}")
        else:
            safe_print(f"❌ PREDICTION TEST FAILED: {test_result['error']}")
        
        # Create usage guide
        usage_guide = f"""


    # MAIL-TO-CALLS PREDICTION SYSTEM - USAGE GUIDE

    SYSTEM PERFORMANCE:

    - Best Model: {best_name}
    - Test R²: {results[best_name]['test_r2']:.3f}
    - Test MAE: {results[best_name]['test_mae']:.0f} calls
    - Test MAPE: {results[best_name]['test_mape']:.1f}%
    - Prediction Lag: {eda_results['best_lag'][0]} days

    ## USAGE EXAMPLE:

    import joblib

    # Load model

    model_info = joblib.load('{data_manager.output_dir}/models/best_model.pkl')

    # Predict calls from mail volumes

    mail_today = {{
    {chr(10).join([f"    '{mt}': 1500," for mt in eda_results['top_mail_types'][:3]])}
    }}

    # This predicts calls {eda_results['best_lag'][0]} days in the future

    prediction = prediction_system.predict_calls(mail_today)
    print(f"Predicted calls: {{prediction['predicted_calls']}}")

    TOP MAIL TYPES (by volume):
    {chr(10).join([f"{i+1:2d}. {mt}" for i, mt in enumerate(eda_results['top_mail_types'])])}

    FILES GENERATED:

    - models/best_model.pkl: Trained model
    - models/model_validation.png: Performance plots
    - eda_plots/: All EDA visualizations
    - USAGE_GUIDE.txt: This guide
    """
    
    
        with open(data_manager.output_dir / "USAGE_GUIDE.txt", 'w') as f:
            f.write(usage_guide)
        
        # Final summary
        safe_print("\n" + "=" * 80)
        safe_print("🎯 SUCCESS! COMPREHENSIVE SYSTEM DEPLOYED!")
        safe_print("=" * 80)
        safe_print(f"✅ Data: {len(merged_data)} days merged successfully")
        safe_print(f"✅ EDA: Full analysis with {len(eda_results['top_mail_types'])} top mail types")
        safe_print(f"✅ Features: {len(X.columns)} engineered features")
        safe_print(f"✅ Model: {best_name} (R²={results[best_name]['test_r2']:.3f})")
        safe_print(f"✅ Lag: {eda_results['best_lag'][0]} days optimal")
        safe_print(f"✅ Files: Saved to {data_manager.output_dir}/")
        safe_print("")
        safe_print("PREDICTION CAPABILITY:")
        safe_print("- Input: Daily mail volumes by type")
        safe_print(f"- Output: Call volume {eda_results['best_lag'][0]} days ahead")
        safe_print("- Use cases: Workforce planning, capacity management")
        safe_print("")
        safe_print("NEXT STEPS:")
        safe_print("1. Review EDA plots for insights")
        safe_print("2. Test with your own mail volume inputs")
        safe_print("3. Monitor prediction accuracy over time")
        safe_print("4. Retrain with more data as available")
        
        return {
            'success': True,
            'best_model': best_model,
            'model_name': best_name,
            'performance': results[best_name],
            'prediction_system': prediction_system,
            'top_mail_types': eda_results['top_mail_types'],
            'best_lag': eda_results['best_lag'],
            'output_dir': str(data_manager.output_dir)
        }
    
    
    except Exception as e:
        safe_print(f"❌ SYSTEM ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

if __name__ == "__main__":
    result = main()
    if result['success']:
        safe_print("🚀 MAIL-TO-CALLS PREDICTION SYSTEM READY FOR PRODUCTION!")
    else:
        safe_print(f"💥 SYSTEM FAILED: {result['error']}")







# File: testing.py (Corrected Version)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib
import logging
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.base import clone

# ============================================================================
# 1. CONFIGURATION
# ============================================================================
CONFIG = {
    "model_path": "mail_call_prediction_system/models/best_model.pkl",
    "output_dir": "mail_call_prediction_system/rigorous_test_results",
    # Data files needed to recreate the original feature set
    "call_file": "ACDMail.csv",
    "mail_file": "mail.csv",
    "holidays_file": "us_holidays.csv",
    # Test Settings
    "cv_splits": 5,
    "top_n_features": 20,
    # Settings from your original training script (must match)
    "top_mail_types_count": 8,
    "rolling_windows": [3, 7],
    "best_lag": 6, # From your EDA output
}

# ============================================================================
# 2. LOGGING AND SETUP
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

output_path = Path(CONFIG["output_dir"])
output_path.mkdir(exist_ok=True)
logging.info(f"Results will be saved to: {output_path.resolve()}")


# ============================================================================
# 3. DATA PREPARATION FUNCTION
# ============================================================================
def load_and_prepare_data():
    """
    Loads data and re-engineers features to EXACTLY match the training environment.
    """
    logging.info("Loading and preparing data...")
    try:
        # --- Load and merge data ---
        calls = pd.read_csv(CONFIG["call_file"])[['Date', 'ACDCalls']].rename(columns={'Date': 'date', 'ACDCalls': 'call_volume'})
        calls['date'] = pd.to_datetime(calls['date'])

        mail = pd.read_csv(CONFIG["mail_file"])
        mail['mail_date'] = pd.to_datetime(mail['mail_date'])
        mail = mail.rename(columns={'mail_date': 'date', 'mail_volume': 'volume', 'mail_type': 'type'})
        mail_pivot = mail.pivot_table(index='date', columns='type', values='volume', aggfunc='sum').fillna(0).reset_index()

        merged_data = pd.merge(calls, mail_pivot, on='date', how='inner')
        merged_data = merged_data.sort_values('date').reset_index(drop=True)
        logging.info("All data loaded and merged successfully.")

        # --- Recreate Features with Original Logic ---
        logging.info("Recreating features to match the trained model...")
        features_list = []
        targets_list = []
        
        top_mail_types = mail_pivot.drop(columns='date').sum().nlargest(CONFIG["top_mail_types_count"]).index.tolist()
        best_lag = CONFIG["best_lag"]
        max_lookback = max(CONFIG["rolling_windows"] + [best_lag])

        for i in range(max_lookback, len(merged_data) - best_lag):
            feature_row = {}
            current_date = merged_data.iloc[i]['date']

            # Mail Features (Original Logic)
            for mail_type in top_mail_types:
                clean_name = mail_type.replace(' ', '').replace('-', '').replace('_', '')[:15]
                feature_row[f"{clean_name}_today"] = merged_data.iloc[i][mail_type]
                for lag in [1, 2, 3]: feature_row[f"{clean_name}_lag{lag}"] = merged_data.iloc[i - lag][mail_type]
                for window in CONFIG["rolling_windows"]: feature_row[f"{clean_name}_avg{window}"] = merged_data[mail_type].iloc[i-window+1:i+1].mean()
            
            # Total Mail Features (Original Logic)
            total_mail_today = sum(merged_data.iloc[i][mt] for mt in top_mail_types)
            feature_row['total_mail_today'] = total_mail_today
            for lag in [1, 2, 3]: feature_row[f'total_mail_lag{lag}'] = sum(merged_data.iloc[i - lag][mt] for mt in top_mail_types)
            for window in CONFIG["rolling_windows"]: feature_row[f'total_mail_avg{window}'] = np.mean([sum(merged_data.iloc[j][mt] for mt in top_mail_types) for j in range(i-window+1, i+1)])

            # Call History Features (Original Logic)
            feature_row['calls_yesterday'] = merged_data.iloc[i - 1]['call_volume']
            feature_row['calls_2days_ago'] = merged_data.iloc[i - 2]['call_volume']
            for window in CONFIG["rolling_windows"]: feature_row[f'calls_avg{window}'] = merged_data['call_volume'].iloc[i-window+1:i+1].mean()

            # Temporal Features (Original Logic)
            feature_row['weekday'] = current_date.weekday()
            #feature_row['month'] = current_date.month
            feature_row['day_of_month'] = current_date.day
            feature_row['is_month_end'] = 1 if current_date.day >= 25 else 0

            features_list.append(feature_row)
            targets_list.append(merged_data.iloc[i + best_lag]['call_volume'])

        X = pd.DataFrame(features_list).fillna(0)
        y = pd.Series(targets_list, name='call_volume')
        
        logging.info(f"Feature set recreated with {len(X)} samples and {len(X.columns)} features.")
        return X, y

    except FileNotFoundError as e:
        logging.error(f"Data file not found: {e}. Cannot proceed with testing.")
        return None, None
    except Exception as e:
        logging.error(f"An error occurred during data preparation: {e}", exc_info=True)
        return None, None

# ============================================================================
# 4. TESTING FUNCTIONS (Unchanged)
# ============================================================================
def test_time_series_cv(model, X, y):
    """Performs Time Series Cross-Validation to get a stable measure of model performance."""
    logging.info("--- Starting Time Series Cross-Validation ---")
    tscv = TimeSeriesSplit(n_splits=CONFIG["cv_splits"])
    r2_scores, mae_scores = [], []

    for i, (train_index, test_index) in enumerate(tscv.split(X)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        model_clone = clone(model)
        model_clone.fit(X_train, y_train)
        preds = model_clone.predict(X_test)

        r2 = r2_score(y_test, preds)
        mae = mean_absolute_error(y_test, preds)
        r2_scores.append(r2)
        mae_scores.append(mae)
        logging.info(f"Fold {i+1}/{CONFIG['cv_splits']} | Test R²: {r2:.3f} | Test MAE: {mae:.2f}")

    logging.info("--- Cross-Validation Summary ---")
    logging.info(f"Average R²: {np.mean(r2_scores):.3f} (Std: {np.std(r2_scores):.3f})")
    logging.info(f"Average MAE: {np.mean(mae_scores):.2f} (Std: {np.std(mae_scores):.2f})")

def analyze_feature_importance(model, features):
    """Extracts, plots, and logs the model's feature importances."""
    logging.info("--- Analyzing Feature Importance ---")
    if not hasattr(model, 'feature_importances_'):
        logging.warning("This model type does not support feature_importances_.")
        return

    importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)
    
    logging.info("Top 10 most important features:")
    for i, (name, importance) in enumerate(importances.head(10).items()):
        logging.info(f"{i+1:2d}. {name:<30} | Importance: {importance:.4f}")

    plt.figure(figsize=(12, 8))
    importances.head(CONFIG["top_n_features"]).sort_values().plot(kind='barh', color='skyblue')
    plt.title(f'Top {CONFIG["top_n_features"]} Feature Importances')
    plt.xlabel('Importance')
    plt.tight_layout()
    plot_path = output_path / "feature_importance.png"
    plt.savefig(plot_path)
    plt.close()
    logging.info(f"Feature importance plot saved to {plot_path}")

def analyze_error_by_day(model, X, y):
    """Analyzes model error broken down by the day of the week."""
    logging.info("--- Analyzing Error by Day of the Week ---")
    
    split_idx = int(len(X) * 0.75)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    model_clone = clone(model)
    model_clone.fit(X_train, y_train)
    preds = model_clone.predict(X_test)

    results_df = pd.DataFrame({'actual': y_test, 'predicted': preds, 'weekday': X_test['weekday']})
    results_df['absolute_error'] = (results_df['actual'] - results_df['predicted']).abs()

    mae_by_day = results_df.groupby('weekday')['absolute_error'].mean()
    day_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri'}
    mae_by_day.index = mae_by_day.index.map(day_map)

    logging.info("Mean Absolute Error by Day of Week:")
    for day, mae in mae_by_day.items():
        logging.info(f"   {day}: {mae:.2f}")
    
    plt.figure(figsize=(10, 6))
    mae_by_day.plot(kind='bar', color='coral')
    plt.title('Mean Absolute Error by Day of Week')
    plt.ylabel('Mean Absolute Error (Calls)')
    plt.xticks(rotation=0)
    plt.tight_layout()
    plot_path = output_path / "error_by_day.png"
    plt.savefig(plot_path)
    plt.close()
    logging.info(f"Error analysis plot saved to {plot_path}")

# ============================================================================
# 5. MAIN ORCHESTRATOR
# ============================================================================
def main():
    """Main function to run all tests."""
    logging.info("Starting Rigorous Model Testing...")

    try:
        model_path = Path(CONFIG["model_path"])
        model_info = joblib.load(model_path)
        model = model_info['model']
        model_features = model_info['features']
        logging.info(f"Successfully loaded model '{model_info['model_name']}' from {model_path}")
    except FileNotFoundError:
        logging.error(f"Model file not found at {CONFIG['model_path']}. Aborting.")
        return

    X, y = load_and_prepare_data()
    if X is None:
        return

    if set(X.columns) != set(model_features):
        logging.error("FATAL: Features created for testing do not match features the model was trained on.")
        logging.error(f"Model needs {len(model_features)} features, but test data has {len(X.columns)}.")
        missing_in_data = set(model_features) - set(X.columns)
        extra_in_data = set(X.columns) - set(model_features)
        if missing_in_data: logging.error(f"Missing from data: {missing_in_data}")
        if extra_in_data: logging.error(f"Extra in data: {extra_in_data}")
        return

    X = X[model_features]

    test_time_series_cv(model, X, y)
    analyze_feature_importance(model, model_features)
    analyze_error_by_day(model, X, y)
    
    logging.info("Rigorous testing complete.")

if __name__ == "__main__":
    main()






















rD/OneDrive - Computershare/Desktop/acdmodel/acd.py"
COMPREHENSIVE MAIL-TO-CALLS PREDICTION SYSTEM
================================================================================
APPROACH:
1. Load clean call data (Date, ACDCalls) + mail data
2. Comprehensive EDA with plots and correlations
3. Feature engineering with proper lag analysis
4. Simple models first, evaluate thoroughly
5. Build prediction system for daily/weekly inputs
================================================================================
================================================================================
STEP 1A: LOADING CLEAN CALL DATA
================================================================================
 Loading: ACDMail.csv
   Loaded with utf-8 encoding
   Raw data: 547 rows
   Columns: ['Date', 'Product', 'ACDCalls']
   Removing US holidays from call data using CSV file...
   Found 2 US holidays to remove:
     - 2024-10-14: Columbus Day
     - 2024-11-11: Veterans Day
   Removed 2 holiday rows.
   Data after holiday removal: 372 rows.
REMOVEDDDDHOLIDAYSSSSSSSSSSS
 Clean call data: 372 business days
   Date range: 2024-01-02 to 2025-06-30
   Call volume: 2280 to 15764
   Daily average: 10020 calls

================================================================================
STEP 1B: LOADING MAIL DATA
================================================================================
 Loading: mail.csv
   Loaded with utf-8 encoding
   Raw data: 1,409,780 rows, 4 columns
   Using: date=mail_date, volume=mail_volume, type=mail_type
 Clean mail data: 401 business days
   Date range: 2023-08-01 to 2025-05-30
   Mail types: 228

================================================================================
STEP 1C: MERGING CALL AND MAIL DATA
================================================================================
   Call data: 372 days
   Mail data: 401 days
   Common dates: 337 days
 Merged dataset: 337 days
   Columns: 230 (date + calls + 228 mail types)
   Date range: 2024-01-02 to 2025-05-30

================================================================================
STEP 2: COMPREHENSIVE EDA AND VISUALIZATION
================================================================================

--- Creating Overview Plots ---
   Overall correlation: -0.019

--- Creating Time Series Plots ---

--- Analyzing Correlations ---
   Top 10 correlations with call volume:
    1. Elig_Enr_DedChg_Ltr           : r= -0.349
    2. Sales_Chk_CPM                 : r= -0.305
    3. Sales_Wire_CPM                : r= -0.268
    4. Award_Status                  : r= -0.268
    5. Bank_Account_Update           : r= -0.254
    6. BROKER_TRANSFER               : r= -0.245
    7. Sales_Wire                    : r= -0.240
    8. MultiClientLaser              : r=  0.218
    9. MultiClientLodgeCourier       : r=  0.218
   10. RecordsProcessing             : r=  0.218

--- Analyzing Mail Types ---
   Top 8 mail types by volume:
    1. Cheque                   : 13,495,552 total, r=-0.093
    2. DRP Stmt.                : 12,132,767 total, r= 0.001
    3. Scheduled PAYMENT CHECKS : 9,894,910 total, r=-0.052
    4. Envision                 : 5,829,010 total, r= 0.058
    5. Proxy (US)               : 5,703,365 total, r= 0.087
    6. Notice                   : 4,924,059 total, r= 0.041
    7. Cheque 1099              : 4,018,239 total, r=-0.079
    8. DRP 1099                 : 3,613,191 total, r=-0.074

--- Analyzing Lag Relationships ---
   Lag 0 days: correlation = -0.019
   Lag 1 days: correlation = -0.019
   Lag 2 days: correlation = -0.019
   Lag 3 days: correlation = -0.020
   Lag 4 days: correlation = -0.020
   Lag 5 days: correlation = -0.021
   Lag 6 days: correlation = -0.022
   Lag 7 days: correlation = -0.022
   Best lag: 6 days (correlation: -0.022)

 EDA Complete! Plots saved to: mail_call_prediction_system\eda_plots

================================================================================
STEP 3: FEATURE ENGINEERING
================================================================================
   Using lag: 6 days
   Top mail types: 8
 Created 61 features from 324 samples
   Mail features: 48
   Call history: 4
   Temporal: 14

================================================================================
STEP 4: SIMPLE MODEL TRAINING
================================================================================
   Train: 243 samples
   Test: 81 samples

--- Testing linear ---
   Train R: 0.663
   Test R:  -115.726
   Test MAE: 10828
   Test MAPE: 104.8%
   Overfitting: 116.389

--- Testing ridge_light ---
   Train R: 0.663
   Test R:  -114.403
   Test MAE: 10753
   Test MAPE: 104.0%
   Overfitting: 115.066

--- Testing ridge_strong ---
   Train R: 0.663
   Test R:  -107.201
   Test MAE: 10334
   Test MAPE: 100.1%
   Overfitting: 107.863

--- Testing forest_simple ---
   Train R: 0.819
   Test R:  0.528
   Test MAE: 1146
   Test MAPE: 11.2%
   Overfitting: 0.291
    NEW BEST! (Score: 0.433)

 BEST MODEL: forest_simple
   Test R: 0.528
   Test MAE: 1146
   Test MAPE: 11.2%

--- Creating Model Validation Plots ---
   Validation plots saved: mail_call_prediction_system\models/model_validation.png

================================================================================
STEP 5: TESTING PREDICTION SYSTEM
================================================================================
 PREDICTION TEST SUCCESSFUL!
   Mail Input: {'Cheque': 2000, 'DRP Stmt.': 1500, 'Scheduled PAYMENT CHECKS': 1200, 'Envision': 1000, 'Proxy (US)': 800}
   Predicted Calls (+6 days): 12,957
   Total Mail Volume: 6,500
   Model Used: forest_simple

================================================================================
 SUCCESS! COMPREHENSIVE SYSTEM DEPLOYED!
================================================================================
 Data: 337 days merged successfully
 EDA: Full analysis with 8 top mail types
 Features: 61 engineered features
 Model: forest_simple (R=0.528)
 Lag: 6 days optimal
 Files: Saved to mail_call_prediction_system/

PREDICTION CAPABILITY:
- Input: Daily mail volumes by type
- Output: Call volume 6 days ahead
- Use cases: Workforce planning, capacity management

NEXT STEPS:
1. Review EDA plots for insights
2. Test with your own mail volume inputs
3. Monitor prediction accuracy over time
4. Retrain with more data as available
 MAIL-TO-CALLS PREDICTION SYSTEM READY FOR PRODUCTION!
PS C:\Users\BhungarD\OneDrive - Computershare\Desktop\acdmodel> C:\Users\BhungarD\python.exe "c:/Users/BhungarD/OneDrive - Computershare/Desktop/acdmodel/testing.py"
2025-07-24 23:26:16 [INFO] - Results will be saved to: C:\Users\BhungarD\OneDrive - Computershare\Desktop\acdmodel\mail_call_prediction_system\rigorous_test_results
2025-07-24 23:26:16 [INFO] - Starting Rigorous Model Testing...
2025-07-24 23:26:16 [INFO] - Successfully loaded model 'forest_simple' from mail_call_prediction_system\models\best_model.pkl
2025-07-24 23:26:16 [INFO] - Loading and preparing data...
2025-07-24 23:26:17 [INFO] - All data loaded and merged successfully.
2025-07-24 23:26:17 [INFO] - Recreating features to match the trained model...
2025-07-24 23:26:20 [INFO] - Feature set recreated with 264 samples and 61 features.
2025-07-24 23:26:20 [INFO] - --- Starting Time Series Cross-Validation ---
2025-07-24 23:26:20 [INFO] - Fold 1/5 | Test R²: -0.493 | Test MAE: 1204.61
2025-07-24 23:26:20 [INFO] - Fold 2/5 | Test R²: -0.116 | Test MAE: 1375.86
2025-07-24 23:26:20 [INFO] - Fold 3/5 | Test R²: -0.121 | Test MAE: 4181.09
2025-07-24 23:26:21 [INFO] - Fold 4/5 | Test R²: -0.051 | Test MAE: 3858.96
2025-07-24 23:26:21 [INFO] - Fold 5/5 | Test R²: -0.184 | Test MAE: 1561.48
2025-07-24 23:26:21 [INFO] - --- Cross-Validation Summary ---
2025-07-24 23:26:21 [INFO] - Average R²: -0.193 (Std: 0.156)
2025-07-24 23:26:21 [INFO] - Average MAE: 2436.40 (Std: 1301.94)
2025-07-24 23:26:21 [INFO] - --- Analyzing Feature Importance ---
2025-07-24 23:26:21 [INFO] - Top 10 most important features:
2025-07-24 23:26:21 [INFO] -  1. calls_avg3                     | Importance: 0.4150
2025-07-24 23:26:21 [INFO] -  2. calls_avg7                     | Importance: 0.2889
2025-07-24 23:26:21 [INFO] -  3. calls_yesterday                | Importance: 0.0358
2025-07-24 23:26:21 [INFO] -  4. Proxy(US)_lag3                 | Importance: 0.0202
2025-07-24 23:26:21 [INFO] -  5. calls_2days_ago                | Importance: 0.0186
2025-07-24 23:26:21 [INFO] -  6. Proxy(US)_avg7                 | Importance: 0.0167
2025-07-24 23:26:21 [INFO] -  7. weekday                        | Importance: 0.0115
2025-07-24 23:26:21 [INFO] -  8. total_mail_lag1                | Importance: 0.0114
2025-07-24 23:26:21 [INFO] -  9. day_of_month                   | Importance: 0.0112
2025-07-24 23:26:21 [INFO] - 10. Envision_avg7                  | Importance: 0.0104
2025-07-24 23:26:21 [INFO] - Feature importance plot saved to mail_call_prediction_system\rigorous_test_results\feature_importance.png
2025-07-24 23:26:21 [INFO] - --- Analyzing Error by Day of the Week ---
2025-07-24 23:26:21 [INFO] - Mean Absolute Error by Day of Week:
2025-07-24 23:26:21 [INFO] -    Mon: 1758.32
2025-07-24 23:26:21 [INFO] -    Tue: 1925.19
2025-07-24 23:26:21 [INFO] -    Wed: 1325.15
2025-07-24 23:26:21 [INFO] -    Thu: 2568.14
2025-07-24 23:26:21 [INFO] -    Fri: 1431.79
2025-07-24 23:26:21 [INFO] -    nan: 5368.64
2025-07-24 23:26:21 [INFO] -    nan: 7498.21
2025-07-24 23:26:21 [INFO] - Error analysis plot saved to mail_call_prediction_system\rigorous_test_results\error_by_day.png
2025-07-24 23:26:21 [INFO] - Rigorous testing complete.
