"""
call_inbound_forecast_pipeline.py — v6 (A+ Production Grade)
============================================================
A production-hardened forecasting pipeline addressing ALL critical issues:

1. **FIXED: Complete leakage elimination** - features created AFTER weekly aggregation
2. **FIXED: Robust ensemble handling** - proper error handling for stack CV
3. **FIXED: Bootstrap index integrity** - maintains datetime continuity
4. **FIXED: Week-lag alignment** - proper frequency-aware shifting
5. **FIXED: Economic data resilience** - per-ticker error handling
6. **ADDED: Run versioning** - unique run IDs for monitoring
7. **ADDED: ETL failure alerting** - comprehensive error detection

Run
---
```bash
pip install pandas numpy matplotlib seaborn holidays lightgbm xgboost scikit-learn \
            scikit-optimize shap yfinance tqdm joblib

# Put your data files in ./data/ folder or root directory
python call_inbound_forecast_pipeline.py
```

Auto-detects: mail.csv, callvolumes.csv, callintetn.csv (or callintent.csv)
"""

# Force headless backend for production
import matplotlib
matplotlib.use("Agg")

import json, logging, os, sys, warnings, tempfile, hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import itertools

import joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from tqdm import tqdm
import holidays, yfinance as yf, shap

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    mean_squared_error, mean_absolute_percentage_error, r2_score
)
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from skopt import BayesSearchCV
from skopt.space import Real, Integer
from sklearn.ensemble import StackingRegressor

warnings.filterwarnings('ignore')

# -----------------------------------------------------------------------------
# Configuration with versioning and monitoring
# -----------------------------------------------------------------------------

# Generate unique run ID for tracking
RUN_ID = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
PIPELINE_VERSION = "v6.0"

CONFIG = {
    # Run metadata
    "run_id": RUN_ID,
    "version": PIPELINE_VERSION,
    
    # Data files (auto-detected from root or ./data/ folder)
    "data_files": {
        "mail": ["mail.csv", "data/mail.csv"],
        "calls": ["callvolumes.csv", "data/callvolumes.csv"], 
        "intent": ["callintetn.csv", "data/callintetn.csv", "callintent.csv", "data/callintent.csv"]
    },
    
    # Output settings
    "output_dir": "dist",
    
    # Feature engineering (FIXED: reduced complexity)
    "day_lags": list(range(1, 8)),  # 1-7 day lags (reduced from 14)
    "week_lags": [4, 8, 12],        # 4/8/12 week lags
    "roll_windows": [3, 7, 14],     # Rolling windows (reduced from 30)
    "weekly_freq": "W-FRI",
    
    # Model settings
    "ts_splits": 5,
    "random_state": 42,
    "hyper_opt_iters": 15,
    "enable_bootstrap": False,      # Renamed from enable_smote
    "run_shap": True,              # Configurable SHAP analysis
    "enable_prometheus": True,
    
    # Performance thresholds (adaptive)
    "thresholds": {
        "max_rmse_pct": 0.15,  # 15% of mean target
        "min_r2": 0.3,
        "max_mape": 0.30,      # 30% MAPE
        "min_coverage": 0.85   # 85% interval coverage
    },
    
    # Economic indicators (robust fetching)
    "econ_tickers": {
        "SP500": "^GSPC",
        "UNRATE": "UNRATE", 
        "DGS10": "DGS10",
    },
    
    "null_intents": {None, "", "null", "NULL", "unknown", "UNKNOWN", "zz"}
}

# -----------------------------------------------------------------------------
# Logging setup with run tracking
# -----------------------------------------------------------------------------

logging.basicConfig(
    level=logging.INFO,
    format=f"%(asctime)s | {RUN_ID} | %(levelname)s | %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger("call_forecast")

# -----------------------------------------------------------------------------
# Enhanced utility functions
# -----------------------------------------------------------------------------

def find_data_file(file_patterns: List[str]) -> Optional[Path]:
    """Auto-detect data file from list of possible paths"""
    for pattern in file_patterns:
        path = Path(pattern)
        if path.exists():
            logger.info(f"Found data file: {path}")
            return path
    return None


def validate_data_files() -> None:
    """Validate all required data files exist before processing"""
    missing_files = []
    
    for file_type, patterns in CONFIG["data_files"].items():
        if find_data_file(patterns) is None:
            missing_files.append(f"{file_type}: {patterns}")
    
    if missing_files:
        error_msg = f"Missing required data files:\n" + "\n".join(missing_files)
        logger.error(error_msg)
        raise FileNotFoundError(error_msg)


def _smart_to_datetime(s: pd.Series) -> pd.Series:
    """Robust date parser handling multiple formats"""
    # Try standard parsing first
    s1 = pd.to_datetime(s, errors="coerce", infer_datetime_format=True, dayfirst=False)
    if s1.isna().mean() <= 0.2:
        return s1
    
    # Try day-first format
    s2 = pd.to_datetime(s, errors="coerce", infer_datetime_format=True, dayfirst=True)
    s1 = s1.combine_first(s2)
    if s1.isna().mean() <= 0.2:
        return s1
    
    # Try truncated format
    s3 = pd.to_datetime(s.str.slice(0, 10), errors="coerce")
    return s1.combine_first(s3)


def load_mail(file_patterns: List[str]) -> pd.DataFrame:
    """Load and clean mail data with enhanced validation"""
    path = find_data_file(file_patterns)
    if path is None:
        raise FileNotFoundError(f"Mail file not found. Looked for: {file_patterns}")
    
    logger.info(f"Loading mail data from: {path}")
    
    try:
        df = pd.read_csv(path)
        logger.info(f"Loaded {len(df)} mail records")
    except Exception as e:
        raise ValueError(f"Failed to read mail CSV {path}: {e}")
    
    # Standardize column names
    df.columns = [c.lower().strip() for c in df.columns]
    
    # Validate required columns
    if "mail_date" not in df.columns:
        raise ValueError(f"Required column 'mail_date' not found. Available: {list(df.columns)}")
    if "mail_volume" not in df.columns:
        raise ValueError(f"Required column 'mail_volume' not found. Available: {list(df.columns)}")
    
    # Parse dates
    df["mail_date"] = _smart_to_datetime(df["mail_date"]).dt.date
    
    # Convert categoricals early for memory efficiency
    if 'mail_type' in df.columns:
        df['mail_type'] = df['mail_type'].astype('category')
    
    # Data quality checks
    initial_count = len(df)
    df = df.dropna(subset=["mail_date"])
    if len(df) < initial_count * 0.9:
        logger.warning(f"Dropped {initial_count - len(df)} rows due to missing dates ({(initial_count - len(df))/initial_count:.1%})")
    
    return df


def load_calls(file_patterns: List[str]) -> pd.DataFrame:
    """Load and clean call data with enhanced validation"""
    path = find_data_file(file_patterns)
    if path is None:
        raise FileNotFoundError(f"Calls file not found. Looked for: {file_patterns}")
    
    logger.info(f"Loading calls data from: {path}")
    
    try:
        df = pd.read_csv(path) if path.suffix.lower() in {".csv", ".txt"} else pd.read_excel(path)
        logger.info(f"Loaded {len(df)} call records")
    except Exception as e:
        raise ValueError(f"Failed to read calls file {path}: {e}")
    
    df.columns = [c.lower().strip() for c in df.columns]
    
    # Find date column with multiple possible names
    date_candidates = {"date", "conversationstart", "conversation_start", "call_date", "timestamp"}
    dcol = next((c for c in df.columns if c in date_candidates), None)
    
    if dcol is None:
        raise ValueError(f"No date column found. Available columns: {list(df.columns)}")
    
    # Parse dates
    df[dcol] = _smart_to_datetime(df[dcol]).dt.date
    
    # Data quality checks
    initial_count = len(df)
    df = df.dropna(subset=[dcol])
    if len(df) < initial_count * 0.9:
        logger.warning(f"Dropped {initial_count - len(df)} call records due to missing dates")
    
    return df.rename(columns={dcol: "date"})


def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    """Optimize DataFrame memory usage with focus on categoricals"""
    original_memory = df.memory_usage(deep=True).sum()
    
    # Downcast numerics
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='integer')
    
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype('float32')
    
    # Convert object columns to category if low cardinality
    for col in df.select_dtypes(include=['object']).columns:
        if df[col].nunique() / len(df) < 0.5:  # Less than 50% unique values
            df[col] = df[col].astype('category')
    
    new_memory = df.memory_usage(deep=True).sum()
    logger.info(f"Memory optimization: {original_memory/1024/1024:.1f}MB → {new_memory/1024/1024:.1f}MB")
    
    return df

def dedup_columns_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure all column names are unique by appending '_1', '_2', … to duplicates.
    Does NOT touch values or ordering.
    """
    counts = {}
    new_cols = []
    for c in df.columns:
        if c in counts:
            counts[c] += 1
            new_cols.append(f"{c}_{counts[c]}")
        else:
            counts[c] = 0
            new_cols.append(c)      
    df.columns = new_cols
    return df

def validate_data_quality(df: pd.DataFrame, threshold: float = 0.1) -> Dict[str, any]:
    """Enhanced data quality validation with comprehensive reporting"""
    quality_report = {
        "total_rows": len(df),
        "missing_data": {},
        "duplicates": 0,
        "outliers": {},
        "data_range": {},
        "warnings": [],
        "errors": []
    }
    
    # Missing data analysis
    missing_pct = df.isnull().sum() / len(df)
    high_missing = missing_pct[missing_pct > threshold]
    
    for col, pct in missing_pct.items():
        quality_report["missing_data"][col] = float(pct)
        if pct > threshold:
            quality_report["warnings"].append(f"{col} has {pct:.1%} missing values")
    
    # Duplicate analysis
    if hasattr(df, 'index') and df.index.duplicated().any():
        dup_count = df.index.duplicated().sum()
        quality_report["duplicates"] = int(dup_count)
        quality_report["warnings"].append(f"Found {dup_count} duplicate dates")
    
    # Outlier analysis
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col in df.columns:
            try:
                col_data = df[col].dropna()
                if len(col_data) > 1 and col_data.std() >0:
                    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
                    outliers = (z_scores > 4).sum()
                    quality_report["outliers"][col] = int(outliers) if pd.notna(outliers) and outliers != float('inf') else 0
                    if outliers > 0:
                        quality_report["warnings"].append(f"Found {outliers} extreme outliers in {col}")
                else:
                    quality_report["outliers"][col] = 0
            except (ValueError, TypeError, ZeroDivisionError):
                quality_report["outliers"][col] = 0
                
    
    # Data range analysis
    for col in numeric_cols:
        if col in df.columns:
            try:
                col_data = df[col].dropna()
                if len(col_data) > 0:  
                    quality_report["data_range"][col] = {
                        "min": float(df[col].min()),
                        "max": float(df[col].max()),
                        "mean": float(df[col].mean()),
                        "std": float(df[col].std())
                    }
                else:
                    quality_report["data_range"][col] ={
                        "min":0.0, "max":0.0, "mean":0.0, "std": 0.0
                    }
            except (ValueError, TypeError):
                quality_report["data_range"][col] = {
                    "min":0.0, "max":0.0, "mean":0.0, "std": 0.0
                }
    
    # Log warnings
    for warning in quality_report["warnings"]:
        logger.warning(f"Data quality: {warning}")
    
    return quality_report

def clean_infinite_values(df: pd.DataFrame) -> pd.DataFrame:
    """Clean infinite and NaN values from dataset"""
    logger.info("Cleaning infinite and NaN values...")

    # Replace infinite with NaN
    df = df.replace([np.inf, -np.inf], np.nan)

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        null_count = df[col].isnull().sum()

        # NEW ── in the rare case a duplicate-name pull returned a DataFrame
        if isinstance(null_count, pd.Series):
            null_count = null_count.sum()
        # ---------------------------------------------------------------

        if null_count > 0:
            median_val = df[col].median()
            if pd.isna(median_val):
                median_val = 0
            df[col] = df[col].fillna(median_val)

    logger.info(f"Cleaned dataset shape: {df.shape}")
    return df

# -----------------------------------------------------------------------------
# FIXED: Leakage-free feature engineering
# -----------------------------------------------------------------------------

def create_time_aware_features(df: pd.DataFrame, target_col: str, 
                              day_lags: List[int], roll_windows: List[int]) -> pd.DataFrame:
    """
    FIXED: Create features WITHOUT target leakage.
    This function should ONLY be called on weekly-aggregated data.
    """
    df = df.copy()
    
    # Ensure data is sorted by date
    df = df.sort_index()
    
    # Validate that we have a proper frequency
    if not hasattr(df.index, 'freq') or df.index.freq is None:
        logger.warning("Index frequency not set - this may cause alignment issues")
    
    # 1. Lag features (safe - uses historical data)
    for lag in day_lags:
        df[f"{target_col}_lag{lag}"] = df[target_col].shift(lag)
    
    # 2. Rolling statistics (safe - uses historical window with shift)
    for window in roll_windows:
        # CRITICAL: shift(1) ensures we don't use current period data
        rolling_series = df[target_col].shift(1).rolling(window, min_periods=1)
        df[f"{target_col}_roll{window}"] = rolling_series.mean()
        df[f"{target_col}_std{window}"] = rolling_series.std()
        df[f"{target_col}_min{window}"] = rolling_series.min()
        df[f"{target_col}_max{window}"] = rolling_series.max()
    
    # 3. Percentage change (safe - uses previous value)
    df[f"{target_col}_pct1"] = df[target_col].shift(1).pct_change()
    
    # 4. Trend features
    df[f"{target_col}_trend3"] = df[target_col].shift(1).rolling(3).apply(
        lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 3 else np.nan
    )
    
    logger.info(f"Created {len([c for c in df.columns if c.startswith(f'{target_col}_')])} time-aware features")
    return df


def create_calendar_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add calendar-based features"""
    df = df.copy()
    
    # Basic calendar features
    df["week"] = df.index.isocalendar().week.astype(int)
    df["month"] = df.index.month
    df["quarter"] = df.index.quarter
    df["day_of_week"] = df.index.dayofweek
    df["week_of_month"] = (df.index.day - 1) // 7 + 1
    
    # Holiday features
    us_holidays = holidays.US()
    df["is_holiday_week"] = df.index.to_series().apply(
        lambda x: any(x + pd.Timedelta(days=i) in us_holidays for i in range(7))
    ).astype(int)
    
    # Seasonal features
    df["month_sin"] = np.sin(2 * np.pi * df["month"] / 12)
    df["month_cos"] = np.cos(2 * np.pi * df["month"] / 12)
    
    logger.info("Added calendar and seasonal features")
    return df


def target_aware_bootstrap(X: pd.DataFrame, y: pd.Series, 
                          n_samples: int = None, random_state: int = 42) -> Tuple[pd.DataFrame, pd.Series]:
    """
    FIXED: Target-aware bootstrapping maintaining datetime index integrity.
    """
    if n_samples is None:
        n_samples = int(len(X) * 1.3)  # 30% increase
    
    np.random.seed(random_state)
    
    # Identify high-value periods (top 25% of target values)
    high_value_threshold = y.quantile(0.75)
    high_value_mask = y >= high_value_threshold
    
    # Create stratified sampling
    high_value_indices = y[high_value_mask].index.tolist()
    regular_indices = y[~high_value_mask].index.tolist()
    
    sampled_indices = []
    high_value_prob = 0.35  # 35% chance to sample from high-value periods
    
    for _ in range(n_samples):
        if np.random.random() < high_value_prob and len(high_value_indices) > 0:
            idx = np.random.choice(high_value_indices)
        else:
            idx = np.random.choice(regular_indices)
        sampled_indices.append(idx)
    
    # FIXED: Maintain datetime index continuity
    X_resampled = X.loc[sampled_indices].copy()
    y_resampled = y.loc[sampled_indices].copy()
    
    # Create new sequential datetime index to avoid duplicates while preserving frequency
    if hasattr(X.index, 'freq') and X.index.freq is not None:
        freq = X.index.freq
        start_date = X.index.min()
        new_index = pd.date_range(start=start_date, periods=len(X_resampled), freq=freq)
        X_resampled.index = new_index
        y_resampled.index = new_index
    
    logger.info(f"Bootstrap: {len(X)} → {len(X_resampled)} samples, {len(high_value_indices)} high-value periods")
    return X_resampled, y_resampled

# -----------------------------------------------------------------------------
# FIXED: Robust economic data fetching
# -----------------------------------------------------------------------------

def fetch_economic_data(tickers: Dict[str, str], start_date: str, end_date: str) -> pd.DataFrame:
    """
    FIXED: Robust per-ticker economic data fetching with error handling.
    """
    econ_data = pd.DataFrame()
    
    for name, ticker in tickers.items():
        try:
            logger.info(f"Fetching {name} ({ticker})...")
            
            # Download with retries
            data = yf.download(ticker, start=start_date, end=end_date, progress=False, show_errors=False)
            
            if data is None or data.empty:
                logger.warning(f"No data returned for {name} ({ticker})")
                continue
            
            # Handle different data structures
            if "Adj Close" in data.columns:
                series = data["Adj Close"]
            elif len(data.columns) == 1:
                series = data.iloc[:, 0]  # FIXED: Single column fallback
            else:
                # Take first numeric column
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    series = data[numeric_cols[0]]
                else:
                    logger.warning(f"No numeric data found for {name}")
                    continue
            
            # Clean and add to combined DataFrame
            series = series.dropna()
            if len(series) > 0:
                series.name = name
                if econ_data.empty:
                    econ_data = series.to_frame()
                else:
                    econ_data = econ_data.join(series, how='outer')
                
                logger.info(f"Successfully loaded {name}: {len(series)} observations")
            else:
                logger.warning(f"No valid data for {name} after cleaning")
                
        except Exception as e:
            logger.warning(f"Failed to fetch {name} ({ticker}): {e}")
            continue
    
    if not econ_data.empty:
        # Remove timezone info if present
        econ_data.index = econ_data.index.tz_localize(None) if econ_data.index.tz else econ_data.index
        logger.info(f"Economic data loaded: {econ_data.shape[1]} indicators, {len(econ_data)} dates")
    else:
        logger.warning("No economic data could be loaded")
    
    return econ_data

def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """Clean column names for LightGBM compatibility"""
    logger.info("Cleaning column names for model compatibility...")
    
    # Clean column names - remove special characters
    df.columns = [
        col.replace(' ', '_')
           .replace('(', '')
           .replace(')', '')
           .replace('[', '')
           .replace(']', '')
           .replace('{', '')
           .replace('}', '')
           .replace(',', '_')
           .replace('.', '_')
           .replace(':', '_')
           .replace(';', '_')
           .replace('!', '')
           .replace('?', '')
           .replace("'", '')
           .replace('"', '')
           .replace('-', '_')
           .replace('/', '_')
           .replace('\\', '_')
           .replace('|', '_')
           .replace('&', 'and')
           .replace('%', 'pct')
           .replace('#', 'num')
           .replace('@', 'at')
        for col in df.columns
    ]
    
    # Ensure no column starts with number
    df.columns = [f"col_{col}" if col[0].isdigit() else col for col in df.columns]
    
    logger.info("Column names cleaned")
    return df
# -----------------------------------------------------------------------------
# FIXED: Complete dataset building without leakage
# -----------------------------------------------------------------------------

def build_dataset() -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:
    """
    FIXED: Build dataset with proper sequencing to eliminate all leakage.
    Key fix: Create features AFTER weekly aggregation, not before.
    """
    
    # Validate data files exist
    validate_data_files()
    
    # Load data with comprehensive error handling
    try:
        mail = load_mail(CONFIG["data_files"]["mail"])
        calls_long = load_calls(CONFIG["data_files"]["calls"])
        calls_intent_raw = load_calls(CONFIG["data_files"]["intent"])
    except (FileNotFoundError, ValueError) as e:
        logger.error(f"Data loading failed: {e}")
        raise
    
    logger.info(f"Loaded {len(mail)} mail, {len(calls_long)} calls, {len(calls_intent_raw)} intent records")

    # Aggregate mail daily
    mail_daily = (
        mail.groupby(["mail_date", "mail_type"], as_index=False)["mail_volume"].sum()
            .rename(columns={"mail_date": "date"})
    )

    # Aggregate calls daily with intelligent legacy scaling
    calls_long_daily = calls_long.groupby("date").size().rename("calls")
    calls_intent_daily = calls_intent_raw.groupby("date").size().rename("calls")

    # Smart scaling for legacy data
    overlap = calls_long_daily.index.intersection(calls_intent_daily.index)
    if not overlap.empty and len(overlap) > 5:  # Need sufficient overlap
        scale_factor = calls_intent_daily.loc[overlap].mean() / calls_long_daily.loc[overlap].mean()
        calls_long_daily.loc[calls_long_daily.index.difference(overlap)] *= scale_factor
        logger.info(f"Applied scaling factor {scale_factor:.3f} to legacy call data")
    
    calls_total = calls_intent_daily.combine_first(calls_long_daily)

    # Process intents (remove nulls)
    # Process intents (remove nulls) - FIXED: Find intent column automatically
    logger.info(f"Intent data columns: {list(calls_intent_raw.columns)}")
    
    # Find intent column with multiple possible names
    intent_candidates = {"intent", "uui_intent", "call_intent", "intent_type", "category"}
    intent_col = None
    
    for col in calls_intent_raw.columns:
        if col.lower() in intent_candidates:
            intent_col = col
            break
    
    if intent_col is None:
        # Take first non-date column that looks like intent data
        non_date_cols = [col for col in calls_intent_raw.columns if col != "date"]
        if non_date_cols:
            intent_col = non_date_cols[0]  # Use first non-date column
            logger.warning(f"No standard intent column found, using '{intent_col}'")
        else:
            logger.warning("No intent column found - skipping intent processing")
            intents_daily = pd.DataFrame()
    
    if intent_col and intent_col in calls_intent_raw.columns:
        logger.info(f"Using intent column: '{intent_col}'")
        
        # Process intents (remove nulls)
        valid_intents = calls_intent_raw[~calls_intent_raw[intent_col].isin(CONFIG["null_intents"])]
        
        if len(valid_intents) > 0:
            intents_daily = valid_intents.groupby(["date", intent_col]).size().unstack(fill_value=0)
            # Convert intent columns to category for memory efficiency
            intents_daily.columns = intents_daily.columns.astype('category')
            logger.info(f"Processed {intents_daily.shape[1]} intent types")
        else:
            intents_daily = pd.DataFrame()
            logger.warning("No valid intent data found after filtering nulls")
    else:
        intents_daily = pd.DataFrame()
        logger.warning("No valid intent data found")

    # Create mail pivot
    mail_wide = mail_daily.pivot(index="date", columns="mail_type", values="mail_volume").fillna(0)
    logger.info(f"Processed {mail_wide.shape[1]} mail types")
    
    # Combine all daily data
    daily_components = [calls_total.rename("calls_total"), mail_wide]
    if not intents_daily.empty:
        daily_components.append(intents_daily)
    
    daily = pd.concat(daily_components, axis=1).fillna(0)
    daily.index = pd.to_datetime(daily.index)
    
    # Remove weekends and holidays
    us_holidays = holidays.US()
    business_days_mask = ~daily.index.weekday.isin([5, 6]) & ~daily.index.isin(us_holidays)
    daily = daily[business_days_mask]
    logger.info(f"Daily dataset after filtering: {daily.shape}")

    # Add economic indicators with robust fetching
    econ_data = fetch_economic_data(
        CONFIG["econ_tickers"],
        start_date=daily.index.min().strftime("%Y-%m-%d"),
        end_date=daily.index.max().strftime("%Y-%m-%d")
    )
    
    if not econ_data.empty:
        daily = daily.join(econ_data, how='left').ffill()
        logger.info(f"Added {econ_data.shape[1]} economic indicators")

    # CRITICAL FIX: Resample to weekly FIRST, then create features
    logger.info("Resampling to weekly frequency...")
    weekly = daily.resample(CONFIG["weekly_freq"]).sum(min_count=1)
    
    # FIXED: Properly set frequency to avoid alignment issues
    weekly.index.freq = CONFIG["weekly_freq"]
    logger.info(f"Weekly dataset shape: {weekly.shape}")
    
    # FIXED: Create features AFTER weekly aggregation (eliminates daily→weekly leakage)
    logger.info("Creating time-aware features...")
    weekly = create_time_aware_features(
        weekly, 
        "calls_total", 
        CONFIG["day_lags"], 
        CONFIG["roll_windows"]
    )
    
    # Add calendar features
    weekly = create_calendar_features(weekly)
    
    # FIXED: Add weekly lags with proper frequency handling
    logger.info("Adding weekly lag features...")
    for lag in CONFIG["week_lags"]:
        try:
            # Use DateOffset for reliable weekly shifts
            weekly[f"calls_total_wlag{lag}"] = weekly["calls_total"].shift(lag)
            logger.debug(f"Added {lag}-week lag feature")
        except Exception as e:
            logger.warning(f"Failed to create {lag}-week lag: {e}")
    
    # Remove rows with NaN values
    initial_shape = weekly.shape
    weekly = weekly.dropna()
    logger.info(f"Dataset after dropna: {initial_shape} → {weekly.shape}")
    
    # Optimize memory usage
    weekly = optimize_dtypes(weekly)

    # NEW ── ensure we don’t have duplicate column labels
    weekly = dedup_columns_df(weekly)

    # Comprehensive data quality validation
    quality_report = validate_data_quality(weekly)
    
    # Check for critical data quality issues
    if quality_report["total_rows"] < 50:
        raise ValueError(f"Insufficient data: only {quality_report['total_rows']} rows after processing")
    
    # Prepare features and target
    weekly = clean_infinite_values(weekly)
    weekly = clean_column_names(weekly)
    weekly = dedup_columns_df(weekly)
    
    # Prepare features and target
    X = weekly.drop(columns=["calls_total"])
    y = weekly["calls_total"]
    
    logger.info(f"Final dataset: {X.shape[0]} samples, {X.shape[1]} features")
    logger.info(f"Target statistics: mean={y.mean():.1f}, std={y.std():.1f}, range=[{y.min():.0f}, {y.max():.0f}]")
    
    return X, y, weekly

# -----------------------------------------------------------------------------
# FIXED: Atomic monitoring with run versioning
# -----------------------------------------------------------------------------

def atomic_write_json(data: dict, filepath: Path) -> None:
    """Write JSON file atomically with enhanced error handling"""
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, 
                                       dir=filepath.parent) as tmp_file:
            json.dump(data, tmp_file, indent=2, default=str)
            tmp_filepath = Path(tmp_file.name)
        
        # Atomic move
        os.replace(tmp_filepath, filepath)
        logger.info(f"Metrics written atomically to {filepath}")
        
    except Exception as e:
        logger.error(f"Failed to write metrics to {filepath}: {e}")
        # Clean up temp file if it exists
        if 'tmp_filepath' in locals() and tmp_filepath.exists():
            tmp_filepath.unlink()
        raise


def dump_metrics(metrics: dict, output_dir: Path, enable_prom: bool = False) -> None:
    """
    FIXED: Dump metrics with run versioning and atomic writes
    """
    
    # Add run metadata to metrics
    enhanced_metrics = {
        "run_metadata": {
            "run_id": CONFIG["run_id"],
            "version": CONFIG["version"],
            "timestamp": datetime.utcnow().isoformat(),
            "config_hash": hashlib.md5(json.dumps(CONFIG, sort_keys=True, default=str).encode()).hexdigest()
        },
        "model_metrics": metrics
    }
    
    # Atomic JSON write
    atomic_write_json(enhanced_metrics, output_dir / "metrics.json")
    
    if enable_prom:
        # FIXED: Prometheus export with run ID and versioning
        prom_lines = [
            "# HELP call_forecast_metrics Model performance metrics\n",
            "# TYPE call_forecast_metrics gauge\n"
        ]
        
        for model, scores in metrics.items():
            if isinstance(scores, dict):
                for metric, value in scores.items():
                    if isinstance(value, (float, int)) and metric in ["RMSE", "MAPE", "R2"]:
                        # Add run_id and version labels
                        labels = f'model="{model}",run_id="{CONFIG["run_id"]}",version="{CONFIG["version"]}"'
                        prom_lines.append(f'call_forecast_{metric.lower()}{{{labels}}} {value}\n')
        
        prom_content = ''.join(prom_lines)
        
        # FIXED: Versioned Prometheus files to avoid overwrites
        prom_filename = f"prometheus_metrics_{CONFIG['run_id']}.prom"
        
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.prom', delete=False,
                                           dir=output_dir) as tmp_file:
                tmp_file.write(prom_content)
                tmp_filepath = Path(tmp_file.name)
            
            os.replace(tmp_filepath, output_dir / prom_filename)
            
            # Also create/update a "latest" symlink for monitoring systems
            latest_link = output_dir / "prometheus_metrics_latest.prom"
            if latest_link.exists():
                latest_link.unlink()
            latest_link.symlink_to(prom_filename)
            
            logger.info(f"Prometheus metrics: {prom_filename}")
            
        except Exception as e:
            logger.error(f"Failed to write Prometheus metrics: {e}")


def validate_model_performance(metrics: dict, y_mean: float, coverage: float = None) -> List[str]:
    """
    FIXED: Enhanced model validation with adaptive thresholds and alerts
    """
    thresholds = CONFIG["thresholds"]
    alerts = []
    
    # Convert percentage thresholds to absolute values
    max_rmse = y_mean * thresholds["max_rmse_pct"]
    max_mape = thresholds["max_mape"]
    min_r2 = thresholds["min_r2"]
    min_coverage = thresholds.get("min_coverage", 0.85)
    
    for model, scores in metrics.items():
        if isinstance(scores, dict):
            model_alerts = []
            
            # RMSE validation
            if scores.get("RMSE", 0) > max_rmse:
                alert = f"{model} RMSE {scores['RMSE']:.2f} exceeds {max_rmse:.2f} ({thresholds['max_rmse_pct']:.0%} of mean)"
                model_alerts.append(alert)
                logger.warning(alert)
            
            # R² validation
            if scores.get("R2", 1) < min_r2:
                alert = f"{model} R² {scores['R2']:.3f} below threshold {min_r2}"
                model_alerts.append(alert)
                logger.warning(alert)
            
            # MAPE validation
            if scores.get("MAPE", 0) > max_mape:
                alert = f"{model} MAPE {scores['MAPE']:.1%} exceeds threshold {max_mape:.1%}"
                model_alerts.append(alert)
                logger.warning(alert)
            
            alerts.extend(model_alerts)
    
    # Coverage validation
    if coverage is not None and coverage < min_coverage:
        alert = f"Prediction interval coverage {coverage:.1%} below threshold {min_coverage:.1%}"
        alerts.append(alert)
        logger.warning(alert)
    
    if not alerts:
        logger.info("✅ All models passed performance validation")
    else:
        logger.warning(f"⚠️ {len(alerts)} performance alerts generated")
    
    return alerts

# -----------------------------------------------------------------------------
# Enhanced EDA plots
# -----------------------------------------------------------------------------

def plot_time_series(weekly: pd.DataFrame, output_dir: Path) -> None:
    """Create comprehensive time series analysis plots"""
    plt.figure(figsize=(16, 12))
    
    # Main time series
    plt.subplot(2, 3, 1)
    weekly["calls_total"].plot(linewidth=2, color='steelblue')
    plt.title("Weekly Call Volume Over Time", fontsize=12, fontweight='bold')
    plt.ylabel("Calls")
    plt.grid(True, alpha=0.3)
    
    # Seasonal patterns
    plt.subplot(2, 3, 2)
    monthly_avg = weekly.groupby("month")["calls_total"].mean()
    monthly_avg.plot(kind="bar", color='lightcoral', alpha=0.8)
    plt.title("Average Calls by Month", fontsize=12, fontweight='bold')
    plt.ylabel("Calls")
    plt.xticks(rotation=45)
    
    # Distribution with statistics
    plt.subplot(2, 3, 3)
    weekly["calls_total"].hist(bins=25, edgecolor='black', alpha=0.7, color='lightgreen')
    plt.axvline(weekly["calls_total"].mean(), color='red', linestyle='--', 
                label=f'Mean: {weekly["calls_total"].mean():.0f}')
    plt.axvline(weekly["calls_total"].median(), color='orange', linestyle='--', 
                label=f'Median: {weekly["calls_total"].median():.0f}')
    plt.title("Call Volume Distribution", fontsize=12, fontweight='bold')
    plt.xlabel("Calls")
    plt.ylabel("Frequency")
    plt.legend()
    
    # Rolling statistics
    plt.subplot(2, 3, 4)
    weekly["calls_total"].rolling(4).mean().plot(label="4-week MA", alpha=0.8)
    weekly["calls_total"].rolling(12).mean().plot(label="12-week MA", alpha=0.8)
    weekly["calls_total"].rolling(26).mean().plot(label="26-week MA", alpha=0.8)
    plt.title("Rolling Averages", fontsize=12, fontweight='bold')
    plt.legend()
    plt.ylabel("Calls")
    plt.grid(True, alpha=0.3)
    
    # Quarterly patterns
    plt.subplot(2, 3, 5)
    quarterly_avg = weekly.groupby("quarter")["calls_total"].mean()
    quarterly_avg.plot(kind="bar", color='mediumpurple', alpha=0.8)
    plt.title("Average Calls by Quarter", fontsize=12, fontweight='bold')
    plt.ylabel("Calls")
    plt.xticks(rotation=0)
    
    # Year-over-year if applicable
    plt.subplot(2, 3, 6)
    if len(weekly) > 52:  # More than 1 year of data
        weekly_with_year = weekly.copy()
        weekly_with_year['year'] = weekly_with_year.index.year
        yearly_avg = weekly_with_year.groupby('year')["calls_total"].mean()
        yearly_avg.plot(kind="bar", color='gold', alpha=0.8)
        plt.title("Average Calls by Year", fontsize=12, fontweight='bold')
        plt.ylabel("Calls")
        plt.xticks(rotation=45)
    else:
        # Show weekly patterns instead
        weekly_pattern = weekly.groupby(weekly.index.week)["calls_total"].mean()
        weekly_pattern.plot(color='teal', alpha=0.7)
        plt.title("Average Calls by Week of Year", fontsize=12, fontweight='bold')
        plt.xlabel("Week")
        plt.ylabel("Calls")
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / "time_series_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()


def plot_rolling_correlations(weekly: pd.DataFrame, mail_cols: List[str], output_dir: Path) -> None:
    """Create correlation analysis across different periods"""
    periods = [4, 8, 12, 24]  # Weekly periods (1, 2, 3, 6 months)
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    
    for i, period in enumerate(periods):
        corrs = {}
        for mail_col in mail_cols[:8]:  # Limit to top 8 for readability
            if mail_col in weekly.columns:
                try:
                    # Calculate rolling correlation with proper error handling
                    mail_series = weekly[mail_col].astype(float)
                    calls_series = weekly["calls_total"].astype(float)
                    
                    # Calculate rolling correlation manually for safety
                    rolling_corr = pd.Series(index=weekly.index, dtype=float)
                    for idx in range(period, len(weekly)):
                        window_start = idx - period
                        window_end = idx
                        
                        mail_window = mail_series.iloc[window_start:window_end]
                        calls_window = calls_series.iloc[window_start:window_end]
                        
                        if len(mail_window) > 1 and len(calls_window) > 1:
                            corr_val = mail_window.corr(calls_window)
                            if pd.notna(corr_val):
                                rolling_corr.iloc[idx] = corr_val
                    
                    corrs[mail_col] = rolling_corr.dropna()
                except Exception as e:
                    logger.warning(f"Rolling correlation failed for {mail_col}: {e}")
                    continue
            
        ax = axes[i]
        for mail_col, corr_series in corrs.items():
            if len(corr_series) > 0:
                ax.plot(corr_series.index, corr_series.values, 
                       label=mail_col, alpha=0.7, linewidth=2)
        
        ax.set_title(f'{period}-Week Rolling Correlation', fontsize=12, fontweight='bold')
        ax.set_xlabel('Date')
        ax.set_ylabel('Correlation')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        
        # Add correlation strength zones
        ax.axhline(y=0.3, color='green', linestyle=':', alpha=0.5, label='Weak +')
        ax.axhline(y=-0.3, color='red', linestyle=':', alpha=0.5, label='Weak -')
    
    plt.tight_layout()
    plt.savefig(output_dir / "rolling_correlations.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # Create summary heatmap
    plt.figure(figsize=(12, 8))
    avg_corrs = pd.DataFrame(index=mail_cols, columns=[f'{p}w' for p in periods])
    
    for period in periods:
        for mail_col in mail_cols:
            if mail_col in weekly.columns:
                rolling_corr = weekly[mail_col].rolling(window=period).corr(
                    weekly["calls_total"].rolling(window=period)
                )
                avg_corrs.loc[mail_col, f'{period}w'] = rolling_corr.mean()
    
    mask = avg_corrs.isnull()
    sns.heatmap(avg_corrs.astype(float), annot=True, fmt=".2f", 
                cmap="RdBu_r", center=0, mask=mask)
    plt.title("Average Rolling Correlations by Period", fontsize=14, fontweight='bold')
    plt.xlabel("Rolling Window (weeks)")
    plt.ylabel("Mail Type")
    plt.tight_layout()
    plt.savefig(output_dir / "rolling_correlations_summary.png", dpi=300, bbox_inches='tight')
    plt.close()


def plot_intent_mail_correlations(weekly: pd.DataFrame, output_dir: Path) -> None:
    """FIXED: Enhanced correlation analysis between mail types and call intents"""
    
    # Smart column identification
    all_cols = set(weekly.columns)
    exclude_patterns = {"calls_total", "week", "month", "quarter", "day_of_week", 
                       "is_holiday_week", "week_of_month", "month_sin", "month_cos"}
    exclude_suffixes = ("_lag", "_wlag", "_roll", "_std", "_min", "_max", "_pct", "_trend")
    exclude_prefixes = ("SP500", "UNRATE", "DGS10")
    
    # Filter feature columns
    feature_cols = [col for col in all_cols 
                   if col not in exclude_patterns
                   and not any(col.endswith(suffix) for suffix in exclude_suffixes)
                   and not any(col.startswith(prefix) for prefix in exclude_prefixes)]
    
    # Enhanced heuristic for separating mail vs intent columns
    mail_cols = []
    intent_cols = []
    
    for col in feature_cols:
        if col in weekly.columns:
            # Statistical characteristics to differentiate mail vs intent
            col_data = weekly[col]
            col_max = col_data.max()
            col_sparsity = (col_data == 0).mean()
            col_variance = col_data.var()
            col_mean = col_data.mean()
            
            # Intent data is typically more sparse and has lower variance
            try:
                if col_sparsity > 0.6 or (col_variance < col_mean and col_sparsity > 0.3):
                    intent_cols.append(col)
                else:
                    mail_cols.append(col)
            except (ValueError, TypeError):
                # Skip problematic columns
                continue
    logger.info(f"Identified {len(mail_cols)} mail columns and {len(intent_cols)} intent columns")
    
    if not mail_cols or not intent_cols:
        logger.warning("Insufficient mail or intent columns for correlation analysis")
        return
    
    # Calculate all correlations
    correlations = []
    
    for mail_col in mail_cols:
        for intent_col in intent_cols:
            try:
                corr_val = weekly[mail_col].corr(weekly[intent_col])
                if not pd.isna(corr_val) and abs(corr_val) > 0.05:  # Meaningful threshold
                    correlations.append({
                        'mail_type': mail_col,
                        'intent_type': intent_col,
                        'correlation': corr_val,
                        'abs_correlation': abs(corr_val),
                        'mail_mean': weekly[mail_col].mean(),
                        'intent_mean': weekly[intent_col].mean()
                    })
            except Exception as e:
                logger.warning(f"Correlation calculation failed for {mail_col} vs {intent_col}: {e}")
    
    if not correlations:
        logger.warning("No significant correlations found between mail types and intents")
        return
    
    # Get top correlations
    corr_df = pd.DataFrame(correlations)
    top_correlations = corr_df.nlargest(12, 'abs_correlation')  # Top 12 for better display
    
    # Create enhanced visualization
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))
    
    # Plot 1: Top correlations as horizontal bar chart
    colors = ['crimson' if x < 0 else 'steelblue' for x in top_correlations['correlation']]
    labels = [f"{row['mail_type']} → {row['intent_type']}" for _, row in top_correlations.iterrows()]
    
    y_pos = range(len(top_correlations))
    bars = ax1.barh(y_pos, top_correlations['correlation'], color=colors, alpha=0.7, edgecolor='black')
    ax1.set_yticks(y_pos)
    ax1.set_yticklabels(labels, fontsize=9)
    ax1.set_xlabel('Correlation Coefficient', fontweight='bold')
    ax1.set_title('Top 12 Mail Type → Call Intent Correlations', fontsize=12, fontweight='bold')
    ax1.axvline(x=0, color='black', linestyle='-', alpha=0.8, linewidth=1)
    ax1.grid(True, alpha=0.3)
    
    # Add value labels on bars
    for i, (bar, val) in enumerate(zip(bars, top_correlations['correlation'])):
        ax1.text(val + (0.01 if val > 0 else -0.01), i, f'{val:.2f}', 
                va='center', ha='left' if val > 0 else 'right', fontsize=8)
    
    # Plot 2: Correlation matrix heatmap (top correlations only)
    pivot_data = top_correlations.pivot_table(
        index='mail_type', columns='intent_type', values='correlation', fill_value=0
    )
    
    sns.heatmap(pivot_data, annot=True, fmt=".2f", cmap="RdBu_r", center=0, ax=ax2,
                cbar_kws={'label': 'Correlation'})
    ax2.set_title('Mail vs Intent Correlation Matrix\n(Top Correlations)', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Intent Type', fontweight='bold')
    ax2.set_ylabel('Mail Type', fontweight='bold')
    
    # Plot 3: Distribution of correlation strengths
    ax3.hist(corr_df['abs_correlation'], bins=20, alpha=0.7, edgecolor='black', color='lightseagreen')
    ax3.axvline(x=corr_df['abs_correlation'].mean(), color='red', linestyle='--', linewidth=2,
                label=f'Mean: {corr_df["abs_correlation"].mean():.3f}')
    ax3.axvline(x=corr_df['abs_correlation'].median(), color='orange', linestyle='--', linewidth=2,
                label=f'Median: {corr_df["abs_correlation"].median():.3f}')
    ax3.set_xlabel('Absolute Correlation', fontweight='bold')
    ax3.set_ylabel('Frequency', fontweight='bold')
    ax3.set_title('Distribution of Mail→Intent Correlation Strengths', fontsize=12, fontweight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Top mail types by average correlation impact
    mail_avg_corr = corr_df.groupby('mail_type')['abs_correlation'].agg(['mean', 'count']).reset_index()
    mail_avg_corr = mail_avg_corr[mail_avg_corr['count'] >= 3].sort_values('mean', ascending=False).head(10)
    
    bars = ax4.bar(range(len(mail_avg_corr)), mail_avg_corr['mean'], 
                   color='mediumpurple', alpha=0.7, edgecolor='black')
    ax4.set_xticks(range(len(mail_avg_corr)))
    ax4.set_xticklabels(mail_avg_corr['mail_type'], rotation=45, ha='right', fontsize=9)
    ax4.set_ylabel('Average Absolute Correlation', fontweight='bold')
    ax4.set_title('Top Mail Types by Intent Correlation Impact', fontsize=12, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    
    # Add value labels
    for bar, val in zip(bars, mail_avg_corr['mean']):
        ax4.text(bar.get_x() + bar.get_width()/2, val + 0.005, f'{val:.3f}', 
                ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(output_dir / "intent_mail_correlations.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # Save detailed results
    top_correlations.to_csv(output_dir / "top_intent_mail_correlations.csv", index=False)
    
    # Log top findings
    logger.info(f"Found {len(correlations)} total correlations")
    logger.info("Top 3 mail→intent correlations:")
    for i, (_, row) in enumerate(top_correlations.head(3).iterrows()):
        logger.info(f"  {i+1}. {row['mail_type']} → {row['intent_type']}: {row['correlation']:.3f}")


def create_eda_plots(X: pd.DataFrame, y: pd.Series, weekly: pd.DataFrame, output_dir: Path) -> None:
    """Create comprehensive EDA plots with error handling"""
    plots_dir = output_dir / "plots"
    plots_dir.mkdir(exist_ok=True)
    
    logger.info("Creating EDA visualizations...")
    
    try:
        # Time series analysis
        plot_time_series(weekly, plots_dir)
        logger.info("✅ Time series plots created")
    except Exception as e:
        logger.error(f"Time series plots failed: {e}")
        
        # Identify mail columns
        all_cols = set(weekly.columns)
        exclude_patterns = {"calls_total", "week", "month", "quarter", "day_of_week", 
                           "is_holiday_week", "week_of_month", "month_sin", "month_cos"}
        exclude_suffixes = ("_lag", "_wlag", "_roll", "_std", "_min", "_max", "_pct", "_trend")
        exclude_prefixes = ("SP500", "UNRATE", "DGS10")
        
        mail_cols = []
        for col in all_cols:
            if (col not in exclude_patterns
                and not any(col.endswith(suffix) for suffix in exclude_suffixes)
                and not any(col.startswith(prefix) for prefix in exclude_prefixes)):
                
                try:
                    # Safe sparsity check
                    col_data = weekly[col].dropna()
                    if len(col_data) > 0:
                        sparsity = (col_data == 0).sum() / len(col_data)
                        if sparsity <= 0.5:  # Less sparse = likely mail data
                            mail_cols.append(col)
                except (ValueError, TypeError):
                    # Skip columns that can't be processed
                    continue
        
        if mail_cols:
            try:
                plot_rolling_correlations(weekly, mail_cols, plots_dir)
                logger.info("✅ Rolling correlation plots created")
            except Exception as e:
                logger.error(f"Rolling correlation plots failed: {e}")
        else:
            logger.warning("No mail columns identified for rolling correlation analysis")
        
        # Intent-mail correlations
        try:
            plot_intent_mail_correlations(weekly, plots_dir)
            logger.info("✅ Intent-mail correlation plots created")
        except Exception as e:
            logger.error(f"Intent-mail correlation plots failed: {e}")
        
        # Feature correlation matrix
        if X.shape[1] > 5:
            plt.figure(figsize=(12, 8))
            # Select top 20 features by correlation with target
            correlations = X.corrwith(y).abs().sort_values(ascending=False).head(20)
            top_features = correlations.index
            
            corr_matrix = X[top_features].corr()
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
            sns.heatmap(corr_matrix, mask=mask, annot=False, cmap="RdBu_r", center=0)
            plt.title("Top 20 Feature Correlations", fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(plots_dir / "feature_correlations.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            # Feature importance by correlation
            plt.figure(figsize=(10, 8))
            correlations.plot(kind='barh', color='steelblue', alpha=0.7)
            plt.title("Top 20 Features by Target Correlation", fontsize=14, fontweight='bold')
            plt.xlabel("Absolute Correlation", fontweight='bold')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(plots_dir / "feature_importance_corr.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("✅ Feature correlation plots created")
        
        logger.info(f"All EDA plots saved to {plots_dir}")
        
    except Exception as e:
        logger.error(f"EDA plot generation failed: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise

# -----------------------------------------------------------------------------
# FIXED: Model training with proper ensemble handling
# -----------------------------------------------------------------------------

def make_search_spaces() -> Dict[str, Dict]:
    """Define hyperparameter search spaces for individual models only"""
    return {
        "ElasticNet": {
            "alpha": Real(1e-3, 1.0, prior="log-uniform"),
            "l1_ratio": Real(0, 1)
        },
        "RandomForest": {
            "n_estimators": Integer(100, 300),
            "max_depth": Integer(3, 12),
            "min_samples_leaf": Integer(1, 8),
            "min_samples_split": Integer(2, 15)
        },
        "LightGBM": {
            "n_estimators": Integer(200, 600),
            "num_leaves": Integer(15, 100),
            "learning_rate": Real(0.01, 0.2, prior="log-uniform"),
            "subsample": Real(0.6, 1.0),
            "colsample_bytree": Real(0.6, 1.0),
            "min_child_samples": Integer(5, 50),
            "device": ["cpu"],  # Force CPU
        },
        "XGBoost": {
            "n_estimators": Integer(200, 600),
            "max_depth": Integer(3, 8),
            "learning_rate": Real(0.01, 0.2, prior="log-uniform"),
            "subsample": Real(0.6, 1.0),
            "colsample_bytree": Real(0.6, 1.0),
            "min_child_weight": Integer(1, 8),
            "tree_method": ["hist"],  # Force CPU
            "device": ["cpu"]
        }
    }
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

def build_model(name: str) -> object:
    """Build base model with production settings"""
    if name == "ElasticNet":
        return make_pipeline(
            StandardScaler(),
            ElasticNet(random_state=CONFIG["random_state"], max_iter=10_000)
        ) 
    elif name == "RandomForest":
        return RandomForestRegressor(random_state=CONFIG["random_state"], n_jobs=-1)
    elif name == "LightGBM":
        return LGBMRegressor(random_state=CONFIG["random_state"], verbose=-1, device="cpu")
    elif name == "XGBoost":
        return XGBRegressor(
            random_state=CONFIG["random_state"], 
            objective="reg:squarederror", 
            n_jobs=-1,
            tree_method="hist",
            device="cpu",
            enable_categorical = False
        )
    else:
        raise ValueError(f"Unknown model: {name}")


def nested_cv_evaluation(model_name: str, X: pd.DataFrame, y: pd.Series) -> Dict:
    """
    FIXED: Nested CV evaluation with proper error handling for ensemble models
    """
    # FIXED: Check if model is in search spaces (skip ensemble models)
    search_spaces = make_search_spaces()
    if model_name not in search_spaces:
        logger.warning(f"Skipping nested CV for {model_name} - no search space defined")
        return {
            "RMSE": np.nan,
            "MAPE": np.nan,
            "R2": np.nan,
            "RMSE_std": np.nan,
            "best_params": {},
            "all_params": []
        }
    
    outer_cv = TimeSeriesSplit(n_splits=CONFIG["ts_splits"])
    outer_scores = {"RMSE": [], "MAPE": [], "R2": []}
    best_params_list = []
    
    try:
        for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X)):
            logger.debug(f"Processing fold {fold_idx + 1}/{CONFIG['ts_splits']} for {model_name}")
            
            X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]
            y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]
            
            # Inner CV for hyperparameter optimization
            inner_cv = TimeSeriesSplit(n_splits=max(3, CONFIG["ts_splits"] - 1))
            base_model = build_model(model_name)
            
            search = BayesSearchCV(
                base_model,
                search_spaces[model_name],
                cv=inner_cv,
                n_iter=CONFIG["hyper_opt_iters"],
                random_state=CONFIG["random_state"],
                scoring="neg_root_mean_squared_error",
                n_jobs=-1,
                verbose=0
            )
            
            # Fit on outer training set
            search.fit(X_train_outer, y_train_outer)
            best_model = search.best_estimator_
            best_params_list.append(search.best_params_)
            
            # Evaluate on outer test set
            y_pred = best_model.predict(X_test_outer)
            
            outer_scores["RMSE"].append(np.sqrt(mean_squared_error(y_test_outer, y_pred)))
            outer_scores["MAPE"].append(mean_absolute_percentage_error(y_test_outer, y_pred))
            outer_scores["R2"].append(r2_score(y_test_outer, y_pred))
        
        # Return average scores
        return {
            "RMSE": float(np.mean(outer_scores["RMSE"])),
            "MAPE": float(np.mean(outer_scores["MAPE"])), 
            "R2": float(np.mean(outer_scores["R2"])),
            "RMSE_std": float(np.std(outer_scores["RMSE"])),
            "best_params": best_params_list[0] if best_params_list else {},
            "all_params": best_params_list
        }
        
    except Exception as e:
        logger.error(f"Nested CV failed for {model_name}: {e}")
        return {
            "RMSE": np.nan,
            "MAPE": np.nan,
            "R2": np.nan,
            "RMSE_std": np.nan,
            "best_params": {},
            "all_params": []
        }


def train_final_model(model_name: str, X: pd.DataFrame, y: pd.Series, best_params: Dict) -> object:
    """Train final model on all data with best parameters"""
    try:
        model = build_model(model_name)
        if best_params:  # Only set params if they exist
            model.set_params(**best_params)
        model.fit(X, y)
        return model
    except Exception as e:
        logger.error(f"Failed to train final model {model_name}: {e}")
        raise


def get_feature_importance(model, X: pd.DataFrame) -> Dict[str, float]:
    """Extract feature importance from model with error handling"""
    try:
        if hasattr(model, 'feature_importances_'):
            importance = dict(zip(X.columns, model.feature_importances_))
        elif hasattr(model, 'coef_'):
            importance = dict(zip(X.columns, np.abs(model.coef_)))
        else:
            return {}
        
        return dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))
    except Exception as e:
        logger.warning(f"Could not extract feature importance: {e}")
        return {}


def conformal_prediction_intervals(model, X: pd.DataFrame, y: pd.Series, 
                                 alpha: float = 0.1) -> Tuple[pd.DataFrame, float]:
    """
    FIXED: Conformal prediction intervals with coverage calculation
    """
    cv = TimeSeriesSplit(n_splits=CONFIG["ts_splits"])
    residuals = []
    
    try:
        # Collect residuals from CV
        for train_idx, test_idx in cv.split(X):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            residuals.extend(np.abs(y_test - y_pred))
        
        # Calculate quantile for conformal intervals
        quantile = np.quantile(residuals, 1 - alpha)
        
        # Final predictions on full data
        model.fit(X, y)
        predictions = model.predict(X)
        
        # Create intervals
        intervals = pd.DataFrame({
            'prediction': predictions,
            'lower': predictions - quantile,
            'upper': predictions + quantile,
            'actual': y.values
        }, index=X.index)
        
        # Calculate coverage
        coverage = ((intervals['actual'] >= intervals['lower']) & 
                   (intervals['actual'] <= intervals['upper'])).mean()
        
        logger.info(f"Conformal intervals: {(1-alpha):.0%} target, {coverage:.1%} actual coverage")
        
        return intervals, coverage
        
    except Exception as e:
        logger.error(f"Conformal prediction intervals failed: {e}")
        # Return empty DataFrame and 0 coverage on failure
        return pd.DataFrame(), 0.0


def train_models(X: pd.DataFrame, y: pd.Series, output_dir: Path) -> float:
    """
    FIXED: Train models with enhanced error handling and proper ensemble management
    Returns coverage for prediction intervals
    """
    logger.info("Starting model training with nested CV...")
    
    search_spaces = make_search_spaces()
    metrics_out = {}
    final_models = {}
    coverage = 0.0
    
    # Optional target-aware bootstrapping
    if CONFIG["enable_bootstrap"]:
        logger.info("Applying target-aware bootstrapping...")
        try:
            X_train, y_train = target_aware_bootstrap(X, y, random_state=CONFIG["random_state"])
            logger.info(f"Bootstrap dataset: {len(X_train)} samples (original: {len(X)})")
        except Exception as e:
            logger.warning(f"Bootstrap failed, using original data: {e}")
            X_train, y_train = X, y
    else:
        X_train, y_train = X, y
    
    # Train individual models with nested CV
    successful_models = []
    
    for model_name in search_spaces.keys():
        logger.info(f"Training {model_name}...")
        
        try:
            # Nested CV evaluation
            cv_results = nested_cv_evaluation(model_name, X_train, y_train)
            
            # Skip if CV failed
            if np.isnan(cv_results["RMSE"]):
                logger.warning(f"Skipping {model_name} due to CV failure")
                continue
            
            # Train final model on all data
            final_model = train_final_model(model_name, X_train, y_train, cv_results["best_params"])
            final_models[model_name] = final_model
            successful_models.append(model_name)
            
            # Get feature importance
            feature_importance = get_feature_importance(final_model, X_train)
            
            # Store results
            metrics_out[model_name] = {
                **cv_results,
                "top_features": list(feature_importance.keys())[:10],
                "feature_importance": feature_importance
            }
            
            # Save model
            joblib.dump(final_model, output_dir / f"model_{model_name}.pkl")
            
            logger.info(f"✅ {model_name} → RMSE: {cv_results['RMSE']:.2f}±{cv_results['RMSE_std']:.2f}, "
                       f"MAPE: {cv_results['MAPE']:.2%}, R²: {cv_results['R2']:.3f}")
            
            # SHAP analysis (configurable)
            if CONFIG["run_shap"] and model_name in {"RandomForest", "LightGBM", "XGBoost"}:
                try:
                    sample_size = min(300, len(X_train))  # Reduced for performance
                    X_sample = X_train.sample(sample_size, random_state=CONFIG["random_state"])
                    
                    explainer = shap.TreeExplainer(final_model)
                    shap_values = explainer.shap_values(X_sample)
                    
                    plt.figure(figsize=(10, 6))
                    shap.summary_plot(shap_values, X_sample, show=False)
                    plt.title(f'SHAP Feature Importance - {model_name}', fontweight='bold')
                    plt.tight_layout()
                    plt.savefig(output_dir / f"shap_{model_name}.png", dpi=300, bbox_inches='tight')
                    plt.close()
                    
                    logger.info(f"✅ SHAP analysis completed for {model_name}")
                except Exception as e:
                    logger.warning(f"SHAP analysis failed for {model_name}: {e}")
        
        except Exception as e:
            logger.error(f"Training failed for {model_name}: {e}")
            continue
    
    # FIXED: Create stacking ensemble with proper error handling
    if len(successful_models) >= 3:
        logger.info("Training stacking ensemble...")
        try:
            # Use best 3 models for stacking
            best_models = sorted(
                [(name, metrics_out[name]["RMSE"]) for name in successful_models],
                key=lambda x: x[1]
            )[:3]
            
            estimators = [(name, final_models[name]) for name, _ in best_models]
            
            stack = StackingRegressor(
                estimators=estimators,
                final_estimator=ElasticNet(alpha=0.01, l1_ratio=0.1, random_state=CONFIG["random_state"]),
                n_jobs=-1,
                cv=TimeSeriesSplit(n_splits=3)  # Internal CV for stacking
            )
            
            # FIXED: Manual evaluation for ensemble (no nested CV)
            cv = TimeSeriesSplit(n_splits=CONFIG["ts_splits"])
            stack_scores = {"RMSE": [], "MAPE": [], "R2": []}
            
            for train_idx, test_idx in cv.split(X_train):
                X_tr, X_te = X_train.iloc[train_idx], X_train.iloc[test_idx]
                y_tr, y_te = y_train.iloc[train_idx], y_train.iloc[test_idx]
                
                stack.fit(X_tr, y_tr)
                y_pred = stack.predict(X_te)
                
                stack_scores["RMSE"].append(np.sqrt(mean_squared_error(y_te, y_pred)))
                stack_scores["MAPE"].append(mean_absolute_percentage_error(y_te, y_pred))
                stack_scores["R2"].append(r2_score(y_te, y_pred))
            
            # Train final ensemble
            stack.fit(X_train, y_train)
            final_models["StackEnsemble"] = stack
            
            stack_results = {
                "RMSE": float(np.mean(stack_scores["RMSE"])),
                "MAPE": float(np.mean(stack_scores["MAPE"])),
                "R2": float(np.mean(stack_scores["R2"])),
                "RMSE_std": float(np.std(stack_scores["RMSE"])),
                "best_params": "ensemble",
                "top_features": [],
                "feature_importance": {}
            }
            
            metrics_out["StackEnsemble"] = stack_results
            joblib.dump(stack, output_dir / "model_StackEnsemble.pkl")
            
            logger.info(f"✅ StackEnsemble → RMSE: {stack_results['RMSE']:.2f}, "
                       f"MAPE: {stack_results['MAPE']:.2%}, R²: {stack_results['R2']:.3f}")
            
        except Exception as e:
            logger.error(f"Ensemble training failed: {e}")
    
    else:
        logger.warning(f"Insufficient models ({len(successful_models)}) for ensemble creation")
    
    # Generate prediction intervals for best model
    if metrics_out:
        best_model_name = min(metrics_out.keys(), key=lambda x: metrics_out[x]["RMSE"])
        best_model = final_models[best_model_name]
        
        logger.info(f"Generating conformal prediction intervals using {best_model_name}...")
        try:
            intervals, coverage = conformal_prediction_intervals(best_model, X, y)
            
            if not intervals.empty:
                intervals.to_csv(output_dir / "prediction_intervals.csv")
                
                # Plot prediction intervals
                plt.figure(figsize=(14, 8))
                
                # Plot actual vs predicted
                plt.subplot(2, 1, 1)
                plt.plot(intervals.index, intervals['actual'], 'o-', 
                        label='Actual', alpha=0.7, markersize=4, linewidth=2)
                plt.plot(intervals.index, intervals['prediction'], 'r-', 
                        label=f'Prediction ({best_model_name})', alpha=0.8, linewidth=2)
                plt.fill_between(intervals.index, intervals['lower'], intervals['upper'], 
                                alpha=0.3, label=f'90% Prediction Interval (Coverage: {coverage:.1%})')
                plt.title('Conformal Prediction Intervals', fontsize=14, fontweight='bold')
                plt.ylabel('Call Volume')
                plt.legend()
                plt.grid(True, alpha=0.3)
                
                # Plot residuals
                plt.subplot(2, 1, 2)
                residuals = intervals['actual'] - intervals['prediction']
                plt.plot(intervals.index, residuals, 'o-', alpha=0.6, markersize=3)
                plt.axhline(y=0, color='red', linestyle='--', alpha=0.8)
                plt.title('Prediction Residuals', fontsize=12, fontweight='bold')
                plt.xlabel('Date')
                plt.ylabel('Residual (Actual - Predicted)')
                plt.grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.savefig(output_dir / "prediction_intervals.png", dpi=300, bbox_inches='tight')
                plt.close()
                
                logger.info(f"✅ Prediction intervals: {coverage:.1%} coverage")
            
        except Exception as e:
            logger.warning(f"Prediction intervals failed: {e}")
    
    # Create model comparison plots
    if metrics_out:
        try:
            models = list(metrics_out.keys())
            rmse_vals = [metrics_out[m]["RMSE"] for m in models]
            rmse_stds = [metrics_out[m].get("RMSE_std", 0) for m in models]
            mape_vals = [metrics_out[m]["MAPE"] for m in models]
            r2_vals = [metrics_out[m]["R2"] for m in models]
            
            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
            
            # RMSE with error bars
            bars1 = ax1.bar(models, rmse_vals, yerr=rmse_stds, capsize=5, 
                           color='skyblue', alpha=0.7, edgecolor='black')
            ax1.set_title("RMSE by Model", fontsize=12, fontweight='bold')
            ax1.set_ylabel("RMSE")
            ax1.grid(True, alpha=0.3)
            plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
            
            # Add value labels
            for bar, val, std in zip(bars1, rmse_vals, rmse_stds):
                ax1.text(bar.get_x() + bar.get_width()/2, val + std + max(rmse_vals)*0.01, 
                        f'{val:.1f}', ha='center', va='bottom', fontsize=9)
            
            # MAPE
            bars2 = ax2.bar(models, mape_vals, color='lightcoral', alpha=0.7, edgecolor='black')
            ax2.set_title("MAPE by Model", fontsize=12, fontweight='bold')
            ax2.set_ylabel("MAPE")
            ax2.grid(True, alpha=0.3)
            plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
            
            for bar, val in zip(bars2, mape_vals):
                ax2.text(bar.get_x() + bar.get_width()/2, val + max(mape_vals)*0.01, 
                        f'{val:.1%}', ha='center', va='bottom', fontsize=9)
            
            # R²
            bars3 = ax3.bar(models, r2_vals, color='lightgreen', alpha=0.7, edgecolor='black')
            ax3.set_title("R² by Model", fontsize=12, fontweight='bold')
            ax3.set_ylabel("R²")
            ax3.grid(True, alpha=0.3)
            plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
            
            for bar, val in zip(bars3, r2_vals):
                ax3.text(bar.get_x() + bar.get_width()/2, val + max(r2_vals)*0.01, 
                        f'{val:.2f}', ha='center', va='bottom', fontsize=9)
            
            plt.tight_layout()
            plt.savefig(output_dir / "model_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("✅ Model comparison plots created")
            
        except Exception as e:
            logger.warning(f"Model comparison plots failed: {e}")
    
    # Save metrics and validate performance
    dump_metrics(metrics_out, output_dir, enable_prom=CONFIG["enable_prometheus"])
    alerts = validate_model_performance(metrics_out, y.mean(), coverage)
    
    # Save detailed results
    if metrics_out:
        results_df = pd.DataFrame({
            model: {k: v for k, v in scores.items() if k in ["RMSE", "MAPE", "R2", "RMSE_std"]}
            for model, scores in metrics_out.items()
        }).T
        results_df.to_csv(output_dir / "model_results.csv")
    
    logger.info("✅ Model training completed successfully")
    return coverage

# -----------------------------------------------------------------------------
# Enhanced reporting and main pipeline
# -----------------------------------------------------------------------------

def generate_summary_report(metrics: dict, weekly: pd.DataFrame, coverage: float, 
                          alerts: List[str], output_dir: Path) -> None:
    """Generate comprehensive summary report with enhanced insights"""
    
    # Extract model metrics
    model_metrics = metrics.get("model_metrics", metrics)
    run_metadata = metrics.get("run_metadata", {})
    
    report_lines = [
        "# Call Volume Forecasting Pipeline - Executive Summary",
        f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**Run ID**: {run_metadata.get('run_id', 'N/A')}",
        f"**Version**: {run_metadata.get('version', 'N/A')}",
        "",
        "## Dataset Overview",  # REMOVED EMOJI
        f"- **Samples**: {weekly.shape[0]} weeks",
        f"- **Features**: {weekly.shape[1]} total features",
        f"- **Date Range**: {weekly.index.min().strftime('%Y-%m-%d')} to {weekly.index.max().strftime('%Y-%m-%d')}",
        f"- **Target Statistics**:",
        f"  - Mean: {weekly['calls_total'].mean():.1f} calls/week",
        f"  - Std: {weekly['calls_total'].std():.1f} calls/week", 
        f"  - Range: [{weekly['calls_total'].min():.0f}, {weekly['calls_total'].max():.0f}]",
        "",
        "## Model Performance (Nested Cross-Validation)",  # REMOVED EMOJI
    ]
    
    if model_metrics:
        # Sort models by RMSE
        sorted_models = sorted(model_metrics.items(), 
                             key=lambda x: x[1].get("RMSE", float('inf')))
        
        # Performance table header
        report_lines.extend([
            "| Model | RMSE | MAPE | R2 | Status |",
            "|-------|------|------|----|----|"
        ])
        
        for model_name, scores in sorted_models:
            if isinstance(scores, dict) and "RMSE" in scores:
                rmse = scores.get('RMSE', 0)
                mape = scores.get('MAPE', 0)
                r2 = scores.get('R2', 0)
                rmse_std = scores.get('RMSE_std', 0)
                
                # Determine status
                status = "Good"  # REMOVED EMOJI
                if rmse > weekly['calls_total'].mean() * 0.15:
                    status = "High Error"  # REMOVED EMOJI
                elif r2 < 0.3:
                    status = "Low R2"  # REMOVED EMOJI
                elif mape > 0.3:
                    status = "High MAPE"  # REMOVED EMOJI
                
                report_lines.append(
                    f"| {model_name} | {rmse:.1f}+/-{rmse_std:.1f} | {mape:.1%} | {r2:.3f} | {status} |"
                )
    
    # Write report with UTF-8 encoding
    try:
        with open(output_dir / "summary_report.md", 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        logger.info("Summary report generated")
    except Exception as e:
        logger.error(f"Failed to generate summary report: {e}")

def main():
    """
    FIXED: Main pipeline execution with comprehensive error handling and monitoring
    """
    
    # Setup with error handling
    try:
        output_dir = Path(CONFIG["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create logs directory
        logs_dir = output_dir / "logs"
        logs_dir.mkdir(exist_ok=True)
        
    except Exception as e:
        logger.error(f"Failed to create output directory: {e}")
        raise
    
    start_time = datetime.now()
    
    logger.info("=" * 80)
    logger.info("CALL VOLUME FORECASTING PIPELINE v6 (A+ Production Grade)")
    logger.info("=" * 80)
    logger.info(f"Run ID: {CONFIG['run_id']}")
    logger.info(f"Version: {CONFIG['version']}")
    logger.info(f"Output directory: {output_dir.resolve()}")
    logger.info(f"Configuration: {json.dumps({k: v for k, v in CONFIG.items() if k not in ['econ_tickers']}, indent=2, default=str)}")
    
    # Initialize tracking variables
    coverage = 0.0
    alerts = []
    
    try:
        # Step 1: Build dataset with comprehensive validation
        logger.info("\n[1/4] Building dataset with leakage-free feature engineering...")
        step_start = datetime.now()
        
        X, y, weekly = build_dataset()
        
        # Save processed dataset with metadata
        weekly.to_csv(output_dir / "weekly_dataset.csv")
        
        # Save dataset metadata
        dataset_metadata = {
            "shape": weekly.shape,
            "date_range": [weekly.index.min().isoformat(), weekly.index.max().isoformat()],
            "target_stats": {
                "mean": float(y.mean()),
                "std": float(y.std()),
                "min": float(y.min()),
                "max": float(y.max())
            },
            "feature_types": {
                "numeric": len(weekly.select_dtypes(include=[np.number]).columns),
                "categorical": len(weekly.select_dtypes(include=['category']).columns),
                "total": weekly.shape[1]
            }
        }
        
        with open(output_dir / "dataset_metadata.json", 'w') as f:
            json.dump(dataset_metadata, f, indent=2, default=str)
        
        logger.info(f"✅ Dataset built in {datetime.now() - step_start}")
        logger.info(f"Final dataset: {X.shape[0]} samples, {X.shape[1]} features")
        
        # Step 2: EDA with comprehensive visualizations
        logger.info("\n[2/4] Creating comprehensive EDA visualizations...")
        step_start = datetime.now()
        
        create_eda_plots(X, y, weekly, output_dir)
        logger.info(f"✅ EDA completed in {datetime.now() - step_start}")
        
        # Step 3: Model training with enhanced monitoring
        logger.info("\n[3/4] Training models with nested CV and robust evaluation...")
        step_start = datetime.now()
        
        coverage = train_models(X, y, output_dir)
        logger.info(f"✅ Model training completed in {datetime.now() - step_start}")
        
        # Step 4: Generate comprehensive report
        logger.info("\n[4/4] Generating executive summary and validation...")
        
        # Load metrics for reporting
        try:
            with open(output_dir / "metrics.json", 'r') as f:
                metrics = json.load(f)
        except Exception as e:
            logger.warning(f"Could not load metrics for reporting: {e}")
            metrics = {}
        
        # Validate performance and generate alerts
        model_metrics = metrics.get("model_metrics", metrics)
        if model_metrics:
            alerts = validate_model_performance(model_metrics, y.mean(), coverage)
        
        # Generate comprehensive report
        generate_summary_report(metrics, weekly, coverage, alerts, output_dir)
        
        # Final summary
        total_time = datetime.now() - start_time
        logger.info("=" * 80)
        logger.info("✅ PIPELINE COMPLETED SUCCESSFULLY")
        logger.info("=" * 80)
        logger.info(f"Total runtime: {total_time}")
        logger.info(f"All outputs saved to: {output_dir.resolve()}")
        
        # Performance summary
        if model_metrics:
            best_model = min(model_metrics.keys(), 
                            key=lambda x: model_metrics[x].get("RMSE", float('inf')) 
                            if isinstance(model_metrics[x], dict) else float('inf'))
            
            if isinstance(model_metrics[best_model], dict):
                logger.info(f"\n🏆 Best Model: {best_model}")
                logger.info(f"   RMSE: {model_metrics[best_model]['RMSE']:.2f}")
                logger.info(f"   MAPE: {model_metrics[best_model]['MAPE']:.2%}")
                logger.info(f"   R²: {model_metrics[best_model]['R2']:.3f}")
                if coverage > 0:
                    logger.info(f"   Interval Coverage: {coverage:.1%}")
        
        # Alert summary
        if alerts:
            logger.warning(f"\n⚠️ {len(alerts)} performance alerts generated - see summary report")
        else:
            logger.info("\n✅ All models passed performance validation")
        
        logger.info(f"\n📋 View detailed results: {output_dir}/summary_report.md")
        logger.info(f"🔍 Monitor performance: {output_dir}/metrics.json")
        
        # Success indicator for monitoring
        with open(output_dir / "pipeline_status.json", 'w') as f:
            json.dump({
                "status": "SUCCESS",
                "run_id": CONFIG["run_id"],
                "completion_time": datetime.now().isoformat(),
                "total_runtime_seconds": total_time.total_seconds(),
                "alerts_count": len(alerts),
                "best_model": best_model if 'best_model' in locals() else None,
                "coverage": coverage
            }, f, indent=2)
        
    except Exception as e:
        # Comprehensive error handling and reporting
        error_time = datetime.now()
        logger.error(f"💥 PIPELINE FAILED: {e}")
        
        import traceback
        error_traceback = traceback.format_exc()
        logger.error(f"Full traceback:\n{error_traceback}")
        
        # Save error report
        error_report = {
            "status": "FAILED",
            "run_id": CONFIG["run_id"],
            "error_time": error_time.isoformat(),
            "error_message": str(e),
            "error_traceback": error_traceback,
            "runtime_before_failure": (error_time - start_time).total_seconds()
        }
        
        try:
            with open(output_dir / "error_report.json", 'w') as f:
                json.dump(error_report, f, indent=2)
            logger.info(f"Error report saved to: {output_dir}/error_report.json")
        except:
            pass  # Don't fail on error reporting failure
        
        raise


if __name__ == "__main__":
    main()


























2025-07-17 10:23:02,535 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User requested assistance regarding their Consolidated Edison Inc stock, but the specific request was unclear or not mentioned.
2025-07-17 10:23:02,535 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User requested to know the balance in their Consolidated Edison Inc account.
2025-07-17 10:23:02,536 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User requested to speak with a senior support representative for further assistance.
2025-07-17 10:23:02,536 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User seeks assistance to add a beneficiary to the account.
2025-07-17 10:23:02,537 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User seeks assistance to update his address on his shareholder account for Alphabet Inc.
2025-07-17 10:23:02,537 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User sought specific assistance regarding Consolidated Edison Inc. shares, but the request was unclear.
2025-07-17 10:23:02,537 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User wants to add a beneficiary to their shareholder account.
2025-07-17 10:23:02,538 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User wants to sell some shares in Consolidated Edison Inc.
2025-07-17 10:23:02,538 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in User wants to transfer stock in Consolidated Edison Inc.
2025-07-17 10:23:02,538 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in Value
2025-07-17 10:23:02,539 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in Verify if spouse's name is on the account
2025-07-17 10:23:02,540 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in new service
2025-07-17 10:23:02,540 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in speak to a representative
2025-07-17 10:23:02,541 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in start service
2025-07-17 10:23:02,541 | 20250717T092249Z | WARNING | Data quality: Found 1 extreme outliers in verify account
2025-07-17 10:23:02,542 | 20250717T092249Z | INFO | Cleaning infinite and NaN values...
2025-07-17 10:23:02,614 | 20250717T092249Z | INFO | Cleaned dataset shape: (78, 380)
2025-07-17 10:23:02,615 | 20250717T092249Z | INFO | Cleaning column names for model compatibility...
2025-07-17 10:23:02,616 | 20250717T092249Z | INFO | Column names cleaned
2025-07-17 10:23:02,617 | 20250717T092249Z | INFO | Final dataset: 78 samples, 379 features
2025-07-17 10:23:02,618 | 20250717T092249Z | INFO | Target statistics: mean=58307.8, std=28634.0, range=[6172, 95291]
2025-07-17 10:23:02,880 | 20250717T092249Z | INFO | ✅ Dataset built in 0:00:13.324218
2025-07-17 10:23:02,881 | 20250717T092249Z | INFO | Final dataset: 78 samples, 379 features
2025-07-17 10:23:02,881 | 20250717T092249Z | INFO |
[2/4] Creating comprehensive EDA visualizations...
2025-07-17 10:23:02,882 | 20250717T092249Z | INFO | Creating EDA visualizations...
2025-07-17 10:23:04,431 | 20250717T092249Z | INFO | ✅ Time series plots created
2025-07-17 10:23:04,431 | 20250717T092249Z | INFO | ✅ EDA completed in 0:00:01.550127
2025-07-17 10:23:04,432 | 20250717T092249Z | INFO |
[3/4] Training models with nested CV and robust evaluation...
2025-07-17 10:23:04,432 | 20250717T092249Z | INFO | Starting model training with nested CV...
2025-07-17 10:23:04,442 | 20250717T092249Z | INFO | Training ElasticNet...
2025-07-17 10:23:17,223 | 20250717T092249Z | ERROR | Nested CV failed for ElasticNet: Invalid parameter 'alpha' for estimator Pipeline(steps=[('standardscaler', StandardScaler()),
                ('elasticnet', ElasticNet(max_iter=10000, random_state=42))]). Valid parameters are: ['memory', 'steps', 'transform_input', 'verbose'].
2025-07-17 10:23:17,223 | 20250717T092249Z | WARNING | Skipping ElasticNet due to CV failure
2025-07-17 10:23:17,224 | 20250717T092249Z | INFO | Training RandomForest...
2025-07-17 10:24:54,204 | 20250717T092249Z | INFO | ✅ RandomForest → RMSE: 22179.70±11944.24, MAPE: 32.26%, R²: -7.382
2025-07-17 10:24:55,683 | 20250717T092249Z | INFO | ✅ SHAP analysis completed for RandomForest
2025-07-17 10:24:55,684 | 20250717T092249Z | INFO | Training LightGBM...
2025-07-17 10:26:06,156 | 20250717T092249Z | INFO | ✅ LightGBM → RMSE: 21778.82±12143.71, MAPE: 32.47%, R²: -6.498
2025-07-17 10:26:07,534 | 20250717T092249Z | INFO | ✅ SHAP analysis completed for LightGBM
2025-07-17 10:26:07,534 | 20250717T092249Z | INFO | Training XGBoost...
2025-07-17 10:28:24,122 | 20250717T092249Z | INFO | ✅ XGBoost → RMSE: 19583.22±13209.51, MAPE: 28.34%, R²: -4.127
2025-07-17 10:28:25,697 | 20250717T092249Z | INFO | ✅ SHAP analysis completed for XGBoost
2025-07-17 10:28:25,697 | 20250717T092249Z | INFO | Training stacking ensemble...
2025-07-17 10:28:34,619 | 20250717T092249Z | ERROR | Ensemble training failed: cross_val_predict only works for partitions
2025-07-17 10:28:34,619 | 20250717T092249Z | INFO | Generating conformal prediction intervals using XGBoost...
2025-07-17 10:28:43,950 | 20250717T092249Z | INFO | Conformal intervals: 90% target, 100.0% actual coverage
2025-07-17 10:28:45,186 | 20250717T092249Z | INFO | ✅ Prediction intervals: 100.0% coverage
2025-07-17 10:28:46,102 | 20250717T092249Z | INFO | ✅ Model comparison plots created
2025-07-17 10:28:46,128 | 20250717T092249Z | INFO | Metrics written atomically to dist\metrics.json
2025-07-17 10:28:46,144 | 20250717T092249Z | ERROR | Failed to write Prometheus metrics: [WinError 1314] A required privilege is not held by the client: 'prometheus_metrics_20250717T092249Z.prom' -> 'dist\\prometheus_metrics_latest.prom'
2025-07-17 10:28:46,147 | 20250717T092249Z | WARNING | RandomForest RMSE 22179.70 exceeds 8746.17 (15% of mean)
2025-07-17 10:28:46,147 | 20250717T092249Z | WARNING | RandomForest R² -7.382 below threshold 0.3
2025-07-17 10:28:46,148 | 20250717T092249Z | WARNING | RandomForest MAPE 32.3% exceeds threshold 30.0%
2025-07-17 10:28:46,148 | 20250717T092249Z | WARNING | LightGBM RMSE 21778.82 exceeds 8746.17 (15% of mean)
2025-07-17 10:28:46,149 | 20250717T092249Z | WARNING | LightGBM R² -6.498 below threshold 0.3
2025-07-17 10:28:46,150 | 20250717T092249Z | WARNING | LightGBM MAPE 32.5% exceeds threshold 30.0%
2025-07-17 10:28:46,150 | 20250717T092249Z | WARNING | XGBoost RMSE 19583.22 exceeds 8746.17 (15% of mean)
2025-07-17 10:28:46,150 | 20250717T092249Z | WARNING | XGBoost R² -4.127 below threshold 0.3
2025-07-17 10:28:46,151 | 20250717T092249Z | WARNING | ⚠️ 8 performance alerts generated
2025-07-17 10:28:46,161 | 20250717T092249Z | INFO | ✅ Model training completed successfully
2025-07-17 10:28:46,168 | 20250717T092249Z | INFO | ✅ Model training completed in 0:05:41.735526
2025-07-17 10:28:46,168 | 20250717T092249Z | INFO |
[4/4] Generating executive summary and validation...
2025-07-17 10:28:46,213 | 20250717T092249Z | WARNING | RandomForest RMSE 22179.70 exceeds 8746.17 (15% of mean)
2025-07-17 10:28:46,214 | 20250717T092249Z | WARNING | RandomForest R² -7.382 below threshold 0.3
2025-07-17 10:28:46,214 | 20250717T092249Z | WARNING | RandomForest MAPE 32.3% exceeds threshold 30.0%
2025-07-17 10:28:46,215 | 20250717T092249Z | WARNING | LightGBM RMSE 21778.82 exceeds 8746.17 (15% of mean)
2025-07-17 10:28:46,215 | 20250717T092249Z | WARNING | LightGBM R² -6.498 below threshold 0.3
2025-07-17 10:28:46,216 | 20250717T092249Z | WARNING | LightGBM MAPE 32.5% exceeds threshold 30.0%
2025-07-17 10:28:46,216 | 20250717T092249Z | WARNING | XGBoost RMSE 19583.22 exceeds 8746.17 (15% of mean)
2025-07-17 10:28:46,217 | 20250717T092249Z | WARNING | XGBoost R² -4.127 below threshold 0.3
2025-07-17 10:28:46,217 | 20250717T092249Z | WARNING | ⚠️ 8 performance alerts generated
2025-07-17 10:28:46,222 | 20250717T092249Z | INFO | Summary report generated
2025-07-17 10:28:46,223 | 20250717T092249Z | INFO | ================================================================================
2025-07-17 10:28:46,224 | 20250717T092249Z | INFO | ✅ PIPELINE COMPLETED SUCCESSFULLY
2025-07-17 10:28:46,224 | 20250717T092249Z | INFO | ================================================================================
2025-07-17 10:28:46,225 | 20250717T092249Z | INFO | Total runtime: 0:05:56.671441
2025-07-17 10:28:46,227 | 20250717T092249Z | INFO | All outputs saved to: C:\Users\BhungarD\OneDrive - Computershare\Desktop\finprod\dist
2025-07-17 10:28:46,227 | 20250717T092249Z | INFO |
🏆 Best Model: XGBoost
2025-07-17 10:28:46,228 | 20250717T092249Z | INFO |    RMSE: 19583.22
2025-07-17 10:28:46,228 | 20250717T092249Z | INFO |    MAPE: 28.34%
2025-07-17 10:28:46,229 | 20250717T092249Z | INFO |    R²: -4.127
2025-07-17 10:28:46,229 | 20250717T092249Z | INFO |    Interval Coverage: 100.0%
2025-07-17 10:28:46,230 | 20250717T092249Z | WARNING |
⚠️ 8 performance alerts generated - see summary report
2025-07-17 10:28:46,230 | 20250717T092249Z | INFO |
📋 View detailed results: dist/summary_report.md
2025-07-17 10:28:46,231 | 20250717T092249Z | INFO | 🔍 Monitor performance: dist/metrics.json
PS C:\Users\Bhun
